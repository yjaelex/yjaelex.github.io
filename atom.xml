<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>蓝天和白云的博客</title>
  <subtitle>一个计算机图形爱好者</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yjaelex.github.io/"/>
  <updated>2016-12-29T08:39:59.143Z</updated>
  <id>http://yjaelex.github.io/</id>
  
  <author>
    <name>Alex</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[原]SPIR-V 研究：编译器基本原理（一）</title>
    <link href="http://yjaelex.github.io/2015/12/03/%E5%8E%9F-SPIR-V-%E7%A0%94%E7%A9%B6%EF%BC%9A%E7%BC%96%E8%AF%91%E5%99%A8%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yjaelex.github.io/2015/12/03/原-SPIR-V-研究：编译器基本原理（一）/</id>
    <published>2015-12-03T09:31:05.000Z</published>
    <updated>2016-12-29T08:39:59.143Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><h1 id="SPIR-V-研究：编译器基本原理（一）"><a href="#SPIR-V-研究：编译器基本原理（一）" class="headerlink" title="SPIR-V 研究：编译器基本原理（一）"></a>SPIR-V 研究：编译器基本原理（一）</h1><p>前面转过两篇关于SPIR-V 中间语言的介绍；接下来笔者准备深入学习一下SPIR-V的标准。根据标准，<strong>SPIR-V是以一种二进制格式存在的，并且函数还是以控制流图CFG的形式存在；数据结构也保留了高级语言里的层级关系</strong>。（<a href="https://en.wikipedia.org/wiki/Standard_Portable_Intermediate_Representation" rel="external nofollow noopener noreferrer" target="_blank">https://en.wikipedia.org/wiki/Standard_Portable_Intermediate_Representation</a>）</p><p>这样做的目的是为了更好的在目标平台上进行优化；同时Khronos也放出了官方标准的开源编译器<a href="https://github.com/KhronosGroup/glslang" rel="external nofollow noopener noreferrer" target="_blank">Glslang</a>。 所以，为了更好的了解SPIR-V，我们有必要先温习一下编译器的基本原理，特别是前端的词法分析、语法分析、语义分析和中间语言生成。</p><a id="more"></a><hr><h2 id="编译器基本结构"><a href="#编译器基本结构" class="headerlink" title="编译器基本结构"></a>编译器基本结构</h2><p>下图是一个一般编译器的编译高级语言的过程。</p><p><img src="http://img.blog.csdn.net/20151203172900409" alt="这里写图片描述"></p><p>基本上就是预处理（比如C/C++里宏汇编），编译并汇编（每个源码文件都生成对应的.o文件，这时的代码是relocatable的），链接生成可执行文件（linker，把许多.o文件以及用到的库函数.lib文件合并并作全局优化），最后OS的加载器会加载可执行文件并运行之（Windows上是PE格式，Linux上市ELF格式）。具体细节可以参考<a href="http://www.amazon.com/Linkers-Kaufmann-Software-Engineering-Programming/dp/1558604960" rel="external nofollow noopener noreferrer" target="_blank">Linkers and Loaders</a> 这本书。</p><p>下面我们重点看编译器的结构。</p><p><img src="http://img.blog.csdn.net/20151203173049347" alt="这里写图片描述"></p><p>这是一般编译器的结构，主要分前端和后端，以中间语言生成为界限。前端主要是分析语法语义并生成代码的中间表示，这主要包括lexer 和 parser，以及语义分析；后端主要是生成目标机器代码，包括分析并优化，寄存器分配等。</p><p>下图是编译器的主要编译过程（phase）。</p><p><img src="http://img.blog.csdn.net/20151203172941801" alt="这里写图片描述"></p><p><strong>这里我们研究的目标SPIR-V就是一种标准化的中间语言表示。</strong>所以我们更关心的是前端的研究，这也是编译器里比较成熟，相对简单的部分。以前的OpenGL里，GLSL写成的Shader是高级语言（相当于C++)，OpenGL的驱动会把它编译成显卡GPU对应的机器码。所以各家会有不同的中间表示；Vulkan会统一标准用SPIR-V了。</p><p><img src="http://img.blog.csdn.net/20151203173018532" alt="这里写图片描述"></p><h2 id="计算机语言"><a href="#计算机语言" class="headerlink" title="计算机语言"></a>计算机语言</h2><p>计算机科学里的语言可以看作是一个字母表（alphabet ）上的某些有限长字符串的集合；这一般可以包含无限多个字符串，也就是无限集合（Infinite Sets）。这里有限长字符串就是“sentences”；sentences是word（或token）组成，它有一定的结构；token则是letter由一定规则组成；letter个数是有限的，它们全部取自字母表（alphabet ）。</p><p>这是个有层次的定义。</p><table><br><thead><br><tr><br><th align="left">Layer</th><br><th align="right">分析对象</th><br><th align="center">组成元素</th><br></tr><br></thead><br><tbody><tr><br><td align="left">词法（Lexical structure ）</td><br><td align="right">token</td><br><td align="center">letter</td><br></tr><br><tr><br><td align="left">语法（Syntactic structure）</td><br><td align="right">sentences</td><br><td align="center">token</td><br></tr><br><tr><br><td align="left">语义（Semantics）</td><br><td align="right">语法树</td><br><td align="center"></td><br></tr><br></tbody></table><p>语言是有一定的语法规则的（grammar）。而grammar是一组有限的规则的集合；计算机语言里的grammar是不同于一般自然语言的语法，它可以构造出所有可能的sentences。所以这是一个利用有限规则来产生无限的合法语句的问题。</p><p>而grammar又可以用正则表达式来定义；比如GLSL的语法<a href="https://github.com/KhronosGroup/glslang/blob/master/glslang/MachineIndependent/glslang.y" rel="external nofollow noopener noreferrer" target="_blank">glslang.y</a>。</p><p><br>本文地址 <a href="http://yjaelex.github.io/2015/12/03/原-SPIR-V-研究：编译器基本原理（一）/">http://yjaelex.github.io/2015/12/03/原-SPIR-V-研究：编译器基本原理（一）/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;h1 id=&quot;SPIR-V-研究：编译器基本原理（一）&quot;&gt;&lt;a href=&quot;#SPIR-V-研究：编译器基本原理（一）&quot; class=&quot;headerlink&quot; title=&quot;SPIR-V 研究：编译器基本原理（一）&quot;&gt;&lt;/a&gt;SPIR-V 研究：编译器基本原理（一）&lt;/h1&gt;&lt;p&gt;前面转过两篇关于SPIR-V 中间语言的介绍；接下来笔者准备深入学习一下SPIR-V的标准。根据标准，&lt;strong&gt;SPIR-V是以一种二进制格式存在的，并且函数还是以控制流图CFG的形式存在；数据结构也保留了高级语言里的层级关系&lt;/strong&gt;。（&lt;a href=&quot;https://en.wikipedia.org/wiki/Standard_Portable_Intermediate_Representation&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://en.wikipedia.org/wiki/Standard_Portable_Intermediate_Representation&lt;/a&gt;）&lt;/p&gt;&lt;p&gt;这样做的目的是为了更好的在目标平台上进行优化；同时Khronos也放出了官方标准的开源编译器&lt;a href=&quot;https://github.com/KhronosGroup/glslang&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Glslang&lt;/a&gt;。 所以，为了更好的了解SPIR-V，我们有必要先温习一下编译器的基本原理，特别是前端的词法分析、语法分析、语义分析和中间语言生成。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[转][转]The Mali GPU: An Abstract Machine, Part 3 - The Shader Core</title>
    <link href="http://yjaelex.github.io/2015/12/03/%E8%BD%AC-%E8%BD%AC-The-Mali-GPU-An-Abstract-Machine-Part-3-The-Shader-Core/"/>
    <id>http://yjaelex.github.io/2015/12/03/转-转-The-Mali-GPU-An-Abstract-Machine-Part-3-The-Shader-Core/</id>
    <published>2015-12-03T02:08:43.000Z</published>
    <updated>2016-12-29T08:41:14.215Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><p>第三篇，介绍Shader Core。</p><h2 id="1-英文原文"><a href="#1-英文原文" class="headerlink" title="1. 英文原文"></a>1. 英文原文</h2><p>In the first two blogs of this series I introduced the frame-level pipelining [<a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/03/the-mali-gpu-an-abstract-machine-part-1" rel="external nofollow noopener noreferrer" target="_blank">The<br>Mali GPU: An Abstract Machine, Part 1 - Frame Pipelining</a>] and tile based rendering architecture [<a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/20/the-mali-gpu-an-abstract-machine-part-2" rel="external nofollow noopener noreferrer" target="_blank">The<br>Mali GPU: An Abstract Machine, Part 2 - Tile-based Rendering</a>] used by the Mali GPUs, aiming to develop a mental model which developers can use to explain the behavior of the graphics stack when optimizing the performance of their applications.</p><p>&nbsp;</p><p>In this blog I will finish the construction of this abstract machine, forming the final component: the Mali GPU itself.&nbsp; This blog assumes you have read the first two parts in the series, so I would recommend starting with those if you have not read them already.</p><p>&nbsp;</p><a id="more"></a><p>##<br>GPU Architecture</p><p>&nbsp;</p><p>The &quot;Midgard&quot; family of Mali GPUs&nbsp; (the Mali-T600 and Mali-T700 series) use a unified shader core architecture, meaning that only a single type of shader core exists in the design. This single core can execute all types of programmable shader code, including<br>vertex shaders, fragment shaders, and compute kernels.</p><p>&nbsp;</p><p>The exact number of shader cores present in a particular silicon chip varies; our silicon partners can choose how many shader cores they implement based on their performance needs and silicon area constraints. The Mali-T760 GPU can scale from a single core<br>for low-end devices all the way up to 16 cores for the highest performance designs, but between 4 and 8 cores are the most common implementations.</p><p>&nbsp;</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2906-7387/mali-top-level.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2906-7387/mali-top-level.png" alt="mali-top-level.png"></a></p><p>The graphics work for the GPU is queued in a pair of queues, one for vertex/tiling workloads and one for fragment workloads, with all work for one render target being submitted as a single submission into each queue. Workloads from both queues can be processed<br>by the GPU at the same time, so vertex processing and fragment processing for different render targets can be running in parallel (see the first blog for more details on this pipelining methodology). The workload for a single render target is broken into smaller<br>pieces and distributed across all of the shader cores in the GPU, or in the case of tiling workloads (see the second blog in this series for an overview of tiling) a fixed function tiling unit.</p><p>&nbsp;</p><p>The shader cores in the system share a level 2 cache to improve performance, and to reduce memory bandwidth caused by repeated data fetches. Like the number of cores, the size of the L2 is configurable by our silicon partners, but is typically in the range<br>of 32-64KB per shader core in the GPU depending on how much silicon area is available. The number and bus width of the memory ports this cache has to external memory is configurable, again allowing our partners to tune the implementation to meet their performance,<br>power, and area needs. In general we aim to be able to write one 32-bit pixel per core per clock, so it would be reasonable to expect an 8-core design to have a total of 256-bits of memory bandwidth (for both read and write) per clock cycle.</p><p>&nbsp;</p><p>##<br>Mali GPU Shader Core</p><p>&nbsp;</p><p>The Mali shader core is structured as a number of fixed-function hardware blocks wrapped around a programmable &quot;tripipe&quot; execution core. The fixed function units perform the setup for a shader operation - such as rasterizing triangles or performing depth testing</p><ul><li>or handling the post-shader activities - such as blending, or writing back a whole tile’s worth of data at the end of rendering. The tripipe itself is the programmable part responsible for the execution of shader programs.</li></ul><p>&nbsp;</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2906-7388/mali-top-core.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2906-7388/mali-top-core.png" alt="mali-top-core.png"></a></p><p>&nbsp;</p><p>###<br>The Tripipe</p><p>&nbsp;</p><p>There are three classes of execution pipeline in the tripipe design: one handling arithmetic operations, one handling memory load/store and varying access, and one handling texture access. There is one load/store and one texture pipe per shader core, but the<br>number of arithmetic pipelines can vary depending on which GPU you are using; most silicon shipping today will have two arithmetic pipelines, but GPU variants with up to four pipelines are also available.</p><p>&nbsp;</p><p>###<br>Massively Multi-threaded Machine</p><p>&nbsp;</p><p>Unlike a traditional CPU architecture, where you will typically only have a single thread of execution at a time on a single core, the tripipe is a massively multi-threaded processing engine. There may well be hundreds of hardware threads running at the same<br>time in the tripipe, with one thread created for each vertex or fragment which is shaded. This large number of threads exists to hide memory latency; it doesn’t matter if some threads are stalled waiting for memory, as long as at least one thread is available<br>to execute then we maintain efficient execution.</p><p>&nbsp;</p><p>###<br>Arithmetic Pipeline: Vector Core</p><p>&nbsp;</p><p>The arithmetic pipeline (A-pipe) is a&nbsp;<a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSIMD" rel="external nofollow noopener noreferrer" target="_blank">SIMD</a>&nbsp;(single<br>instruction multiple data) vector processing engine, with arithmetic units which operate on 128-bit quad-word registers. The registers can be flexibly accessed as either 2 x FP64, 4 x FP32, 8 x FP16, 2 x int64, 4 x int32, 8 x int16, or 16 x int8. It is therefore<br>possible for a single arithmetic vector task to operate on 8 &quot;mediump&quot; values in a single operation, and for OpenCL kernels operating on 8-bit luminance data to process 16 pixels per SIMD unit per clock cycle.</p><p>&nbsp;</p><p>While I can’t disclose the internal architecture of the arithmetic pipeline, our public performance data for each GPU can be used to give some idea of the number of maths units available. For example, the Mali-T760 with 16 cores is rated at 326 FP32 GFLOPS<br>at 600MHz. This gives a total of 34 FP32 FLOPS per clock cycle for this shader core; it has two pipelines, so that’s 17 FP32 FLOPS per pipeline per clock cycle. The available performance in terms of operations will increase for FP16/int16/int8 and decrease<br>for FP64/int64 data types.</p><p>&nbsp;</p><p>###<br>Texture Pipeline</p><p>&nbsp;</p><p>The texture pipeline (T-pipe) is responsible for all memory access to do with textures. The texture pipeline can return one bilinear filtered texel per clock; trilinear filtering requires us to load samples from two different mipmaps in memory, so requires<br>a second clock cycle to complete.</p><p>&nbsp;</p><p>###<br>Load/Store Pipeline</p><p>&nbsp;</p><p>The load/store pipeline (LS-pipe) is responsible for all memory accesses which are not related to texturing.&nbsp; For graphics workloads this means reading attributes and writing varyings during vertex shading, and reading varyings during fragment shading. In general<br>every instruction is a single memory access operation, although like the arithmetic pipeline they are vector operations and so could load an entire &quot;highp&quot; vec4 varying in a single instruction.</p><p>&nbsp;</p><p>###<br>Early ZS Testing and Late ZS Testing</p><p>&nbsp;</p><p>In the OpenGL ES specification &quot;fragment operations&quot; - which include depth and stencil testing - happen at the end of the pipeline, after fragment shading has completed. This makes the specification very simple, but implies that you have to spend lots of time<br>shading something, only to throw it away at the end of the frame if it turns out to be killed by ZS testing. Coloring fragments just to discard them would cost a huge amount of performance and wasted energy, so where possible we will do ZS testing early (i.e.<br>before fragment shading), only falling back to late ZS testing (i.e. after fragment shading) where it is unavoidable (e.g. a dependency on fragment which may call &quot;discard&quot; and as such has indeterminate depth state until it exits the tripipe).</p><p>&nbsp;</p><p>In addition to the traditional early-z schemes, we also have some overdraw removal capability which can stop fragments which have already been rasterized from turning into real rendering work if they do not contribute to the output scene in a useful way. My<br>colleague&nbsp;<a href="https://community.arm.com/people/seanellis" rel="external nofollow noopener noreferrer" target="_blank">seanellis</a>&nbsp;has<br>a great blog looking at this technology -<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;line-height:1.5em">&nbsp;</span><a href="https://community.arm.com/groups/arm-mali-graphics/blog/2013/08/08/killing-pixels--a-new-optimization-for-shading-on-arm-mali-gpus" rel="external nofollow noopener noreferrer" target="_blank">Killing<br>Pixels - A New Optimization for Shading on ARM Mali GPUs&nbsp;</a><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;line-height:1.5em">- so I won’t dive into<br>any more detail here.</span></p><p>&nbsp;</p><p>##<br>Memory System</p><p>&nbsp;</p><p>This section is an after-the-fact addition to this blog, so if you have read this blog before and don’t remember this section, don’t worry you’re not going crazy. We have been getting a lot of questions from developers writing OpenCL kernels and OpenGL ES compute<br>shaders asking for more information about the GPU cache structure, as it can be really beneficial to lay out data structures and buffers to optimize cache locality. The salient facts are:</p><p>&nbsp;</p><ul><li>Two 16KB L1 data caches per shader core; one for texture access and one for generic memory access.</li><li>A single logical L2 which is shared by all of the shader cores. The size of this is variable and can be configured by the silicon integrator, but is typically between 32 and 64 KB per instantiated shader core.</li><li>Both cache levels<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;line-height:1.5em">&nbsp;use 64 byte cache lines.</span></li></ul><p>&nbsp;</p><p>If you are new to optimization of massively multi-threaded algorithms on massively multi-threaded architectures I would heartily recommend the SGEMM matrix multiplication video on our Mali Developer portal here:</p><p>&nbsp;</p><ul><li><a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fmalideveloper.arm.com%2Fdevelop-for-mali%2Fopencl-renderscript-tutorials%2F%23example" rel="external nofollow noopener noreferrer" target="_blank">http://malideveloper.arm.com/develop-for-mali/opencl-renderscript-tutorials/#example</a></li></ul><p>&nbsp;</p><p>… as the overall system behavior can be very different to what you are used to if you are coming from a traditional CPU background.</p><p>&nbsp;</p><p>##<br>GPU Limits</p><p>&nbsp;</p><p>Based on this simple model it is possible to outline some of the fundamental properties underpinning the GPU performance.</p><p>&nbsp;</p><ul><li>The GPU can issue one vertex per shader core per clock</li><li>The GPU can issue one fragment per shader core per clock</li><li>The GPU can retire one pixel per shader core per clock</li><li><p>We can issue one instruction per pipe per clock, so for a typical shader core we can issue four instructions in parallel if we have them available to run</p><ul><li>We can achieve 17 FP32 operations per A-pipe</li><li>One vector load, one vector store, or one vector varying per LS-pipe</li><li>One bilinear filtered texel per T-pipe</li></ul></li><li><p>The GPU will typically have 32-bits of DDR access (read and write) per core per clock [configurable]</p></li></ul><p>&nbsp;</p><p><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;line-height:1.5em">If we scale this to a Mali-T760 MP8 running at 600MHz we can calculate the theoretical<br>peak performance as:</span></p><p>&nbsp;</p><ul><li><p>Fillrate:</p><ul><li>8 pixels per clock = 4.8 GPix/s</li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-family:inherit;vertical-align:baseline">That’s 2314 complete 1080p frames per second!</span></li></ul></li><li><p>Texture rate:</p><ul><li>8 bilinear texels per clock = 4.8 GTex/s</li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-family:inherit;vertical-align:baseline">That’s 38 bilinear filtered texture lookups per pixel for 1080p @ 60 FPS!</span></li></ul></li><li><p>Arithmetic rate:</p><ul><li>17 FP32 FLOPS per pipe per core = 163 FP32 GFLOPS</li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-family:inherit;vertical-align:baseline">That’s 1311 FLOPS per pixel for 1080p @ 60 FPS!</span></li></ul></li><li><p>Bandwidth:</p><ul><li>256-bits of memory access per clock = 19.2GB/s read and write bandwidth<sup>1</sup><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;line-height:1.5em">.</span></li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;line-height:1.5em"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-size:13.33px;font-family:inherit;vertical-align:baseline">That’s<br>154 bytes per pixel for 1080p @ 60 FPS!</span></span></li></ul></li></ul><p>&nbsp;</p><p>##<br>OpenCL and Compute</p><p>&nbsp;</p><p>The observant reader will have noted that I’ve talked a lot about vertices and fragments - the staple of graphics work - but have mentioned very little about how OpenCL and RenderScript compute threads come into being inside the core. Both of these types of<br>work behave almost identically to vertex threads - you can view running a vertex shader over an array of vertices as a 1-dimensional compute problem. So the vertex thread creator also spawns compute threads, although more accurately I would say the compute<br>thread creator also spawns vertices&nbsp;<span class="emoticon-inline emoticon_wink" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline;display:inline-block;height:16px;width:16px"></span>.</p><p>&nbsp;</p><p>##<br>Next Time …</p><p>&nbsp;</p><p>This blog concludes the first chapter of this series, developing the abstract machine which defines the basic behaviors which an application developer should expect to see for a Mali GPU in the Midgard family. Over the rest of this series I’ll start to put<br>this new knowledge to work, investigating some common application development pitfalls, and useful optimization techniques, which can be identified and debugged using the Mali integration into the ARM DS-5 Streamline profiling tools.</p><p>&nbsp;</p><p><span style="margin:0;padding:0;border:0;font-style:inherit;font-family:inherit;vertical-align:baseline">EDIT: Next blog now available:</span></p><ul><li><a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/04/02/mali-graphics-performance-1-checking-the-pipeline" rel="external nofollow noopener noreferrer" target="_blank">Mali<br>Performance 1: Checking the Pipeline</a></li></ul><p>&nbsp;</p><p>Comments and questions welcomed as always,</p><p>TTFN,</p><p>Pete</p><p>&nbsp;</p><p>###<br>Footnotes</p><p>&nbsp;</p><ol><li>… 19.2GB/s subject to the ability of the rest of the memory system outside of the GPU to give us data this quickly. Like most features of an ARM-based chip, the down-stream memory system is highly configurable in order to allow different vendors to tune power,<br>performance, and silicon area according to their needs. For most SoC parts the rest of the system will throttle the available bandwidth before the GPU runs out of an ability to request data. It is unlikely you would want to sustain this kind of bandwidth for<br>prolonged periods, but short burst performance is important.</li></ol><p><br>本文地址 <a href="http://yjaelex.github.io/2015/12/03/转-转-The-Mali-GPU-An-Abstract-Machine-Part-3-The-Shader-Core/">http://yjaelex.github.io/2015/12/03/转-转-The-Mali-GPU-An-Abstract-Machine-Part-3-The-Shader-Core/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;第三篇，介绍Shader Core。&lt;/p&gt;&lt;h2 id=&quot;1-英文原文&quot;&gt;&lt;a href=&quot;#1-英文原文&quot; class=&quot;headerlink&quot; title=&quot;1. 英文原文&quot;&gt;&lt;/a&gt;1. 英文原文&lt;/h2&gt;&lt;p&gt;In the first two blogs of this series I introduced the frame-level pipelining [&lt;a href=&quot;https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/03/the-mali-gpu-an-abstract-machine-part-1&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The&lt;br&gt;Mali GPU: An Abstract Machine, Part 1 - Frame Pipelining&lt;/a&gt;] and tile based rendering architecture [&lt;a href=&quot;https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/20/the-mali-gpu-an-abstract-machine-part-2&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The&lt;br&gt;Mali GPU: An Abstract Machine, Part 2 - Tile-based Rendering&lt;/a&gt;] used by the Mali GPUs, aiming to develop a mental model which developers can use to explain the behavior of the graphics stack when optimizing the performance of their applications.&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;In this blog I will finish the construction of this abstract machine, forming the final component: the Mali GPU itself.&amp;nbsp; This blog assumes you have read the first two parts in the series, so I would recommend starting with those if you have not read them already.&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[转][转] The Mali GPU: An Abstract Machine, Part 2 - Tile-based Rendering</title>
    <link href="http://yjaelex.github.io/2015/12/03/%E8%BD%AC-%E8%BD%AC-The-Mali-GPU-An-Abstract-Machine-Part-2-Tile-based-Rendering/"/>
    <id>http://yjaelex.github.io/2015/12/03/转-转-The-Mali-GPU-An-Abstract-Machine-Part-2-Tile-based-Rendering/</id>
    <published>2015-12-03T02:05:18.000Z</published>
    <updated>2016-12-29T08:41:45.281Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><p>第二篇，介绍了Tile-based rendering。一样有中英文对照。</p><h2 id="1-英文原文"><a href="#1-英文原文" class="headerlink" title="1. 英文原文"></a>1. 英文原文</h2><p>In my&nbsp;<a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/03/the-mali-gpu-an-abstract-machine-part-1" rel="external nofollow noopener noreferrer" target="_blank">previous<br>blog</a>&nbsp;I started defining an abstract machine which can be used to describe the application-visible behaviors of the Mali GPU and driver software. The purpose of this machine is to give developers a mental model of the interesting behaviors beneath the OpenGL<br>ES API, which can in turn be used to explain issues which impact their application’s performance. I will use this model in the future blogs of this series to explore some common performance pot-holes which developers encounter when developing graphics applications.</p><p>&nbsp;</p><p>This blog continues the development of this abstract machine, looking at the tile-based rendering model of the Mali GPU family. I’ll assume you’ve read the first blog on pipelining; if you haven’t I would suggest reading that first.</p><p>&nbsp;</p><a id="more"></a><p>##<br>The “Traditional” Approach</p><p>&nbsp;</p><p>In a traditional mains-powered desktop GPU architecture — commonly called an immediate mode architecture — the fragment shaders are executed on each primitive, in each draw call, in sequence. Each primitive is rendered to completion before starting the next<br>one, with an algorithm which approximates to:</p><p>&nbsp;</p><div class="dp-highlighter" style="padding:1px 0 0;border:0;font-family:Consolas,'Courier New',Courier,mono,serif;vertical-align:baseline;width:auto;overflow:visible;color:#565b5b;line-height:25px;margin:0!important;background-color:#e7e5dc"><br><div class="bar" style="margin:0;padding:0 0 0 45px;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br></div><br></div><ol><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;vertical-align:baseline;color:#000;font-size:9pt!important;background-color:inherit"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;vertical-align:baseline;font-size:9pt!important;background-color:inherit">&nbsp;&nbsp;&nbsp;&nbsp;foreach(&nbsp;primitive&nbsp;)&nbsp;&nbsp;</span></span></li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;vertical-align:baseline;color:#000;font-size:9pt!important;background-color:inherit">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;foreach(&nbsp;fragment&nbsp;)&nbsp;&nbsp;</span></li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;vertical-align:baseline;color:#000;font-size:9pt!important;background-color:inherit">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;render&nbsp;fragment&nbsp;&nbsp;</span><br></li></ol></div><p><span lang="EN-US" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"></span></p><p></p><p>As any triangle in the stream may cover any part of the screen the working set of data maintained by these renderers is large; typically at least a full-screen size color buffer, depth buffer, and possibly a stencil buffer too. A typical working set for a modern<br>device will be 32 bits-per-pixel (bpp) color, and 32bpp packed depth/stencil. A 1080p display therefore has a working set of 16MB, and a 4k2k TV has a working set of 64MB.&nbsp; Due to their size these working buffers must be stored off-chip in a DRAM.</p><p>&nbsp;</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2804-7077/model-imr.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2804-7077/model-imr.png" alt="model-imr.png"></a></p><p>&nbsp;</p><p>Every blending, depth testing, and stencil testing operation requires the current value of the data for the current fragment’s pixel coordinate to be fetched from this working set. All fragments shaded will typically touch this working set, so at high resolutions<br>the bandwidth load placed on this memory can be exceptionally high, with multiple read-modify-write operations per fragment, although caching can mitigate this slightly. This need for high bandwidth access in turn drives the need for a wide memory interface<br>with lots of pins, as well as specialized high-frequency memory, both of which result in external memory accesses which are particularly energy intensive.</p><p>&nbsp;</p><p>##<br>The Mali Approach</p><p>&nbsp;</p><p>The Mali GPU family takes a very different approach, commonly called tile-based rendering, designed to minimize the amount of power hungry external memory accesses which are needed during rendering. As described in&nbsp;<a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/03/the-mali-gpu-an-abstract-machine-part-1" rel="external nofollow noopener noreferrer" target="_blank">the<br>first blog</a>&nbsp;in this series, Mali uses a distinct two-pass rendering algorithm for each render target. It first executes all of the geometry processing, and then executes all of the fragment processing. During the geometry processing stage, Mali GPUs break<br>up the screen into small 16x16 pixel tiles and construct a list of which rendering primitives are present in each tile. When the GPU fragment shading step runs, each shader core processes one 16x16 pixel tile at a time, rendering it to completion before starting<br>the next one. For tile-based architectures the algorithm equates to:</p><p>&nbsp;</p><div class="dp-highlighter" style="padding:1px 0 0;border:0;font-family:Consolas,'Courier New',Courier,mono,serif;vertical-align:baseline;width:auto;overflow:visible;color:#565b5b;line-height:25px;margin:0!important;background-color:#e7e5dc"><br><div class="bar" style="margin:0;padding:0 0 0 45px;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br></div><br></div><ol><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;vertical-align:baseline;color:#000;font-size:9pt!important;background-color:inherit"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;vertical-align:baseline;font-size:9pt!important;background-color:inherit">&nbsp;&nbsp;&nbsp;&nbsp;foreach(&nbsp;tile&nbsp;)&nbsp;&nbsp;</span></span></li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;vertical-align:baseline;color:#000;font-size:9pt!important;background-color:inherit">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;foreach(&nbsp;primitive&nbsp;in&nbsp;tile&nbsp;)&nbsp;&nbsp;</span></li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;vertical-align:baseline;color:#000;font-size:9pt!important;background-color:inherit">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;foreach(&nbsp;fragment&nbsp;in&nbsp;primitive&nbsp;in&nbsp;tile&nbsp;)&nbsp;&nbsp;</span></li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;vertical-align:baseline;color:#000;font-size:9pt!important;background-color:inherit">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;render&nbsp;fragment&nbsp;&nbsp;</span><br></li></ol></div><p>&nbsp;</p><p>As a 16x16 tile is only a small fraction of the total screen area it is possible to keep the entire working set (color, depth, and stencil) for a whole tile in a fast RAM which is tightly coupled with the GPU shader core.</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2804-7079/model-tbr.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2804-7079/model-tbr.png" alt="model-tbr.png"></a></p><p>&nbsp;</p><p>This tile-based approach has a number of advantages. They are mostly transparent to the developer but worth knowing about, in particular when trying to understand bandwidth costs of your content:</p><p>&nbsp;</p><ul><li>All accesses to the working set are local accesses, which is both fast and low power. The power consumed reading or writing to an external DRAM will vary with system design, but it can easily be around 120mW for each 1GByte/s of bandwidth provided. Internal<br>memory accesses are approximately an order of magnitude less energy intensive than this, so you can see that this really does matter.</li><li>Blending is both fast and power-efficient, as the destination color data required for many blend equations is readily available.</li><li>A tile is sufficiently small that we can actually store enough samples locally in the tile memory to allow 4x, 8x and 16x&nbsp;<a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FMultisample_anti-aliasing" rel="external nofollow noopener noreferrer" target="_blank">multisample<br>antialising</a><sup>1</sup>. This provides high quality and very low overhead anti-aliasing. Due to the size of the working set involved (4, 8 or 16 times that of a normal single-sampled render target; a massive 1GB of working set data is needed for 16x MSAA<br>for a 4k2k display panel) few immediate mode renderers even offer MSAA as a feature to developers, because the external memory footprint and bandwidth normally make it prohibitively expensive.</li><li>Mali only has to write the color data for a single tile back to memory at the end of the tile, at which point we know its final state. We can compare the block’s color with the current data in main memory via a CRC check — a process called&nbsp;<a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fwww.arm.com%2Fproducts%2Fmultimedia%2Fmali-technologies%2Ftransaction-elimination.php" rel="external nofollow noopener noreferrer" target="_blank">Transaction<br>Elimination</a>&nbsp;— skipping the write completely if the tile contents are the same, saving SoC power. My colleague&nbsp;<a href="https://community.arm.com/people/tomolson" rel="external nofollow noopener noreferrer" target="_blank">tomolson</a>&nbsp;has<br>written a&nbsp;<a href="https://community.arm.com/groups/arm-mali-graphics/blog/2012/08/17/how-low-can-you-go-building-low-power-low-bandwidth-arm-mali-gpus" rel="external nofollow noopener noreferrer" target="_blank">great<br>blog</a>&nbsp;on this technology, complete with a real world example of Transaction Elimination (some game called Angry Birds; you might have heard of it). I’ll let Tom’s blog explain this technology in more detail, but here is a sneak peek of the technology in<br>action (only the “extra pink” tiles were written by the GPU - all of the others were successfully discarded).</li></ul><p><span lang="EN-US" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2804-7080/blogentry-107443-087661400&#43;1345199231_thumb.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2804-7080/blogentry-107443-087661400&#43;1345199231_thumb.png" alt="blogentry-107443-087661400 1345199231_thumb.png"></a></span></p><p><span lang="EN-US" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"></span></p><p></p><ul><li>We can compress the color data for the tiles which survive Transaction Elimination using a fast, lossless, compression scheme —&nbsp;<a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fwww.arm.com%2Fproducts%2Fmultimedia%2Fmali-technologies%2Farm-frame-buffer-compression.php" rel="external nofollow noopener noreferrer" target="_blank">ARM<br>Frame Buffer Compression</a>&nbsp;(AFBC) — allowing us to lower the bandwidth and power consumed even further. This compression can be applied to offscreen FBO render targets, which can be read back as textures in subsequent rendering passes by the GPU, as well<br>as the main window surface, provided there is an AFBC compatible display controller such as&nbsp;<a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fwww.arm.com%2Fproducts%2Fmultimedia%2Fmali-display%2Fmali-dp500.php" rel="external nofollow noopener noreferrer" target="_blank">Mali-DP500</a>&nbsp;in<br>the system.</li><li>Most content has a depth and stencil buffer, but doesn’t need to keep their contents once the frame rendering has finished. If developers tell the Mali drivers that depth and stencil buffers do not need to be preserved<sup>2</sup>&nbsp;— ideally via a call to&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:'courier new',courier;vertical-align:baseline">glDiscardFramebufferEXT</span>&nbsp;(OpenGL<br>ES 2.0) or<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:'courier new',courier;vertical-align:baseline">glInvalidateFramebuffer</span>&nbsp;(OpenGL ES 3.0), although it can be inferred by the drivers in some<br>cases — then the depth and stencil content of tile is never written back to main memory at all. Another big bandwidth and power saving!</li></ul><p>&nbsp;</p><p>It is clear from the list above that tile-based rendering carries a number of advantages, in particular giving very significant reductions in the bandwidth and power associated with framebuffer data, as well as being able to provide low-cost anti-aliasing.<br>What is the downside?</p><p>&nbsp;</p><p>The principal additional overhead of any tile-based rendering scheme is the point of hand-over from the vertex shader to the fragment shader. The output of the geometry processing stage, the per-vertex varyings and tiler intermediate state, must be written<br>out to main memory and then re-read by the fragment processing stage. There is therefore a balance to be struck between costing extra bandwidth for the varying data and tiler state, and saving bandwidth for the framebuffer data.</p><p>&nbsp;</p><p>In modern consumer electronics today there is a significant shift towards higher resolution displays; 1080p is now normal for smartphones, tablets such as the Mali-T604 powered Google Nexus 10 are running at WQXGA (2560x1600), and 4k2k is becoming the new “must<br>have” in the television market. Screen resolution, and hence framebuffer bandwidth, is growing fast. In this area Mali really shines, and does so in a manner which is mostly transparent to the application developer - you get all of these goodies for free with<br>no application changes!</p><p>&nbsp;</p><p>On the geometry side of things, Mali copes well with complexity. Many high-end benchmarks are approaching a million triangles a frame, which is an order of magnitude (or two) more complex than popular gaming applications on the Android app stores. However,<br>as the intermediate geometry data does hit main memory there are some useful tips and tricks which can be applied to fine tune the GPU performance, and get the best out of the system. These are worth an entire blog by themselves, so we’ll cover these at a<br>later point in this series.</p><p>&nbsp;</p><p>##<br>Summary</p><p>&nbsp;</p><p>In this blog I have compared and contrasted the desktop-style immediate mode renderer, and the tile-based approach used by Mali, looking in particular at the memory bandwidth implications of both.</p><p>&nbsp;</p><p>Tune in next time and I’ll finish off the definition of the abstract machine, looking at a simple block model of the Mali shader core itself. Once we have that out of the way we can get on with the useful part of the series: putting this model to work and earning<br>a living optimizing your applications running on Mali.</p><p>&nbsp;</p><p><span style="margin:0;padding:0;border:0;font-style:inherit;font-family:inherit;vertical-align:baseline">Note:</span>&nbsp;The next blog in this series has now been published:&nbsp;<a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/03/12/the-mali-gpu-an-abstract-machine-part-3--the-shader-core" rel="external nofollow noopener noreferrer" target="_blank">The<br>Mali GPU: An Abstract Machine, Part 3 - The Shader Core</a></p><p>&nbsp;</p><p>As always comments and questions more than welcome,</p><p>Pete</p><p>&nbsp;</p><p>##<br>Footnotes</p><p>&nbsp;</p><ol><li>Exactly which multisampling options are available depends on the GPU. The recently announced&nbsp;<a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fwww.arm.com%2Fproducts%2Fmultimedia%2Fmali-high-end-graphics%2Fmali-t760.php" rel="external nofollow noopener noreferrer" target="_blank">Mali-T760<br>GPU</a>&nbsp;includes support for up to 16x MSAA.</li><li>The depth and stencil discard is automatic for EGL window surfaces, but for offscreen render targets they may be preserved and reused in a future rendering operation.<br>3.</li></ol><h2 id="2-中文翻译"><a href="#2-中文翻译" class="headerlink" title="2. 中文翻译"></a>2. 中文翻译</h2><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">在<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;vertical-align:baseline">我</span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline;color:windowtext"><u><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5"><a href="https://community.arm.com/community/arm-cc-cn/blog/2014/06/06/mali-gpu-%E6%8A%BD%E8%B1%A1%E6%9C%BA%E5%99%A8-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E5%B8%A7%E7%AE%A1%E7%BA%BF%E5%8C%96" rel="external nofollow noopener noreferrer" target="_blank">上一篇博文</a></span></u></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">中</span></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">，我开始定义一台抽象机器，用于描述</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Mali<br>GPU<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">和驱动程序软件对应用程序可见的行为。此机器的用意是为开发人员提供</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">&nbsp;OpenGL</span></span></p><p>ES API&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">下有趣行为的一个心智模型，而这反过来也可用于解释影响其应用程序性能的问题。我在本系列后面几篇博文中继续使用这一模型，探讨开发人员在开发图形应用程序时常常遇到的一些性能缺口。</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">这篇博文将继续开发这台抽象机器，探讨</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Mali<br>GPU<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">系列基于区块的渲染模型。你应该已经阅读了关于管线化的第一篇博文；如果还没有，建议你先读一下。</span></span></p><p>##</p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:15pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">“传统”方式</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">在传统的主线驱动型桌面</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">架构中</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;—&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">通常称为直接模式架构</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;—&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">片段着色器按照顺序在每一绘制调用、每一原语上执行。每一原语渲染结束后再开始下一个，其利用类&#20284;于如下所示的算法：</span></p><p><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;&nbsp;&nbsp;<br>foreach( primitive )&nbsp;</span></p><p><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>foreach( fragment )&nbsp;</span></p><p><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>render fragment&nbsp;</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">由于流中的任何三角形可能会覆盖屏幕的任何部分，由这些渲染器维护的数据工作集将会很大；通常至少包含全屏尺寸颜色缓冲、深度缓冲，还可能包含模板缓冲。现代设备的典型工作集是</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;32&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">位</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">/</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">像素</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;(bpp)&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">颜色，以及</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;32<br>bpp&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">封装的深度</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">/</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">模板。因此，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">1080p&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">显示屏拥有一个</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;16MB&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">工作集，而</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;4k2k&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">电视机则有一个</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">64MB&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">工作集。由于其大小原因，这些工作缓冲必须存储在芯片外的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;DRAM&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">中。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"></span></p><p></p><div style="margin:0;padding:0;border:0;font-size:14px;font-family:'Gill Sans Alt One WGL W01 Lt';vertical-align:baseline;color:#565b5b;line-height:25px"><br><br><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-3305-8424/model-imr.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-3305-8424/model-imr.png" alt="model-imr.png"></a><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">每一次混合、深度测试和模板测试运算都需要从这一工作集中获取当前片段像素坐标的数据&#20540;。被着色的所有片段通常会接触到这一工作集，因此在高清显示中，置于这一内存上的带宽负载可能会特别高，每一片段也都有多个读</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">-</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">改</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">-</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">写运算，尽管缓存可能会稍稍缓减这一问题。这一对高带宽存取的需求反过来推动了对具备许多针脚的宽内存接口和专用高频率内存的需求，这两者都会造成能耗特别密集的外部内存访问。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><br><br><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:15pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#4e5584">Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:15pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">方式</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Mali GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">系列采用非常不同的方式，通常称为基于区块的的渲染，其设计宗旨是竭力减少渲染期间所需的功耗巨大的外部内存访问。如本系列</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/03/the-mali-gpu-an-abstract-machine-part-1" rel="external nofollow noopener noreferrer" target="_blank"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">第一篇博文</span></a></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">中</span></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">所述，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">对每一渲染目标使用独特的两步骤渲染算法。它首先执行全部的几何处理，然后执行所有的片段处理。在几何处理阶段中，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Mali<br>GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">将屏幕分割为微小的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;16x16&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">像素区块，并对每个区块中存在的渲染原语构建一份清单。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">片段着色步骤开始时，每一着色器核心一次处理一个</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;16x16&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">像素区块，将它渲染完后再开始下一区块。对于基于区块的架构，其算法相当于：</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;&nbsp;&nbsp;<br>foreach( tile )&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>foreach( primitive in tile )&nbsp;</span><br><br><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>foreach( fragment in primitive in tile )&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>render fragment&nbsp;&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">由于</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;16x16&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">区块仅仅是总屏幕面积的一小部分，所以有可能将整个区块的完整工作集（颜色、深度和模板）存放在和</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">着色器核心紧密耦合的快速</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;RAM</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">中。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span></div><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-3305-8425/model-tbr.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-3305-8425/model-tbr.png" alt="model-tbr.png"></a></div><br><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">这种基于区块的方式有诸多优势。它们大体上对开发人员透明，但也&#20540;得了解，尤其是在尝试了解你内容的带宽成本时：</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span></div><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">对工作集的所有访问都属于本地访问，速度快、功耗低。读取或写入外部</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;DRAM&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">的功耗因系统设计而异，但对于提供的每</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;1GB/s&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">带宽，它很容易达到大约</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">120mW</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">。与这相比，内部内存访问的功耗要大约少一个数量级，所以你会发现这真的大有关系。</span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">混合不仅速度快，而且功耗低，因为许多混合方式需要的目标颜色数据都随时可用。</span></span></div><br><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">区块足够小，我们实际上可以在区块内存中本地存储足够数量的样本，实现</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;4&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">倍、</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">8&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">倍和</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;16&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:inherit;vertical-align:baseline"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">倍</span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FMultisample_anti-aliasing" rel="external nofollow noopener noreferrer" target="_blank"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:SimSun;vertical-align:baseline;color:#0481a5">多采样抗锯齿</span></a></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline"><sup>1</sup></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">。这可提供质量高、开销很低的抗锯齿。由于涉及的工作集大小（一般单一采样渲染目标的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;4</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">、</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">8&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">或</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;16&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">倍；</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">4k2k&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">显示面板的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;16x<br>MSAA<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">需要巨大的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">&nbsp;1GB&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">工作集数据），少数直接模式渲染器甚至将</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">&nbsp;MSAA</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">作为一项功能提供给开发人员，因为外部内存大小和带宽通常导致其成本过于高昂。</span></span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">仅仅需要将单一区块的颜色数据写回到区块末尾的内存，此时我们便能知道其最终状态。我们可以通过</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;CRC&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">检查将块的颜色与主内存中的当前数据进行比较</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;—&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">这一过程叫做</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">“</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fwww.arm.com%2Fproducts%2Fmultimedia%2Fmali-technologies%2Ftransaction-elimination.php" rel="external nofollow noopener noreferrer" target="_blank"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">事务消除</span></a></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">”</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">—&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">如果区块内容相同，则可完全跳过写出，从而节省了</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;SoC&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">功耗。我的同事</span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/people/tomolson" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">Tom<br>Olson</span></a></span>&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">针对这一技术写了<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;vertical-align:baseline">一篇</span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/groups/arm-mali-graphics/blog/2012/08/17/how-low-can-you-go-building-low-power-low-bandwidth-arm-mali-gpus" rel="external nofollow noopener noreferrer" target="_blank">&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">优秀的博文</span></a></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:inherit;vertical-align:baseline"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">，</span></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">文中还提供了</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">“</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">事务消除</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">”</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">的一个现实世界示例（某个名叫</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">“</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">愤怒的小鸟</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">”</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">的游戏；你或许听说过）。有关这一技术的详细信息还是由</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;Tom&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">的博文来介绍；不过，这儿也稍稍了解一下该技术的运用（仅</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">“</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">多出的粉色</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">”</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">区块由</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">写入</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;-&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">其他全被成功丢弃）。</span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://community.arm.com/servlet/JiveServlet/showImage/38-3305-8426/blogentry-107443-087661400&#43;1345199231_thumb.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-3305-8426/blogentry-107443-087661400&#43;1345199231_thumb.png" alt="blogentry-107443-087661400&#43;1345199231_thumb.png"></a><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span></div><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">我们可以采用快速的无损压缩方案</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;—&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fwww.arm.com%2Fproducts%2Fmultimedia%2Fmali-technologies%2Farm-frame-buffer-compression.php" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">ARM&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">帧缓冲压缩</span></a></span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">(AFBC)<br>—&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">，对逃过事务消除的区块的颜色数据进行压缩，从而进一步降低带宽和功耗。这一压缩可以应用到离屏</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;FBO&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">渲染目标，后者可在随后的渲染步骤中由</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">作为纹理读回；也可以应用到主窗口表面，只要系统中存在兼容</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;AFBC&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">的显示控制器，如</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fwww.arm.com%2Fproducts%2Fmultimedia%2Fmali-display%2Fmali-dp500.php" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">Mali-DP500</span></a></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">。</span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">大多数内容拥有深度缓冲和模板缓冲，但帧渲染结束后就不必再保留其内容。如果开发人员告诉</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">驱动程序不需要保留深度缓冲和模板缓冲</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline"><sup>2</sup></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">—&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">理想方式是通过调用</span>&nbsp;&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">glDiscardFramebufferEXT<br>(OpenGL ES 2.0)&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">或</span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">glInvalidateFramebuffer<br>(OpenGLES 3.0)</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">，虽然在某些情形中可由驱动程序推断</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">&nbsp;—&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">那么区块的深度内容和模板内容也就彻底不用写回到主内存中。我们又大幅节省了带宽和功耗！</span></span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">上表中可以清晰地看出，基于区块的渲染具有诸多优势，尤其是可以大幅降低与帧缓冲数据相关的带宽和功耗，而且还能够提供低成本的抗锯齿功能。那么，有些什么劣势呢？</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">任何基于区块的渲染方案的主要额外开销是从顶点着色器到片段着色器的交接点。几何处理阶段的输出、各顶点可变数和区块中间状态必须写出到主内存，再由片段处理阶段重新读取。因此，必须要在可变数据和区块状态消耗的额外带宽与帧缓冲数据节省的带宽之间取得平衡。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">当今的现代消费类电子设备正大步向更高分辨率显示屏迈进；</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">1080p&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">现在已是智能手机的常态，配备</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000"><br><br>Mali-T604&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Google<br>Nexus 10&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">等平板电脑以</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;WQXGA<br>(2560x1600)&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">分辨率运行，而</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;4k2k&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">正逐渐成为电视机市场上新的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">“</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">不二之选</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">”</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">。屏幕分辨率以及帧缓冲带宽正快速发展。在这一方面，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">确实表现出众，而且以对应用程序开发人员基本透明的方式实现</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;-&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">无需任何代价，就能获得所有这些好处，而且还不用更改应用程序！</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">在几何处理方面，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">也能处理好复杂度。许多高端基准测试正在接近每帧百万个三角形，其复杂度比</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Android&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">应用商店中的热门游戏应用程序高出一个（或两个）数量级。然而，由于中间几何数据的确到达主内存，所以可以应用一些有用的技巧和诀窍，来优化</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">性能并充分发挥系统能力。这些技巧&#20540;得通过一篇博文来细谈，所以我们会在这一系列的后续博文中再予以介绍。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:15pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">小结</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">在这篇博文中，我比较了桌面型直接模式渲染器与</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">所用的基于区块方式的异同，尤其探讨了两种方式对内存带宽的影响。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">敬请期待下一篇博文。我将通过介绍</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">着色器核心本身的简单块模型，完成对这一抽象机器的定义。理解这部分内容后，我们就能继续介绍系列博文的其他有用部分：将这一模型应用到实践中，使其发挥实际作用，优化你在</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">上运行的应用程序。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">注意：</span>&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">本系列的下一篇博文已经发布：</span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/03/12/the-mali-gpu-an-abstract-machine-part-3--the-shader-core" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">Mali<br>GPU:&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">抽象机器，第</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">3</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">部分</span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">–&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">着色器核心</span></a></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">与往常一样，欢迎提出任何意见和问题。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span></div><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Pete</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span></div><br><div style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline"><br><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:15pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">脚注</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">具体有哪些多采样选项可用要视</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">&nbsp;GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">而定。最近推出的</span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;vertical-align:baseline"><a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fwww.arm.com%2Fproducts%2Fmultimedia%2Fmali-high-end-graphics%2Fmali-t760.php" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">Mali-T760<br>GPU</span></a></span>&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">最高支持</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">&nbsp;16&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">倍</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline">&nbsp;MSAA</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">。</span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><br><br></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">对</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;EGL&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">窗口表面而言，深度丢弃与模板丢弃是自动执行的；但对于离屏渲染对象，它们可能会予以保留，供将来的渲染运算重新利用。</span></div><br></div><br></div><p><br>本文地址 <a href="http://yjaelex.github.io/2015/12/03/转-转-The-Mali-GPU-An-Abstract-Machine-Part-2-Tile-based-Rendering/">http://yjaelex.github.io/2015/12/03/转-转-The-Mali-GPU-An-Abstract-Machine-Part-2-Tile-based-Rendering/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;第二篇，介绍了Tile-based rendering。一样有中英文对照。&lt;/p&gt;&lt;h2 id=&quot;1-英文原文&quot;&gt;&lt;a href=&quot;#1-英文原文&quot; class=&quot;headerlink&quot; title=&quot;1. 英文原文&quot;&gt;&lt;/a&gt;1. 英文原文&lt;/h2&gt;&lt;p&gt;In my&amp;nbsp;&lt;a href=&quot;https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/03/the-mali-gpu-an-abstract-machine-part-1&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;previous&lt;br&gt;blog&lt;/a&gt;&amp;nbsp;I started defining an abstract machine which can be used to describe the application-visible behaviors of the Mali GPU and driver software. The purpose of this machine is to give developers a mental model of the interesting behaviors beneath the OpenGL&lt;br&gt;ES API, which can in turn be used to explain issues which impact their application’s performance. I will use this model in the future blogs of this series to explore some common performance pot-holes which developers encounter when developing graphics applications.&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;This blog continues the development of this abstract machine, looking at the tile-based rendering model of the Mali GPU family. I’ll assume you’ve read the first blog on pipelining; if you haven’t I would suggest reading that first.&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[转][转]The Mali GPU: An Abstract Machine, Part 1 - Frame Pipelining</title>
    <link href="http://yjaelex.github.io/2015/12/03/%E8%BD%AC-%E8%BD%AC-The-Mali-GPU-An-Abstract-Machine-Part-1-Frame-Pipelining/"/>
    <id>http://yjaelex.github.io/2015/12/03/转-转-The-Mali-GPU-An-Abstract-Machine-Part-1-Frame-Pipelining/</id>
    <published>2015-12-03T01:55:35.000Z</published>
    <updated>2016-12-29T08:42:10.263Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><p>转三篇ARM一个牛人写的怎样优化OpenGL ES应用程序的文章。该系列三篇文章，深入浅出介绍了OpenGL ES API的背后实现和</p><p>ARM GPU硬件架构。其中第一篇是所有图形API都通用的概念；后面介绍了Tile-based rendering和ARM自己Shader Core。</p><p>这是第一篇，有中英文对照的。<img src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/smile.gif" alt="微笑"></p><a id="more"></a><h2 id="1-英文原文"><a href="#1-英文原文" class="headerlink" title="1. 英文原文"></a>1. 英文原文</h2><p>Optimization of graphics workloads is often essential to many modern mobile applications, as almost all rendering is now handled directly or indirectly by an OpenGL ES based rendering back-end. One of my colleagues,&nbsp;<a href="https://community.arm.com/people/mcgeagh" rel="external nofollow noopener noreferrer" target="_blank">Michael<br>McGeagh</a><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline">&nbsp;, recently posted a work guide [</span><a href="https://community.arm.com/docs/DOC-8055" rel="external nofollow noopener noreferrer" target="_blank">http://community.arm.com/docs/DOC-8055</a><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline">]<br>on getting the ARM® DS-5™ Streamline™ profiling tools working with the Google Nexus 10 for the purposes of profiling and optimizing graphical applications using the Mali™-T604 GPU. Streamline is a powerful tool giving high resolution visibility of the entire<br>system’s behavior, but it requires the engineer driving it to interpret the data, identify the problem area, and subsequently propose a fix.</span></p><p>&nbsp;</p><p>For developers who are new to graphics optimization it is fair to say that there is a little bit of a learning curve when first starting out, so this new series of blogs is all about giving content developers the essential knowledge they need to successfully<br>optimize for Mali GPUs. Over the course of the series, I will explore the fundamental macro-scale architectural structures and behaviors developers have to worry about, how this translates into possible problems which can be triggered by content, and finally<br>how to spot them in Streamline.</p><p>&nbsp;</p><p>##<br>Abstract Rendering Machine</p><p>&nbsp;</p><p>The most essential piece of knowledge which is needed to successfully analyze the graphics performance of an application is a mental model of how the system beneath the OpenGL ES API functions, enabling an engineer to reason about the behavior they observe.</p><p>&nbsp;</p><p>To avoid swamping developers in implementation details of the driver software and hardware subsystem, which they have no control over and which is therefore of limited value, it is useful to define a simplified abstract machine which can be used as the basis<br>for explanations of the behaviors observed. There are three useful parts to this machine, and they are mostly orthogonal so I will cover each in turn over the first few blogs in this series, but just so you know what to look forward to the three parts of the<br>model are:</p><p>&nbsp;</p><ul><li>The CPU-GPU rendering pipeline</li><li>Tile-based rendering</li><li>Shader core architecture</li></ul><p>&nbsp;</p><p>In this blog we will look at the first of these, the CPU-GPU rendering pipeline.</p><p>&nbsp;</p><p>##<br>Synchronous API, Asynchronous Execution</p><p>&nbsp;</p><p>The most fundamental piece of knowledge which is important to understand is the temporal relationship between the application’s function calls at the OpenGL ES API and the execution of the rendering operations those API calls require. The OpenGL ES API is specified<br>as a synchronous API from the application perspective. The application makes a series of function calls to set up the state needed by its next drawing task, and then calls a&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:'courier new',courier;vertical-align:baseline">glDraw</span><sup><small style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:.92em;font-family:inherit;vertical-align:baseline">[1]</small></sup>&nbsp;function<br>— commonly called a draw call — to trigger the actual drawing operation. As the API is synchronous all subsequent API behavior after the draw call has been made is specified to behave as if that rendering operation has already happened, but on nearly all hardware-accelerated<br>OpenGL ES implementations this is an elaborate illusion maintained by the driver stack.</p><p>&nbsp;</p><p>In a similar fashion to the draw calls, the second illusion that is maintained by the driver is the end-of-frame buffer flip. Most developers first writing an OpenGL ES application will tell you that calling&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:'courier new',courier;vertical-align:baseline">eglSwapBuffers</span>&nbsp;swaps<br>the front and back-buffer for their application. While this is logically true, the driver again maintains the illusion of synchronicity; on nearly all platforms the physical buffer swap may happen a long time later.</p><p>&nbsp;</p><p>###<br>Pipelining</p><p>&nbsp;</p><p>The reason for needing to create this illusion at all is, as you might expect, performance. If we forced the rendering operations to actually happen synchronously you would end up with the GPU idle when the CPU was busy creating the state for the next draw<br>operation, and the CPU idle while the GPU was rendering. For a performance critical accelerator all of this idle time is obviously not an acceptable state of affairs.</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2755-6948/gles-sync.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2755-6948/gles-sync.png" alt="gles-sync.png"></a></p><p>&nbsp;</p><p>To remove this idle time we use the OpenGL ES driver to maintain the illusion of synchronous rendering behavior, while actually processing rendering and frame swaps asynchronously under the hood. By running asynchronously we can build a small backlog of work,<br>allowing a pipeline to be created where the GPU is processing older workloads from one end of the pipeline, while the CPU is busy pushing new work into the other. The advantage of this approach is that, provided we keep the pipeline full, there is always work<br>available to run on the GPU giving the best performance.</p><p>&nbsp;</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2755-6949/gles-async.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2755-6949/gles-async.png" alt="gles-async.png"></a></p><p>&nbsp;</p><p>The units of work in the Mali GPU pipeline are scheduled on a per render-target basis, where a render target may be a window surface or an off-screen render buffer. A single render target is processed in a two step process. First, the GPU processes the vertex<br>shading<sup><small style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:.92em;font-family:inherit;vertical-align:baseline">[2]</small></sup>&nbsp;for all draw calls in the render target, and second, the fragment shading<sup><small style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:.92em;font-family:inherit;vertical-align:baseline">[3]</small></sup>&nbsp;for<br>the entire render target is processed. The logical rendering pipeline for Mali is therefore a three-stage pipeline of: CPU processing, geometry processing, and fragment processing stages.</p><p>&nbsp;</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2755-6950/gles-mali.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2755-6950/gles-mali.png" alt="gles-mali.png"></a></p><p>&nbsp;</p><p>###<br>Pipeline Throttling</p><p>&nbsp;</p><p>An observant reader may have noticed that the fragment work in the figure above is the slowest of the three operations, lagging further and further behind the CPU and geometry processing stages. This situation is not uncommon; most content will have far more<br>fragments to shade than vertices, so fragment shading is usually the dominant processing operation.</p><p>&nbsp;</p><p>In reality it is desirable to minimize the amount of latency from the CPU work completing to the frame being rendered – nothing is more frustrating to an end user than interacting with a touch screen device where their touch event input and the data on-screen<br>are out of sync by a few 100 milliseconds – so we don’t want the backlog of work waiting for the fragment processing stage to grow too large. In short we need some mechanism to slow down the CPU thread periodically, stopping it queuing up work when the pipeline<br>is already full-enough to keep the performance up.</p><p>&nbsp;</p><p>This throttling mechanism is normally provided by the host windowing system, rather than by the graphics driver itself. On Android for example we cannot process any draw operations in a frame until we know the buffer orientation, because the user may have rotated<br>their device, changing the frame size. SurfaceFlinger — the Android window surface manager – can control the pipeline depth simply by refusing to return a buffer to an application’s graphics stack if it already has more than N buffers queued for rendering.</p><p>&nbsp;</p><p>If this situation occurs you would expect to see the CPU going idle once per frame as soon as “N” is reached, blocking inside an EGL or OpenGL ES API function until the display consumes a pending buffer, freeing up one for new rendering operations.</p><p>&nbsp;</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2755-6951/gles-mali-throttle.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2755-6951/gles-mali-throttle.png" alt="gles-mali-throttle.png"></a></p><p>&nbsp;</p><p>This same scheme also limits the pipeline buffering if the graphics stack is running faster than the display refresh rate; in this scenario content is &quot;vsync limited&quot; waiting for the vertical blank (vsync) signal which tells the display controller it can switch<br>to the next front-buffer. If the GPU is producing frames faster than the display can show them then SurfaceFlinger will accumulate a number of buffers which have completed rendering but which still need showing on the screen; even though these buffers are<br>no longer part of the Mali pipeline, they count towards the N frame limit for the application process.</p><p>&nbsp;</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-2755-6952/gles-mali-vsync.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2755-6952/gles-mali-vsync.png" alt="gles-mali-vsync.png"></a></p><p>As you can see in the pipeline diagram above, if content is vsync limited it is common to have periods where both the CPU and GPU are totally idle. Platform dynamic voltage and frequency scaling (DVFS) will typically try to reduce the current operating frequency<br>in these scenarios, allowing reduced voltage and energy consumption, but as DVFS frequency choices are often relatively coarse some amount of idle time is to be expected.</p><p>&nbsp;</p><p>###<br>Summary</p><p>&nbsp;</p><p>In this blog we have looked at synchronous illusion provided by the OpenGL ES API, and the reasons for actually running an asynchronous rendering pipeline beneath the API. Tune in&nbsp;<a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/20/the-mali-gpu-an-abstract-machine-part-2" rel="external nofollow noopener noreferrer" target="_blank">next<br>time</a>, and I’ll continue to develop the abstract machine further, looking at the Mali GPU’s tile-based rendering approach.</p><p>&nbsp;</p><p>Comments and questions welcomed,</p><p>Pete</p><p>&nbsp;</p><p>###<br>Footnotes</p><p>&nbsp;</p><ul><li>[1] There are many OpenGL ES draw functions which draw things, it doesn’t really matter which one you pick for this example.</li><li>[2]&nbsp;<a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FVertex_%28computer_graphics%29" rel="external nofollow noopener noreferrer" target="_blank">Vertex<br>(computer graphics) - Wikipedia, the free encyclopedia</a></li><li>[3]&nbsp;<a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFragment_%28computer_graphics%29" rel="external nofollow noopener noreferrer" target="_blank">Fragment<br>(computer graphics) - Wikipedia, the free encyclopedia</a></li></ul><h2 id="2-中文翻译"><a href="#2-中文翻译" class="headerlink" title="2. 中文翻译"></a>2. 中文翻译</h2><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">图形工作负载的优化对于许多现代移动应用程序而言往往必不可少，因为几乎所有渲染现在都直接或间接地由基于</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;OpenGL</span></p><p>ES&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">的渲染后端负责处理。我的同事</span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/people/mcgeagh" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">Michael<br>McGeagh</span></a></span>&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">最近发表了一篇工作指南</span>&nbsp;<span style="margin:0;padding:0;border:1pt windowtext;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">[</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/docs/DOC-8055" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">http://community.arm.com/docs/DOC-8055</span></a></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">]</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">，介绍如何将</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;ARM®DS-5™<br>Streamline™&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">性能分析工具用于</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Google<br>Nexus 10</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">，对利用</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Mali™-T604<br>GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">的图形应用程序进行性能分析和优化。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Streamline&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">是一款强大的工具，能够深入细致地洞悉整个系统的行为，但也需要驾驭它的工程师能够解读相关数据，识别问题区域，进而提出修复建议。</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">对于初涉图形优化的开发人员而言，起步阶段总会遇到一些困难，所以我写了新的系列博文，给开发人员提供必要的知识，以便他们能够成功地针对</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Mali<br>GPU<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">进行优化。在整个系列博文中，我将阐述开发人员必须要考虑的基本宏观体系结构和行为、这些因素如何转化为能被内容触发的潜在问题，以及最终如何在</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;vertical-align:baseline"></span></span></p><p>Streamline&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">中找出这些问题。</span></p><p>&nbsp;</p><p>##<br><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:15pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">抽象渲染机器</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">要想成功分析应用程序的图形性能，必须先掌握一个最基本的知识，也就是对</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;OpenGL<br>ES API&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">底下系统运作方式建立一个心智模型，让工程师能够推断他们观察到的行为。</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">为避免让开发人员陷于驱动程序软件和硬件子系统的实施细节的沼泽之中（这些他们无法控制，因而价&#20540;有限），有必要定义一个简化的抽象机器，用作解读所观察到的行为的基础。这一机器包含三个有用部分，它们大体上是独立不相干的，所以我将在本系列博文的开头几篇中逐一介绍。不过，为了让你对它们有个初步印象，下面列出该模型的三个部分：</span></p><ul><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">CPU-GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">渲染管线</span></span></li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">基于区块的渲染</span></span></li><li><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline"></span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">着色器核心架构</span></span></li></ul><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">在本篇博文中，我们将探讨第一个部分，即</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;CPU-GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">渲染管线。</span></p><p>&nbsp;</p><p>##<br><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:15pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">同步</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:15pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#4e5584">API</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:15pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">，异步执行</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">务必要了解的一个基本知识是，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">OpenGL<br>ES API&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">上应用程序函数调用和这些</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;API&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">调用所需渲染运算的执行之间的临时关系。从应用程序的角度而言，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">OpenGL<br>ES API&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">被指定为同步</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;API</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">。应用程序进行一系列的函数调用来设置其下一绘制任务所需的状态，然后调用</span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">glDraw</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:9pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">[1]</span>&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">函数（通常称为绘制调用）触发实际的绘制运算。由于</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;API&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">是同步的，执行绘制调用后的所有</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;API&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">行为都被指定为要像渲染运算已经发生一样进行，但在几乎所有硬件加速的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;OpenGL<br>ES&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">实现上，这只是一种由驱动程序堆栈维持的美妙假象。</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">与绘制调用相&#20284;，驱动程序维持的第二个假象是帧末缓冲翻转。大多数头一次编写</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;OpenGL<br>ES&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">应用程序的开发人员会告诉你，调用</span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">eglSwapBuffers<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">将交换其应用程序的前缓冲和后缓冲。虽然这在逻辑上是对的，但驱动程序再一次维持了同步性的假象；在几乎所有平台上，实际的缓冲交换可能会在很久之后才会发生。</span></span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:12pt;font-family:SimSun;vertical-align:baseline;color:#4e5584"></span>&nbsp;</p><p>###<br><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:12pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">管线化</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">正如你所想到的，需要创造这一假象的原因在于性能。如果我们强制渲染运算真正同步发生，你就会面临这样的尴尬：</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">CPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">忙于创建下一绘制运算的状态时，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">会闲置；</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">执行渲染时，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">CPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">会闲置。对于以性能为重的加速器而言，所有这些闲置时间都是绝然不可接受的。</span></p><div style="margin:0;padding:0;border:0;font-size:14px;font-family:'Gill Sans Alt One WGL W01 Lt';vertical-align:baseline;color:#565b5b;line-height:25px;text-align:justify"><br><span style="margin:0;padding:0;border:1pt windowtext;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-3304-8413/gles-sync.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-3304-8413/gles-sync.png" alt="gles-sync.png"></a><a href="https://community.arm.com/servlet/JiveServlet/downloadImage/38-2755-6948/gles-sync.png" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5"><br><br></span></a></span></div><br><div style="margin:0;padding:0;border:0;font-size:14px;font-family:'Gill Sans Alt One WGL W01 Lt';vertical-align:baseline;color:#565b5b;line-height:25px;text-align:justify"><br><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">为了去除这一闲置时间，我们使用</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;OpenGL<br>ES&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">驱动程序来维持同步渲染行为的假象，而在面纱之后实际是以异步执行的方式处理渲染和帧交换。通过异步运行，我们可以建立一个小小的工作储备以允许创建一个管线，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">从管线的一端处理较旧的工作负载，而</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;CPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">则负责将新的工作推入另一端。这一方式的优势在于，只要管线装满，就始终有工作在</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">上运行，提供最佳的性能。</span></div><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-3304-8420/gles-async.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-3304-8420/gles-async.png" alt="gles-async.png"></a></p><p>&nbsp;</p><div style="margin:0;padding:0;border:0;font-size:14px;font-family:'Gill Sans Alt One WGL W01 Lt';vertical-align:baseline;color:#565b5b;line-height:25px;text-align:justify"><br><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Mali GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">管线中的工作单元是以渲染目标为单位进行计划的，其中渲染目标可能是屏幕缓存或离屏缓存。单个渲染目标通过两步处理。首先，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">为渲染目标中的所有绘制调用处理顶点着色</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">[2]</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:SimSun;vertical-align:baseline">。</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">然后，为整个渲染目标处理片段着色</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">[3]</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:SimSun;vertical-align:baseline">。</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">因此，</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">的逻辑渲染管线包含三个阶段：</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">CPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">处理阶段、几何处理阶段，以及片段处理阶段。</span></div><p>&nbsp;</p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-3304-8421/gles-mali.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-3304-8421/gles-mali.png" alt="gles-mali.png"></a></p><p>&nbsp;</p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:12pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">管线节流</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">观察力敏锐的读者可能已注意到，上图中片段部分的工作是三个运算中最慢的，被</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;CPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">和几何处理阶段甩得越来越远。这种情形并不少见；大多数内容中要着色的片段远多于顶点，因此片段着色通常是占主导地位的处理运算。</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">在现实中，最好要尽可能缩短从</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;CPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">工作结束到帧被渲染之间的延时</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;–&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">对最终用户而言，最让人烦躁的莫过于在操作触控屏设备时，其触控事件输入和屏幕中数据显示之间出现数百毫秒的不同步</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;–&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">所以，我们不希望等待片段处理阶段的工作储备变得过大。简而言之，我们需要某种机制来定期减慢</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;CPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">线程，当管线足够满、能够维持良好性能时停止把工作放入队列。</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">这种节流机制通常由主机窗口系统提供，而不是图形驱动程序本身。例如，在</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Android&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">上，我们只有在知道缓冲方向时才能处理任何绘制运算，因为用户可能会旋转其设备，造成帧大小出现变化。</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">SurfaceFlinger—<br>Android&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">窗口表面管理器</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;–&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">可以通过一个简单方式控制管线深度：当管线中排队等待渲染的缓冲数量超过</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;N&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">个时，拒绝将缓冲返回到应用程序的图形堆栈。</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">如果出现这种情形，你就会看到：一旦每一帧达到</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">“N”</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">时</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;CPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">就会进入闲置状态，在内部阻止</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;EGL&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">或</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;OpenGL</span></p><p>ES API&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">函数，直到显示屏消耗完一个待处理缓存，为新的渲染运算空出一个位置。</span></p><p>&nbsp;</p><p>&nbsp;&nbsp;&nbsp;<a href="https://community.arm.com/servlet/JiveServlet/showImage/38-3304-8422/gles-mali-throttle.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-3304-8422/gles-mali-throttle.png" alt="gles-mali-throttle.png"></a></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">如果图形堆栈的运行快于显示刷新率，同样的方案也可限制管线缓冲；在这一情形下，内容受到</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">VSYNC</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">限制</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">”</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">并等待垂直空白（</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">VSYNC</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">同步）信号，该信号告诉显示控制器它可以切换到下一缓冲。如果</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">产生帧的速度快于显示屏显示帧的速度，那么</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000"></span></p><p>SurfaceFlinger&nbsp;<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">将积累一定数量已经完成渲染但依然需要显示在屏幕上的缓冲；即使这些缓冲不再是</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Mali&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">管线的一个部分，它们依然算在应用程序进程的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;N&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">帧限制内。</span></p><p><a href="https://community.arm.com/servlet/JiveServlet/showImage/38-3304-8423/gles-mali-vsync.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://community.arm.com/servlet/JiveServlet/downloadImage/38-3304-8423/gles-mali-vsync.png" alt="gles-mali-vsync.png"></a></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">正如上面的管线示意图所示，如果内容受到</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">VSYNC</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">同步限制，那么会经常出现</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;CPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">和</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;GPU&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">都完全闲置的时段。平台动态电压和频率调节</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;(DVFS)&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">通常会在此类情形中尝试降低当前的工作频率，以降低电压和功耗，但由于</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;DVFS&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">频率选择通常相对粗糙，所以可能会出现一定数量的闲置时间。</span></p><p>&nbsp;</p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:inherit;vertical-align:baseline">小结</span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">本篇博文中，我们探讨了</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;OpenGL<br>ES API&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">提供的同步假象，以及</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;API&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">下实际运行异步渲染管线的原因。敬请<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;vertical-align:baseline">期待</span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-family:Arial,sans-serif;vertical-align:baseline;color:#000"><a href="https://community.arm.com/groups/arm-mali-graphics/blog/2014/02/20/the-mali-gpu-an-abstract-machine-part-2" rel="external nofollow noopener noreferrer" target="_blank"><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">下一篇</span></a></span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">博文，我将继续往下开发这一台抽象机器，探讨</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">&nbsp;Mali<br>GPU<span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">基于区块的渲染做法。</span></span></p><p><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#000">欢迎大家提出意见和问题。</span></p><p><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">Pete</span></p><p>&nbsp;</p><p>&nbsp;</p><div style="margin:0;padding:0;border:0;font-size:14px;font-family:'Gill Sans Alt One WGL W01 Lt';vertical-align:baseline;color:#565b5b;line-height:25px;text-align:justify"><br><br><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:12pt;font-family:SimSun;vertical-align:baseline;color:#4e5584">脚注</span></div><p><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline;color:#000"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">[1]&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">执行绘制的</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline">&nbsp;OpenGL<br>ES&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline">绘制函数有许多，本例中选用哪一个其实没什么关系。</span></span></p><p><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Arial,sans-serif;vertical-align:baseline;color:#000">[2]&nbsp;</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FVertex_%28computer_graphics%29" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">Vertex<br>(computer graphics) –&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">（顶点（计算机图形））</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">&nbsp;-&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">维基百科，自由的百科全书</span>&nbsp;&nbsp;</a></span></span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline"><span class="pasted-list-info" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:inherit;vertical-align:baseline"></span></span></p><p><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:Symbol;vertical-align:baseline"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:Arial,sans-serif;vertical-align:baseline">[3]</span>&nbsp;<span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:13.33px;font-family:Arial,sans-serif;vertical-align:baseline"><a href="https://community.arm.com/external-link.jspa?url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FFragment_%28computer_graphics%29" rel="external nofollow noopener noreferrer" target="_blank"><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">Fragment<br>(computer graphics) -&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">（片段（计算机图形））</span><span style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:inherit;vertical-align:baseline;color:#0481a5">&nbsp;-&nbsp;</span><span lang="ZH-CN" style="margin:0;padding:0;border:0;font-weight:inherit;font-style:inherit;font-size:10pt;font-family:SimSun;vertical-align:baseline;color:#0481a5">维基百科，自由的百科全书</span>&nbsp;&nbsp;</a></span></span></p><p><br>本文地址 <a href="http://yjaelex.github.io/2015/12/03/转-转-The-Mali-GPU-An-Abstract-Machine-Part-1-Frame-Pipelining/">http://yjaelex.github.io/2015/12/03/转-转-The-Mali-GPU-An-Abstract-Machine-Part-1-Frame-Pipelining/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;转三篇ARM一个牛人写的怎样优化OpenGL ES应用程序的文章。该系列三篇文章，深入浅出介绍了OpenGL ES API的背后实现和&lt;/p&gt;&lt;p&gt;ARM GPU硬件架构。其中第一篇是所有图形API都通用的概念；后面介绍了Tile-based rendering和ARM自己Shader Core。&lt;/p&gt;&lt;p&gt;这是第一篇，有中英文对照的。&lt;img src=&quot;http://static.blog.csdn.net/xheditor/xheditor_emot/default/smile.gif&quot; alt=&quot;微笑&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[转][转] 8 reasons why SPIR-V makes a big difference</title>
    <link href="http://yjaelex.github.io/2015/11/08/%E8%BD%AC-%E8%BD%AC-8-reasons-why-SPIR-V-makes-a-big-difference/"/>
    <id>http://yjaelex.github.io/2015/11/08/转-转-8-reasons-why-SPIR-V-makes-a-big-difference/</id>
    <published>2015-11-08T08:56:07.000Z</published>
    <updated>2016-12-29T08:38:29.854Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><p>转另一篇SPIR-V的文章。</p><p><span style="font-size:24px">8 reasons why SPIR-V makes a big difference</span></p><p>From all the news that came out of GDC, I’m most eager to talk about SPIR-V. This intermediate language<a href="http://i0.wp.com/streamcomputing.eu/wp-content/uploads/2015/03/spir-v.jpeg" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://i0.wp.com/streamcomputing.eu/wp-content/uploads/2015/03/spir-v.jpeg?resize=300%2C148" alt="spir-v"></a>will<br>make a big difference for the compute-industry. In this article I’d like to explain why. If you need a technical explanation of what SPIR-V is, I suggest you first read&nbsp;<a href="http://www.g-truc.net/post-0714.html" rel="external nofollow noopener noreferrer" target="_blank">gtruc’s<br>article on SPIR-V</a>&nbsp;and then return here to get an overview of the advantages.</p><a id="more"></a><p>Currently there are several shader and c ompute languages, which SPIR-V tries to replace/support. We have GLSL, HLSL for graphics shaders, SPIR (without the V), OpenCL, CUDA and many others for compute shaders.</p><p>If you have questions after reading this article, feel free to ask them in a comment or to us&nbsp;<a href="http://streamcomputing.eu/about-us/contact/" rel="external nofollow noopener noreferrer" target="_blank">directly</a>.<span id="more-9206" style="margin:0;padding:0;border:0;vertical-align:baseline"></span></p><p>#<br>1. It’s used by Vulkan</p><p>Yes, this is the number one reason. This will make SPIR-V a big standard for compute, probably bigger than OpenCL (in its current form). That’s a reason we will go big on this here at StreamComputing.</p><p>Vulkan is the stateless version of OpenGL – not even fully released, but now already the future de-facto cross-platform standard for modern graphics. Seems support for it is very complete. As it uses SPIR-V as shader language, this will greatly push SPIR-V.<br>You have to understand, this is quite aggressive pushing. It unfortunately is needed with all those big corporations having their me-too languages everywhere. Once SPIR-V is everywhere, all languages that compile to SPIR-V are also supported – that probably<br>will include HLSL, GLSL and various OpenCL languages.</p><p>#<br>2.It supports OpenCL C, C&#43;&#43; and more</p><p>In theory also OpenGL could accept SPIR-V next to GLSL on modern graphics cards – this would avoid the need to have GLSL and SPIR-V next to each other. Same for DirectX and HLSL. There is always demand for writing one single code-base and have support for many<br>platforms, so expect many languages and tools that can export to and import from SPIR-V.</p><div id="attachment_9230" class="wp-caption alignnone" style="margin:5px 20px 20px 0;padding:5px 3px 10px;border:1px solid #f0f0f0;vertical-align:baseline;max-width:96%;text-align:center;color:#555;font-family:Merriweather,Arial,Helvetica,sans-serif;line-height:21px;width:560px"><br><a href="http://i0.wp.com/streamcomputing.eu/wp-content/uploads/2015/03/spir-v_in_out.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://i0.wp.com/streamcomputing.eu/wp-content/uploads/2015/03/spir-v_in_out.png?resize=550%2C368" alt="spir-v_in_out"></a><br><br>Lots of input possibilities from GLSL and OpenCL to complete new languages. Currently only two main driver targets. “Other languages” where SPIR-V can export to, includes LLVM.<br><br></div><p>Even CUDA-kernels or HLSL could get compiled to SPIR. This means that porting can even go faster, if the host-code isn’t too complex.</p><blockquote><p>SPIR-V is the base, not for only&nbsp;<span class="share-body" style="margin:0;padding:0;border:0;vertical-align:baseline">OpenCL-C, but also OpenCL-C&#43;&#43;, OpenCL-Python, OpenCL-C#, OpenCL-Julia,&nbsp;OpenCL-Java and many more. Creating a single source language</span></p><p>is a simple next step, by making splitting a part of the compile-phase or even using JIT.</p></blockquote><p>See what is already&nbsp;<a href="https://github.com/search?q=%22spir-v%22" rel="external nofollow noopener noreferrer" target="_blank">coming<br>around on Github</a>. I already see Python, dotNET/Linq and GLSL! No HLSL or CUDA yet –&nbsp;<a href="http://streamcomputing.eu/about-us/contact/" rel="external nofollow noopener noreferrer" target="_blank">ask<br>us</a>&nbsp;for more info…</p><p>#<br>3. It’s better than the LLVM-based SPIR</p><p>Flexibility is key. LLVM has it’s own agenda, which seemed to be too different from what Khronos wanted with SPIR. A few years I wrote “<a href="http://streamcomputing.eu/blog/2013-12-27/opencl-spir-by-example/" rel="external nofollow noopener noreferrer" target="_blank">SPIR<br>by example</a>” where you already see the limits of LLVM for the purposes of SPIR – important data was put in comments. SPIR-V is SPIR without limits.</p><p><a href="http://i2.wp.com/streamcomputing.eu/wp-content/uploads/2015/05/2015-spir-v-page6.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://i2.wp.com/streamcomputing.eu/wp-content/uploads/2015/05/2015-spir-v-page6.png?resize=550%2C244" alt="2015-spir-v-page6"></a></p><p>#<br>4. Tools can do (static) analysis on SPIR-code</p><p>Many, many languages a problem? Not at all – tools just have to focus on SPIR, and the frontend-compilers need to make sure the mapping to the original source is fluent. This makes it possible to have tool-support for new kernel-languages for free.</p><p>#<br>5. It integrates very well with the existing LLVM too-chain</p><p>SPIR-V is a clean language to have as a frontend-language for LLVM. SPIR-V is expected to get an official LLVM frontend soon. For now there is&nbsp;<a href="https://www.phoronix.com/scan.php?page=news_item&amp;px=LunarGLASS-SPIR-V" rel="external nofollow noopener noreferrer" target="_blank">LunaGLASS’s<br>project</a>.</p><p>#<br>6. It is the same on mobile processors</p><p>As Vulkan has the same API for embedded processors, SPIR-V is also the same for mobile and desktop. There is only the checking of the GPU-capabilities, as we know from OpenCL (memories, compute capabilities), which is simpler to code.</p><p>#<br>7. It is handled by the OpenCL Runtime</p><p><a href="http://i0.wp.com/streamcomputing.eu/wp-content/uploads/2015/05/khronos-SPIR-V-flowchart.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://i0.wp.com/streamcomputing.eu/wp-content/uploads/2015/05/khronos-SPIR-V-flowchart.png?resize=550%2C323" alt="khronos-SPIR-V-flowchart"></a></p><p>There are two options for handling SPIR-V. First is using an adapted version of the GLSL-compiler, but that won’t be able to use the full spectrum of V’s capabilities without a lot of fixing. The better solution is to adapt the OpenCL-compiler built for that<br>platform – and that’s what Khronos also had in mind. Above is an image of the flow – as you see OpenCL 2.1 is the minimum version needed.</p><p><a href="http://i2.wp.com/streamcomputing.eu/wp-content/uploads/2015/05/2015-spir-v-page8.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://i2.wp.com/streamcomputing.eu/wp-content/uploads/2015/05/2015-spir-v-page8.png?resize=550%2C99" alt="2015-spir-v-page8"></a></p><p>#<br>8. Better IP protection</p><p>Where OpenCL kernels could “accidentally” be read, when hidden in the code, SPIR-V gives better legal protection. SPIR-V has to be decoded and falls under the same laws that protect decoding of JAVA or dotNET code.</p><p><br>本文地址 <a href="http://yjaelex.github.io/2015/11/08/转-转-8-reasons-why-SPIR-V-makes-a-big-difference/">http://yjaelex.github.io/2015/11/08/转-转-8-reasons-why-SPIR-V-makes-a-big-difference/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;转另一篇SPIR-V的文章。&lt;/p&gt;&lt;p&gt;&lt;span style=&quot;font-size:24px&quot;&gt;8 reasons why SPIR-V makes a big difference&lt;/span&gt;&lt;/p&gt;&lt;p&gt;From all the news that came out of GDC, I’m most eager to talk about SPIR-V. This intermediate language&lt;a href=&quot;http://i0.wp.com/streamcomputing.eu/wp-content/uploads/2015/03/spir-v.jpeg&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;http://i0.wp.com/streamcomputing.eu/wp-content/uploads/2015/03/spir-v.jpeg?resize=300%2C148&quot; alt=&quot;spir-v&quot;&gt;&lt;/a&gt;will&lt;br&gt;make a big difference for the compute-industry. In this article I’d like to explain why. If you need a technical explanation of what SPIR-V is, I suggest you first read&amp;nbsp;&lt;a href=&quot;http://www.g-truc.net/post-0714.html&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;gtruc’s&lt;br&gt;article on SPIR-V&lt;/a&gt;&amp;nbsp;and then return here to get an overview of the advantages.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[转][转] Redefining the shading languages ecosystem with SPIR-V</title>
    <link href="http://yjaelex.github.io/2015/11/08/%E8%BD%AC-%E8%BD%AC-Redefining-the-shading-languages-ecosystem-with-SPIR-V/"/>
    <id>http://yjaelex.github.io/2015/11/08/转-转-Redefining-the-shading-languages-ecosystem-with-SPIR-V/</id>
    <published>2015-11-08T08:33:01.000Z</published>
    <updated>2016-12-29T08:42:30.130Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><p>SPIR-V，全称<strong><span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px">Standard Portable Intermediate Representation</span><span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px">&nbsp;(</span><span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px">SPIR</span></strong><span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px"><strong>)</strong>&nbsp;是一种用在GPU通用计算和图形学上的中间语言（</span><a href="https://en.wikipedia.org/wiki/Intermediate_language" title="Intermediate language" rel="external nofollow noopener noreferrer" target="_blank">intermediate<br>language</a>，类&#20284;汇编）；由<a href="https://en.wikipedia.org/wiki/Khronos_Group" title="Khronos Group" rel="external nofollow noopener noreferrer" target="_blank">Khronos</a>开发<span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px">,<br>最初是为</span><a href="https://en.wikipedia.org/wiki/OpenCL" title="OpenCL" rel="external nofollow noopener noreferrer" target="_blank">OpenCL</a>准备的。<span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px">&nbsp;目前的版本是<br>SPIR-V，和下一代图形标准Vulkan差不多同时提出。前面版本的SPIR其实基于LLVM IR；而最新的<span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px">SPIR-V则是重新定义了一套；当然还是和LLVM IR有些类&#20284;。</span></span></p><a id="more"></a><p><span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px">SPIR的目的是为Shader或OpenCL kernel，提供一个平台设备无关的类汇编语言的标准。这样应用程序发布时可以使用二进制的类汇编&#26684;式Shader Binary；而不是像以前OpenGL那样直接在线编译GLSL。以前的方式需要提供Shader的源代码，这不安全。Vulkan以后会大量的走类&#20284;D3D的方式，只提供一个Shader二进制&#26684;式。</span></p><p><span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px"></span></p><p></p><p><span style="color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px"></span></p><p>At Siggraph 2014, the Khronos Group announced the development of&nbsp;<a href="https://www.khronos.org/assets/uploads/developers/library/2014-siggraph-bof/OpenGL-Ecosystem-BOF_Aug14.pdf" rel="external nofollow noopener noreferrer" target="_blank">GLnext and a shader<br>IL</a>. They have a name now:&nbsp;<a href="https://www.khronos.org/vulkan" rel="external nofollow noopener noreferrer" target="_blank">Vulkan</a>&nbsp;and&nbsp;<a href="https://www.khronos.org/spir" rel="external nofollow noopener noreferrer" target="_blank">SPIR V</a>.</p><p>First, let’s clear out something: SPIR-V has nothing to do with SPIR. It’s built from scratch with marketing thinking it was a good idea to name two products with different version values. Ohhh, I can’t wait to battle against the confusions this idea will lead<br>to. SPIR-V is not tied to LLVM, it’s a fully specified and self-contained specification. It can represent both graphics code and compute code for any API, including Vulcan, OpenCL, OpenGL, OpenGL ES, WebGL, etc.</p><p>If we trust the industry is interested in solving actual developers’ problems, it will eventually be used outside the Khronos Group, for Direct3D, Metal, consoles and beyond!</p><p>The Khronos Group has published some great documents for SPIR-V:&nbsp;<a href="https://www.khronos.org/registry/spir-v/papers/WhitePaper.pdf" rel="external nofollow noopener noreferrer" target="_blank">a white paper</a>,&nbsp;<a href="https://www.khronos.org/registry/spir-v/specs/1.0/SPIRV.pdf" rel="external nofollow noopener noreferrer" target="_blank">a<br>provisial portable core specification</a>,&nbsp;<a href="https://www.khronos.org/registry/spir-v/specs/1.0/GLSL.std.450.pdf" rel="external nofollow noopener noreferrer" target="_blank">an provisial extended instructions for graphics specification</a>,&nbsp;<a href="https://www.khronos.org/registry/spir-v/specs/1.0/OpenCL.std.21.pdf" rel="external nofollow noopener noreferrer" target="_blank">an<br>provisial extended instructions for compute specification</a>. A work in progress&nbsp;<a href="https://www.khronos.org/opengles/sdk/tools/Reference-Compiler/" rel="external nofollow noopener noreferrer" target="_blank">GLSL to SPIR-V compiler</a>&nbsp;is also available.</p><div class="list" style="padding:8px 0;font-family:verdana;font-size:14px"><span class="list" style="margin:0">SPIR-V in a nutshell:</span><br><br><em>Better portability </em>Better runtime performance<br><em>Source languages independent </em>Fully specified<br><em>Simple binary </em>Extendable<br></div><p>####<br>A current shader cross-compilation pipeline</p><p>Engines will typically use a meta-HLSL or meta shading languages for their shader code. Historically, HLSL on desktop and consoles but GLSL on mobile and CAD/DCC/professional stuff because of markets differences and reality. An engine that wants to address<br>multiple markets is quickly confronted to the requirement of cross-compiling shaders from HLSL to GLSL and/or GLSL to HLSL for example with game engines. Alternatively, in the off-line rendering ecosystem, we could also imagine the issue happenning between&nbsp;<a href="http://www.openshading.com/" rel="external nofollow noopener noreferrer" target="_blank">Open<br>Shading Language</a>&nbsp;and<a href="https://renderman.pixar.com/resources/current/RenderMan/shadingLanguage.html" rel="external nofollow noopener noreferrer" target="_blank">RenderMan</a>.</p><p>To match the reality of the ecosystem, we have to cross compile the shading languages but this is always&nbsp;<a href="http://aras-p.info/blog/2014/03/28/cross-platform-shaders-in-2014/" rel="external nofollow noopener noreferrer" target="_blank">a painful process</a>&nbsp;as<br>illustrated in figure 1.</p><p><img src="http://img.blog.csdn.net/20151108173215688?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p><div class="post-image-white" style="width:816px;padding-top:0;padding-bottom:0;text-align:center;margin-left:auto;margin-right:auto;font-family:verdana;font-size:14px"><br><br></div><br><div class="post-image-title" style="font-style:italic;text-align:center;padding-bottom:32px;font-family:verdana;font-size:14px"><br>Figure 1: Shader compilation pipeline in Unity 5&#43;</div><p>Is this crazy and insane? Yes. However, it addresses the reality of the market which is complex and fragmented with a lot of legacy hardware and software. If there are reasons to use an engine, this is one. This pipeline is particularly complex but still GLSL<br>remains a second class shading language in the sense that this pipeline doesn’t allow to run GLSL code on Direct3D for example.</p><p>####<br>Issues with HLSL11 IL and D3D11 compiler</p><div class="list" style="padding:8px 0;font-family:verdana;font-size:14px"><span class="list" style="margin:0">In this pipeline HLSL and HLSL11 IL have a central place but HLSL11 IL is not specified and it’s produced by a closed source Windows only compiler.<br>Additionally, the D3D11 compiler should be a deprecated software because:</span><br><br><em>1 It was designed for legacy vec4 GPUs which implies wasting 50% of registers on vec2 and %25% on vec3. Considering that most GPUs hide memory access latencies by launching as many wavefronts as possible, GPUs can only launch new wavefronts when there is still<br>space available in the register file. Thus, D3D11 compiler cost a great deal of performance by wasting registers. </em>2 HLSL11 doesn’t expose many hardware features (<a href="https://www.opengl.org/registry/specs/ARB/shader_draw_parameters.txt" rel="external nofollow noopener noreferrer" target="_blank">gl_DrawID</a>,&nbsp;<a href="https://www.khronos.org/registry/gles/extensions/EXT/EXT_shader_pixel_local_storage.txt" rel="external nofollow noopener noreferrer" target="_blank">pixel<br>local storage</a>,&nbsp;<a href="https://www.khronos.org/registry/gles/extensions/EXT/EXT_shader_framebuffer_fetch.txt" rel="external nofollow noopener noreferrer" target="_blank">framebuffer fetch</a>). How to express these features in HLSL when we want to use them<br>in GLSL where they are available? Well, it’s hard and at best possible through ugly hacks!<br>* 3 D3D11 compiler performs destructive &quot;optimizations&quot; on the input shaders. Maybe, these &quot;optimizations&quot; where ok at some points in history but they prevent GPU vendors to properly optimize the shaders because source information gets lost. From where comes<br>from these massive performance gains from hotfix drivers on new AAA released games? At least, part of it from shaders replacements. If HLSL11 IL input is A, use this totally different binary. This process is probably ok for hardware vendors that can afford<br>it but this process won’t scale beyond some flagship AAA games. Hence, the rest of us, the 99%, are paying the price of a poor compiler.<br></div><p>####<br>Easier and more robust cross compilation with SPIR-V</p><p>SPIR-V is a fully specified, cross APIs, binary intermediate language which is easy to read, to extend or to ignore unknown instructions without destructing the source in a tool chain.</p><div class="post-image-white" style="width:816px;padding-top:0;padding-bottom:0;text-align:center;margin-left:auto;margin-right:auto;font-family:verdana;font-size:14px"><br><img src="http://img.blog.csdn.net/20151108173315148?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br></div><br><div class="post-image-title" style="font-style:italic;text-align:center;padding-bottom:32px;font-family:verdana;font-size:14px"><br>Figure 2: A more desiable shader compilation pipeline to match the market reality</div><p>In figure 2 shows how SPIR-V could be used as a center piece of the compilation pipeline allowing multiple front-ends / languages to produce the SPIR-V IL. On platforms supporting SPIR-V, we could directly feed APIs with SPIR-V. The reality is that it will<br>take a lot of time to transform the ecosystem and we need a solution for others platforms (eg: shipped mobiles which will never get new drivers). Hence, in a market real shader compilation pipeline, SPIR-V would need to be converted to HLSL9 IL, HLSL11 IL,<br>GLSL, Metal, etc.</p><p>This is a lot of work but SPIR-V provides a simpler and more robust approach to cross compile to others ILs and languages.&nbsp;<a href="https://github.com/James-Jones/HLSLCrossCompiler" rel="external nofollow noopener noreferrer" target="_blank">HLSLcc</a>&nbsp;is a great<br>tool that demonstrated that cross compilation at IL level is a good direction. Another example is&nbsp;<a href="http://blogs.unity3d.com/2014/05/20/the-future-of-scripting-in-unity/" rel="external nofollow noopener noreferrer" target="_blank">IL2CPP</a>&nbsp;used by Unity<br>to cross compile C# to C&#43;&#43;. However, HLSL11 IL has many issues, as expressed previously. With SPIR-V, the source IL is fully specified and extendable making the translation from SPIR-V to HLSL11 IL easier (or even possible) than HLSL11 IL to SPIR-V for example.</p><p>I am glade to see that some frameworks are already&nbsp;<a href="https://jogamp.org/bugzilla/show_bug.cgi?id=1140#c2" rel="external nofollow noopener noreferrer" target="_blank">investigating about SPIR-V</a>:&nbsp;<a href="https://jogamp.org/" rel="external nofollow noopener noreferrer" target="_blank">Jogamp</a>&nbsp;is<br>a Java binding for multiple APIs including OpenGL and OpenCL. SPIR-V will allow the framework users to target independently OpenGL compute of OpenCL depending framework users needs for example.</p><div class="post-image-white" style="width:816px;padding-top:0;padding-bottom:0;text-align:center;margin-left:auto;margin-right:auto;font-family:verdana;font-size:14px"><br><img src="http://img.blog.csdn.net/20151108173349740?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br></div><br><div class="post-image-title" style="font-style:italic;text-align:center;padding-bottom:32px;font-family:verdana;font-size:14px"><br>Figure 3: Dreaming of a large market adoption, wish: that’s the plan 5 years down this line</div><p>In figure 3, we show what would be the ultimate goal for SPIR-V: A massive adoption of the shading intermediate language so that we only have focus on innovations in the source language world. Great time to start a PHD on shading language!</p><p>In the current ecosystem, we have to generate N shaders per source because Direct3D 9, Direct3D 11, OpenGL and OpenGL ES accept different syntaxes. In many cases, only the syntax sugar is different but the functionality is exactly the same from API to API.<br>The multiplication of generated shaders has a production cost in iteration time and latency until we get the results that SPIR-V could ultimately avoid. One SPIR-V =&gt; N platforms. Ideally, only hardware feature levels would condition the generation of multiple<br>shader outputs.</p><p>####<br>Building bridges between ecosystems</p><p>Supporting multiple source languages as first class citizen is valuable for many reasons: The user case choice the shading language he like best but also to build bridges between ecosystem: off-line rendering and real-time rendering; Mobile and desktop; Simulation<br>and rendering,; etc!</p><p>Let’s take a common real-time use case: the case of skinning. For some games it might be valuable to do the skinning using a computer shader so that this work will be done on the GPU. However, some games might be GPU bound so to get better performance, it’s<br>probably a good idea to rely on the CPU. Let’s imagine that&nbsp;<a href="https://github.com/ispc/ispc" rel="external nofollow noopener noreferrer" target="_blank">ISPC</a>&nbsp;decides to support SPIR-V just like it supports&nbsp;<a href="https://github.com/ispc/ispc/blob/master/ptxtools/ptx.ll" rel="external nofollow noopener noreferrer" target="_blank">NVIDIA<br>PTX</a>&nbsp;for example. In such case an engine could create a single parallel friendly and optimized code and a game could choose either a GPU or CPU target for skinning depending on its needs.</p><p>Futhermore, SPIR-V allows to decouple the tools from the shipping software. There is no reason to ship a SPIR-V compiler such as ISPC or LLVM in a final game. It would increase export time and take a lot of space which is not desiable on mobile or WebGL platforms<br>where memory is critical.</p><p>####<br>Better matching of the engine reality: custom shading languages!</p><p>If we consider engine shading languages, most are actually meta-HLSL using custom syntax to expose built-in shaders, engine specific functionality, fallback systems, making easier/possible cross compilation and platform targeting or offering a more natural<br>way to expose shaders to technical shader artists.&nbsp;<a href="http://docs.unity3d.com/Manual/SL-Reference.html" rel="external nofollow noopener noreferrer" target="_blank">Unity shading language</a>&nbsp;is a good example of such meta-HLSL approach.</p><p>SPIR-V is offering us the opportunity to officialize these languages and even build the custom engine functionalities in their hearts through extended instruction sets. SPIR-V is built around a model where unrecognized blocks are just ignored and preserved,<br>allowing interactions with SPIR-V tools unaware of these extensions.</p><p>Going crazier, we could imagine in the future storing OpenGL states in SPIR-V or describing rendering passes with off-line tools capable to filter redundant states changes. Many opportunities to run our imagination wild!</p><p>On the opposity side, we could imagine innovations to design a shading language friendly for technical artists, exposing&nbsp;<a href="http://www.frostbite.com/2014/11/moving-frostbite-to-pbr/" rel="external nofollow noopener noreferrer" target="_blank">PBR</a>&nbsp;parametrization<br>for example or decoupling&nbsp;<a href="http://aras-p.info/blog/2009/05/07/shaders-must-die-part-2/" rel="external nofollow noopener noreferrer" target="_blank">surface shader, lighting model and light shader</a>&nbsp;natively in the language.</p><p>####<br>NOT resolved by SPIR V: &quot;Ohoho, on line shader compilation time is too long.&quot;</p><p>Some game developers would say that we need an IL for better online compilation performance. I think this is misunderstanding the cause of this issue. The slow part of the GLSL compilation isn’t the translation from GLSL to IHVs IL (eg&nbsp;<a href="http://docs.nvidia.com/cuda/parallel-thread-execution/#abstract" rel="external nofollow noopener noreferrer" target="_blank">NVIDIA<br>PTX</a>&nbsp;or&nbsp;<a href="http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/AMD_Intermediate_Language_%28IL%29_Specification_v2.pdf" rel="external nofollow noopener noreferrer" target="_blank">AMD IL</a>) but the optimizations path on the IHVs IL, particularly<br>register allocation and scheduling transformation. These operations are fully dependent of GPU architectures hence should be performed by a targeted platform hardware vendor compiler.</p><p>Parsing GLSL / HLSL and performing non destruction optimizations have a cost and moving these off-line will give us some online performance gains. However, following the&nbsp;<a href="http://en.wikipedia.org/wiki/Pareto_principle" rel="external nofollow noopener noreferrer" target="_blank">80-20<br>rule</a>, we should expect only 20% performance gain. Compilation is a finer art than just parsing languages.</p><p>####<br>NOT fully resolved by SPIR V: &quot;Ohoho, IHVs shader compilers are rubish.&quot;</p><p>A lot of issues with current GLSL compilers is that all of them support invalid syntaxes but hardware vendors don’t want to fix them because it could break some applications. Removing the language syntax avoid potential errors but if tools generate invalid<br>SPIR-V code, chances are that drivers will still do whatever it takes to garantee the IL works on their implementations. From an IHVs point of view, the worse is an application that doesn’t work on their platforms by run fine else where.</p><p>By design, SPIR-V doesn’t prevent IHVs to run invalid code so something else would need to be done to resolve this issue. Conformance tests? This said, I think that the simplicity of SPIR-V brings us to a much better state but the larger potential SPIR-V ecosystem<br>than GLSL makes it required to ensure SPIR-V code quality.</p><p>The other issue is that some shader compilers are terrible at performance trivial optimizations hence we will probably need to have a shader pipeline path to do this job.</p><p>####<br>Better integration of the shading language in modern graphics APIs:</p><div class="post-image-white" style="width:816px;padding-top:0;padding-bottom:0;text-align:center;margin-left:auto;margin-right:auto;font-family:verdana;font-size:14px"><br><img src="http://img.blog.csdn.net/20151108173431099?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br><br></div><br><div class="post-image-white" style="width:816px;padding-top:0;padding-bottom:0;text-align:center;margin-left:auto;margin-right:auto;font-family:verdana;font-size:14px"><br><span style="font-style:italic">Figure 4: Integration of SPIR-V in an explict graphics API such as Vulkan</span></div><p>Figure 4 shows a potential integration of SPIR-V in Vulkan. Basically, with OpenGL and GLSL, all the cross compilation can be done off-line but the rest, including the actual shader compilations, needs to happen in the rendering thread. With Vulkan, we can<br>decouple the tasks and guarantee that the rendering loop never compile shaders because of shader patching, play with shader cache or validate states. Additionally, we have a full control other the threaded shader compilations. With OpenGL, if we don’t query<br>the compilation results right after requesting a compilation, the drivers may thread and hide the compilation time but this behavior is not specified: Would the drivers actually thread the compilation? How many compilations can happen in parallel? Vulkan is<br>an explict API, hence we have control for each step, when, where and how these steps should be performed.</p><p>####<br>Time for celebrations before hard work for an ecosystem revolution!</p><p>I am absolutely convinced that SPIR-V is a game changer for the industry just like WebGL before it. However, there is no magic. To become really successful, it will take a lot of efforts to ensure adoption by hardware and platform vendors but also to adapt<br>SPIR-V to the reality of the market and build (open source!) cross compilation tools placing SPIR-V at their centers.</p><p>The head lines might be all about iPhone6 and Samsung S6 but the reality of the market is that we are seeing a race to the bottom with lot of old ES2 GPU IP shipping everyday. Furthermore, there is already a lot of devices already shipped that will never see<br>the color of a drivers update during their entire life time. Finally, Vulkan might be great and all but realistically it will take years for the ecosystem to complete this transition. Meanwhile, there is a lot of OpenGL/ES, WebGL, Direct3D, console APIs out<br>there. SPIR-V is solution for all of them!</p><p>I am looking forward the shading language revolution that SPIR-V will lead to, one step at a time!</p><p>Enjoy!</p><p>I cannot express how awesome this is. :-P<span class="quote-author" style="font-style:normal;text-align:right;display:block"><a href="https://twitter.com/JJcoolkl/status/572791033551036417" rel="external nofollow noopener noreferrer" target="_blank">John W<br>Kloetzli, Jr</a></span></p><ul><li><a href="https://www.khronos.org/registry/spir-v/papers/WhitePaper.pdf" rel="external nofollow noopener noreferrer" target="_blank">SPIR-V: whitepaper</a></li><li><a href="https://www.khronos.org/registry/spir-v/specs/1.0/SPIRV.pdf" rel="external nofollow noopener noreferrer" target="_blank">SPIR-V 0.99, provisiontal specification</a></li><li><a href="https://www.khronos.org/registry/spir-v/specs/1.0/GLSL.std.450.pdf" rel="external nofollow noopener noreferrer" target="_blank">SPIR-V Extended Instructions for GLSL, provisiontal specification</a></li><li><a href="https://www.khronos.org/registry/spir-v/specs/1.0/OpenCL.std.21.pdf" rel="external nofollow noopener noreferrer" target="_blank">SPIR-V OpenCL 2.1 Extended Instruction Set, provisiontal specification</a></li><li><a href="https://www.khronos.org/spir" rel="external nofollow noopener noreferrer" target="_blank">SPIR-V front page</a></li><li><a href="https://www.khronos.org/vulkan" rel="external nofollow noopener noreferrer" target="_blank">Vulkan front page</a></li></ul><p><br>本文地址 <a href="http://yjaelex.github.io/2015/11/08/转-转-Redefining-the-shading-languages-ecosystem-with-SPIR-V/">http://yjaelex.github.io/2015/11/08/转-转-Redefining-the-shading-languages-ecosystem-with-SPIR-V/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;SPIR-V，全称&lt;strong&gt;&lt;span style=&quot;color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px&quot;&gt;Standard Portable Intermediate Representation&lt;/span&gt;&lt;span style=&quot;color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px&quot;&gt;&amp;nbsp;(&lt;/span&gt;&lt;span style=&quot;color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px&quot;&gt;SPIR&lt;/span&gt;&lt;/strong&gt;&lt;span style=&quot;color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px&quot;&gt;&lt;strong&gt;)&lt;/strong&gt;&amp;nbsp;是一种用在GPU通用计算和图形学上的中间语言（&lt;/span&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Intermediate_language&quot; title=&quot;Intermediate language&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;intermediate&lt;br&gt;language&lt;/a&gt;，类&amp;#20284;汇编）；由&lt;a href=&quot;https://en.wikipedia.org/wiki/Khronos_Group&quot; title=&quot;Khronos Group&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Khronos&lt;/a&gt;开发&lt;span style=&quot;color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px&quot;&gt;,&lt;br&gt;最初是为&lt;/span&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/OpenCL&quot; title=&quot;OpenCL&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;OpenCL&lt;/a&gt;准备的。&lt;span style=&quot;color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px&quot;&gt;&amp;nbsp;目前的版本是&lt;br&gt;SPIR-V，和下一代图形标准Vulkan差不多同时提出。前面版本的SPIR其实基于LLVM IR；而最新的&lt;span style=&quot;color:#252525;font-family:sans-serif;font-size:14px;line-height:22.4px&quot;&gt;SPIR-V则是重新定义了一套；当然还是和LLVM IR有些类&amp;#20284;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[转][转] A trip through the Graphics Pipeline</title>
    <link href="http://yjaelex.github.io/2015/09/29/%E8%BD%AC-%E8%BD%AC-A-trip-through-the-Graphics-Pipeline/"/>
    <id>http://yjaelex.github.io/2015/09/29/转-转-A-trip-through-the-Graphics-Pipeline/</id>
    <published>2015-09-29T08:56:02.000Z</published>
    <updated>2016-12-29T08:42:45.929Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><p>转一个国外牛人的文章；深入浅出的介绍了现代GPU的方方面面，从软件到硬件都有。理解了这些对理解新一代图形API大有好处！<img src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/smile.gif" alt="微笑"></p><p>原文&nbsp;<a href="https://fgiesen.wordpress.com/2011/07/09/a-trip-through-the-graphics-pipeline-2011-index/" rel="external nofollow noopener noreferrer" target="_blank">A trip through the Graphics Pipeline 2011: Index</a></p><p>PDF下载：<a href="http://download.csdn.net/detail/qwertyu1234/9041011" rel="external nofollow noopener noreferrer" target="_blank">http://download.csdn.net/detail/qwertyu1234/9041011</a></p><a id="more"></a><p><strong>A trip through the Graphics Pipeline 2011: Index</strong></p><p>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; July 9, 2011&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; Welcome.</p><p>This is the index page for a series of blog posts I’m currently writing aboutthe D3D/OpenGL graphics pipelines&nbsp;<em>as actually implemented by GPUs</em>.A lot of this is well known among graphics programmers, and there’s tons ofpapers on various bits and pieces<br>of it, but one bit I’ve been annoyed with isthat while there’s both broad overviews and very detailed information onindividual components, there’s not much in between, and what little there is ismostly out of date.</p><p>This series is intended for graphics programmers that know a modern 3D API (atleast OpenGL 2.0&#43; or D3D9&#43;) well and want to know how it all looks under thehood. It’s&nbsp;<em>not</em>&nbsp;a description of the graphics pipeline fornovices; if you haven’t used a 3D API,<br>most if not all of this will becompletely useless to you. I’m also assuming a working understanding ofcontemporary hardware design – you should at the very least know whatregisters, FIFOs, caches and pipelines are, and understand how they work.Finally, you<br>need a working understanding of at least basic parallelprogramming mechanisms. A GPU is a massively parallel computer, there’s no wayaround it.</p><p>Some readers have commented that this is a really low-level description of thegraphics pipeline and GPUs; well, it all depends on where you’re standing. GPUarchitects would call this a&nbsp;<em>high-level</em>&nbsp;description of a GPU.Not quite as high-level as the<br>multicolored flowcharts you tend to see onhardware review sites whenever a new GPU generation arrives; but, to be honest,that kind of reporting tends to have a very low information density, even whenit’s done well. Ultimately, it’s not meant to explain how<br>anythingactually&nbsp;<em>works</em>&nbsp;– it’s just technology porn that’s trying toshow off shiny new gizmos. Well, I try to be a bit more substantial here, whichunfortunately means less colors and less benchmark results, but instead lotsand lots of text, a few mono-colored<br>diagrams and even some (<em>shudder</em>)equations. If that’s okay with you, then here’s the index:</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/01/a-trip-through-the-graphics-pipeline-2011-part-1/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 1</span></a>: Introduction; the Software stack.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/02/a-trip-through-the-graphics-pipeline-2011-part-2/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 2</span></a>: GPU memory architecture and the Command Processor.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/03/a-trip-through-the-graphics-pipeline-2011-part-3/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 3</span></a>: 3D pipeline overview, vertex processing.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/04/a-trip-through-the-graphics-pipeline-2011-part-4/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 4</span></a>: Texture samplers.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/05/a-trip-through-the-graphics-pipeline-2011-part-5/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 5</span></a>: Primitive Assembly, Clip/Cull, Projection, and Viewporttransform.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/06/a-trip-through-the-graphics-pipeline-2011-part-6/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 6</span></a>: (Triangle) rasterization and setup.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/08/a-trip-through-the-graphics-pipeline-2011-part-7/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 7</span></a>: Z/Stencil processing, 3 different ways.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/10/a-trip-through-the-graphics-pipeline-2011-part-8/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 8</span></a>: Pixel processing – “fork phase”.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/12/a-trip-through-the-graphics-pipeline-2011-part-9/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 9</span></a>: Pixel processing – “join phase”.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/20/a-trip-through-the-graphics-pipeline-2011-part-10/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 10</span></a>: Geometry Shaders.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/08/14/a-trip-through-the-graphics-pipeline-2011-part-11/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 11</span></a>: Stream-Out.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/09/06/a-trip-through-the-graphics-pipeline-2011-part-12/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 12</span></a>: Tessellation.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://fgiesen.wordpress.com/2011/10/09/a-trip-through-the-graphics-pipeline-2011-part-13/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Part 13</span></a>: Compute Shaders.</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><table border="0" cellspacing="0" cellpadding="0" width="927"><br></table><p><strong>A trip through the GraphicsPipeline 2011, part 1</strong></p><p>&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; July 1, 2011</p><p>It’s been awhile since I posted something here, and I figured I might use thisspot to explain some general points about graphics hardware and software as of2011; you can find functional descriptions of what the graphics stack in yourPC does, but usually not<br>the “how” or “why”; I’ll try to fill in the blankswithout getting too specific about any particular piece of hardware. I’m goingto be mostly talking about DX11-class hardware running D3D9/10/11 on Windows,because that happens to be the (PC) stack I’m most<br>familiar with – not that theAPI details etc. will matter much past this first part; once we’re actually onthe GPU it’s all native commands.</p><p>**</p><p>The application**</p><p>This is your code. These are also your bugs. Really. Yes, the API runtime andthe driver have bugs, but this is not one of them. Now go fix it already.</p><p>**</p><p>The API runtime**</p><p>You make your resource creation / state setting / draw calls to the API. TheAPI runtime keeps track of the current state your app has set, validatesparameters and does other error and consistency checking, manages user-visibleresources, may or may not validate<br>shader code and shader linkage (or at leastD3D does, in OpenGL this is handled at the driver level) maybe batches worksome more, and then hands it all over to the graphics driver – more precisely,the user-mode driver.</p><p>**</p><p>The user-mode graphics driver (or UMD)</p><p><em>*This is where most of the “magic” on the CPU side happens. If your appcrashes because of some API call you did, it will usually be in here :). It’scalled “nvd3dum.dll” (NVidia) or “atiumd</em>.dll” (AMD). As the name suggests,this is user-mode code; it’s<br>running in the same context and address space asyour app (and the API runtime) and has no elevated privileges whatsoever. It implementsa lower-level API (the DDI) that is called by D3D; this API is fairly similarto the one you’re seeing on the surface, but<br>a bit more explicit about thingslike memory management and such.</p><p>This module is where things like shader compilation happen. D3D passes apre-validated shader token stream to the UMD – i.e. it’s already checked thatthe code is valid in the sense of being syntactically correct and obeying D3Dconstraints (using the right types,<br>not using more textures/samplers thanavailable, not exceeding the number of available constant buffers, stuff likethat). This is compiled from HLSL code and usually has quite a number ofhigh-level optimizations (various loop optimizations, dead-code elimination,constant<br>propagation, predicating ifs etc.) applied to it – this is good newssince it means the driver benefits from all these relatively costlyoptimizations that have been performed at compile time. However, it also has abunch of lower-level optimizations (such as<br>register allocation and loopunrolling) applied that drivers would rather do themselves; long story short,this usually just gets immediately turned into a intermediate representation(IR) and then compiled some more; shader hardware is close enough to D3Dbytecode<br>that compilation doesn’t need to work wonders to give good results(and the HLSL compiler having done some of the high-yield and high-costoptimizations already definitely helps), but there’s still lots of low-leveldetails (such as HW resource limits and scheduling<br>constraints) that D3Dneither knows nor cares about, so this is not a trivial process.</p><p>And of course, if your app is a well-known game, programmers at NV/AMD haveprobably looked at your shaders and wrote hand-optimized replacements for theirhardware – though they better produce the same results lest there be a scandal:). These shaders get detected<br>and substituted by the UMD too. You’re welcome.</p><p>More fun: Some of the API state may actually end up being compiled into theshader – to give an example, relatively exotic (or at least infrequently used)features such as texture borders are probably not implemented in the texturesampler, but emulated with extra<br>code in the shader (or just not supported atall). This means that there’s sometimes multiple versions of the same shaderfloating around, for different combinations of API states.</p><p>Incidentally, this is also the reason why you’ll often see a delay the firsttime you use a new shader or resource; a lot of the creation/compilation workis deferred by the driver and only executed when it’s actually necessary (youwouldn’t believe how much unused<br>crap some apps create!). Graphics programmersknow the other side of the story – if you want to make sure something isactually created (as opposed to just having memory reserved), you need to issuea dummy draw call that uses it to “warm it up”. Ugly and annoying,<br>but this hasbeen the case since I first started using 3D hardware in 1999 – meaning, it’spretty much a fact of life by this point, so get used to it. :)</p><p>Anyway, moving on. The UMD also gets to deal with fun stuff like all the D3D9“legacy” shader versions and the fixed function pipeline – yes, all of thatwill get faithfully passed through by D3D. The 3.0 shader profile ain’t thatbad (it’s quite reasonable in<br>fact), but 2.0 is crufty and the various 1.xshader versions are seriously whack – remember 1.3 pixel shaders? Or, for thatmatter, the fixed-function vertex pipeline with vertex lighting and such? Yeah,support for all that’s still there in D3D and the guts<br>of every modern graphicsdriver, though of course they just translate it to newer shader versions by now(and have been doing so for quite some time).</p><p>Then there’s things like memory management. The UMD will get things liketexture creation commands and need to provide space for them. Actually, the UMDjust suballocates some larger memory blocks it gets from the KMD (kernel-modedriver); actually mapping and<br>unmapping pages (and managing which part of videomemory the UMD can see, and conversely which parts of system memory the GPU mayaccess) is a kernel-mode privilege and can’t be done by the UMD.</p><p>But the UMD can do things like&nbsp;<a href="http://fgiesen.wordpress.com/2011/01/17/texture-tiling-and-swizzling/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">swizzling textures</span></a>&nbsp;(unlessthe GPU can do this in hardware, usually using 2D<br>blitting units not the real3D pipeline) and schedule transfers between system memory and (mapped) videomemory and the like. Most importantly, it can also write command buffers (or“DMA buffers” – I’ll be using these two names interchangeably) once the KMD hasallocated<br>them and handed them over. A command buffer contains, well, commands:). All your state-changing and drawing operations will be converted by the UMDinto commands that the hardware understands. As will a lot of things you don’ttrigger manually – such as uploading<br>textures and shaders to video memory.</p><p>In general, drivers will try to put as much of the actual processing into theUMD as possible; the UMD is user-mode code, so anything that runs in it doesn’tneed any costly kernel-mode transitions, it can freely allocate memory, farmwork out to multiple threads,<br>and so on – it’s just a regular DLL (even thoughit’s loaded by the API, not directly by your app). This has advantages for driverdevelopment too – if the UMD crashes, the app crashes with it, but not thewhole system; it can just be replaced while the system<br>is running (it’s just aDLL!); it can be debugged with a regular debugger; and so on. So it’s not onlyefficient, it’s also convenient.</p><p>But there’s a big elephant in the room that I haven’t mentioned yet.</p><p><strong>Did I say “user-mode driver”? I meant “user-mode drivers”.</strong></p><p>As said, the UMD is just a DLL. Okay, one that happens to have the blessing ofD3D and a direct pipe to the KMD, but it’s still a regular DLL, and in runs inthe address space of its calling process.</p><p>But we’re using multi-tasking OSes nowadays. In fact, we have been for sometime.</p><p>This “GPU” thing I keep talking about? That’s a shared resource. There’s onlyone that drives your main display (even if you use SLI/Crossfire). Yet we havemultiple apps that try to access it (and pretend they’re the only ones doingit). This doesn’t just work<br>automatically; back in The Olden Days, the solutionwas to only give 3D to one app at a time, and while that app was active, allothers wouldn’t have access. But that doesn’t really cut it if you’re trying tohave your windowing system use the GPU for rendering.<br>Which is why you needsome component that arbitrates access to the GPU and allocates time-slices andsuch.</p><p>**Enter the scheduler.</p><p>**This is a system component – note the “the” is somewhat misleading; I’mtalking about the graphics scheduler here, not the CPU or IO schedulers. Thisdoes exactly what you think it does – it arbitrates access to the 3D pipelineby time-slicing it between<br>different apps that want to use it. A context switchincurs, at the very least, some state switching on the GPU (which generatesextra commands for the command buffer) and possibly also swapping some resourcesin and out of video memory. And of course only one<br>process gets to actuallysubmit commands to the 3D pipe at any given time.</p><p>You’ll often find console programmers complaining about the fairly high-level,hands-off nature of PC 3D APIs, and the performance cost this incurs. But thething is that 3D APIs/drivers on PC really have a more complex problem to solvethan console games – they<br>really do need to keep track of the full currentstate for example, since someone may pull the metaphorical rug from under themat any moment! They also work around broken apps and try to fix performanceproblems behind their backs; this is a rather annoying<br>practice that no-one’shappy with, certainly including the driver authors themselves, but the fact isthat the business perspective wins here; people expect stuff that runs tocontinue running (and doing so smoothly). You just won’t win any friends byyelling<br>“BUT IT’S WRONG!” at the app and then sulking and going through anultra-slow path.</p><p>Anyway, on with the pipeline. Next stop: Kernel mode!</p><p><strong>The kernel-mode driver (KMD)</strong></p><p>This is the part that actually deals with the hardware. There may be multipleUMD instances running at any one time, but there’s only ever one KMD, and ifthat crashes, then boom you’re dead – used to be “blue screen” dead, but by nowWindows actually knows how<br>to kill a crashed driver and reload it (progress!).As long as it happens to be just a crash and not some kernel memory corruptionat least – if that happens, all bets are off.</p><p>The KMD deals with all the things that are just there once. There’s only oneGPU memory, even though there’s multiple apps fighting over it. Someone needsto call the shots and actually allocate (and map) physical memory. Similarly,someone must initialize the<br>GPU at startup, set display modes (and get modeinformation from displays), manage the hardware mouse cursor (yes, there’s HWhandling for this, and yes, you really only get one! :), program the HWwatchdog timer so the GPU gets reset if it stays unresponsive<br>for a certaintime, respond to interrupts, and so on. This is what the KMD does.</p><p>There’s also this whole content protection/DRM bit about setting up aprotected/DRM’ed path between a video player and the GPU so no the actualprecious decoded video pixels aren’t visible to any dirty user-mode code thatmight do awful forbidden things like dump<br>them to disk (…whatever). The KMD hassome involvement in that too.</p><p>Most importantly for us, the KMD manages the&nbsp;<em>actual</em>&nbsp;commandbuffer. You know, the one that the hardware actually consumes. The commandbuffers that the UMD produces aren’t the real deal – as a matter of fact,they’re just random slices of GPU-addressable<br>memory. What actually happenswith them is that the UMD finishes them, submits them to the scheduler, whichthen waits until that process is up and then passes the UMD command buffer onto the KMD. The KMD then writes a call to command buffer into the main commandbuffer,<br>and depending on whether the GPU command processor can read from mainmemory or not, it may also need to DMA it to video memory first. The maincommand buffer is usually a (quite small)&nbsp;<a href="http://fgiesen.wordpress.com/2010/12/14/ring-buffers-and-queues/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">ring<br>buffer</span></a>&nbsp;– the only thing that ever gets written there is system/initializationcommands and calls to the “real”, meaty 3D command buffers.</p><p>But this is still just a buffer in memory right now. Its position is known tothe graphics card – there’s usually a read pointer, which is where the GPU isin the main command buffer, and a write pointer, which is how far the KMD haswritten the buffer yet (or<br>more precisely, how far it has&nbsp;<em>told</em>&nbsp;theGPU it has written yet). These are hardware registers, and they arememory-mapped – the KMD updates them periodically (usually whenever it submitsa new chunk of work)…</p><p><strong>The bus</strong>…</p><p>but of course that write doesn’t go directly to the graphics card (at leastunless it’s integrated on the CPU die!), since it needs to go through the busfirst – usually PCI Express these days. DMA transfers etc. take the same route.This doesn’t take very long,<br>but it’s yet another stage in our journey. Untilfinally…</p><p><strong>The command processor!</strong></p><p>This is the frontend of the GPU – the part that actually reads the commands theKMD writes. I’ll continue from here in the next installment, since this post islong enough already :)</p><p><strong>Small aside: OpenGL</strong></p><p>OpenGL is fairly similar to what I just described, except there’s not as sharpa distinction between the API and UMD layer. And unlike D3D, the (GLSL) shadercompilation is not handled by the API at all, it’s all done by the driver. Anunfortunate side effect<br>is that there are as many GLSL frontends as there are3D hardware vendors, all of them basically implementing the same spec, but withtheir own bugs and idiosyncrasies. Not fun. And it also means that the drivershave to do all the optimizations themselves whenever<br>they get to see theshaders – including expensive optimizations. The D3D bytecode format is reallya cleaner solution for this problem – there’s only one compiler (so no slightlyincompatible dialects between different vendors!) and it allows for somecostlier<br>data-flow analysis than you would normally do.</p><p><strong>Omissions and simplifcations</strong></p><p>This is just an overview; there’s tons of subtleties that I glossed over. Forexample, there’s not just one scheduler, there’s multiple implementations (thedriver can choose); there’s the whole issue of how synchronization between CPUand GPU is handled that<br>I didn’t explain at all so far. And so on. And I mighthave forgotten something important – if so, please tell me and I’ll fix it! Butnow, bye and hopefully see you next time</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>A trip through the GraphicsPipeline 2011, part 2</strong></p><p>&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; July 2, 2011</p><p>**</p><p>Not so fast.</p><p>**In the previous part I explained the various stages that your 3D renderingcommands go through on a PC before they actually get handed off to the GPU;short version: it’s more than you think. I then finished by name-dropping thecommand processor and<br>how it actually finally does something with the commandbuffer we meticulously prepared. Well, how can I say this – I lied to you.We’ll indeed be meeting the command processor for the first time in thisinstallment, but remember, all this command buffer stuff<br>goes through memory –either system memory accessed via PCI Express, or local video memory. We’regoing through the pipeline in order, so before we get to the command processor,let’s talk memory for a second.</p><p><strong>The memory subsystem</strong></p><p>GPUs don’t have your regular memory subsystem – it’s different from what yousee in general-purpose CPUs or other hardware, because it’s designed for verydifferent usage patterns. There’s two fundamental ways in which a GPU’s memorysubsystem differs from what<br>you see in a regular machine:</p><p>The first is that GPU memory subsystems are&nbsp;<em>fast</em>. Seriously fast. ACore i7 2600K will hit maybe 19 GB/s memory bandwidth – on a good day. Withtail wind. Downhill. A GeForce GTX 480, on the other hand, has a total memorybandwidth of close to 180 GB/s<br>– nearly an order of magnitude difference! Whoa.</p><p>The second is that GPU memory subsystems are&nbsp;<em>slow</em>. Seriously slow.A cache miss to main memory on a Nehalem (first-generation Core i7) takes about140 cycles if you divide the&nbsp;<a href="http://www.anandtech.com/show/2542/5" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">memory<br>latencyas given by AnandTech</span></a>&nbsp;by the clock rate. The GeForce GTX 480 I mentionedpreviously has a&nbsp;<a href="http://www.stanford.edu/dept/ICME/docs/seminars/Rennich-2011-04-25.pdf" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">memory access<br>latency of 400-800 clocks</span></a>. So let’s just say that,measured in cycles, the GeForce GTX 480 has a bit more than 4x the averagememory latency of a Core i7. Except that Core i7 I just mentioned is clocked at2.93GHz, whereas GTX 480 shader clock is 1.4<br>GHz – that’s it, another 2x rightthere. Woops – again, nearly an order of magnitude difference! Wait, somethingfunny is going on here. My common sense is tingling. This must be one of thosetrade-offs I keep hearing about in the news!</p><p>Yep – GPUs get a massive increase in bandwidth, but they pay for it with amassive increase in latency (and, it turns out, a sizable hit in power drawtoo, but that’s beyond the scope of this article). This is part of a generalpattern – GPUs are all about throughput<br>over latency; don’t wait for resultsthat aren’t there yet, do something else instead!</p><p>That’s almost all you need to know about GPU memory, except for one generalDRAM tidbit that will be important later on: DRAM chips are organized as a 2Dgrid – both logically and physically. There’s (horizontal) row lines and(vertical) column lines. At each<br>intersection between such lines is atransistor and a capacitor; if at this point you want to know how to actuallybuild memory from these ingredients,&nbsp;<a href="http://en.wikipedia.org/wiki/DRAM#Operation_principle" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Wikipedia<br>isyour friend</span></a>. Anyway, the salient point here is that the address of alocation in DRAM is split into a row address and a column address, and DRAMreads/writes internally always end up accessing all columns in the given row atthe same time. What this<br>means is that it’s much cheaper to access a swath ofmemory that maps to exactly one DRAM row than it is to access the same amountof memory spread across multiple rows. Right now this may seem like just arandom bit of DRAM trivia, but this will become important<br>later on; in otherwords, pay attention: this will be on the exam. But to tie this up with thefigures in the previous paragraphs, just let me note that you can’t reach thosepeak memory bandwidth figures above by just reading a few bytes all overmemory; if you<br>want to saturate memory bandwidth, you better do it one full DRAMrow at a time.</p><p><strong>The PCIe host interface</strong></p><p>From a graphics programmer standpoint, this piece of hardware isn’tsuper-interesting. Actually, the same probably goes for a GPU hardwarearchitect too. The thing is, you still start caring about it once it’s so slowthat it’s a bottleneck. So what you do is<br>get good people on it to do itproperly, to make sure that doesn’t happen. Other than that, well, this givesthe CPU read/write access to video memory and a bunch of GPU registers, the GPUread/write access to (a portion of) main memory, and everyone a headachebecause<br>the latency for all these transactions is even worse than memorylatency because the signals have to go out of the chip, into the slot, travel abit across the mainboard then get to someplace in the CPU about a week later(or that’s how it feels compared to the<br>CPU/GPU speeds anyway). The bandwidthis decent though – up to about 8GB/s (theoretical) peak aggregate bandwidthacross the 16-lane PCIe 2.0 connections that most GPUs use right now, so betweenhalf and a third of the aggregate CPU memory bandwidth; that’s a<br>usable ratio.And unlike earlier standards like AGP, this is a symmetrical point-to-pointlink – that bandwidth goes both directions; AGP had a fast channel from the CPUto the GPU, but not the other way round.</p><p><strong>Some final memory bits and pieces</strong></p><p>Honestly, we’re&nbsp;<em>very very</em>&nbsp;close to actually seeing 3D commandsnow! So close you can almost taste them. But there’s one more thing we need toget out of the way first. Because now we have two kinds of memory – (local)video memory and mapped system memory.<br>One is about a day’s worth of travel tothe north, the other is a week’s journey to the south along the PCI Expresshighway. Which road do we pick?</p><p>The easiest solution: Just add an extra address line that tells you which wayto go. This is simple, works just fine and has been done plenty of times. Ormaybe you’re on a unified memory architecture, like some game consoles (but notPCs). In that case, there’s<br>no choice; there’s just&nbsp;<em>the memory</em>, whichis where you go, period. If you want something fancier, you add a MMU (memorymanagement unit), which gives you a fully virtualized address space and allowsyou to pull nice tricks like having frequently accessed<br>parts of a texture invideo memory (where they’re fast), some other parts in system memory, and mostof it not mapped at all – to be conjured up from thing air, or, more usually,by a magic disk read that will only take about 50 years or so – and by the way,this<br>is not hyperbole; if you stay with the “memory access = 1 day” metaphor,that’s really how long a single HD read takes. A quite fast one at that. Diskssuck. But I digress.</p><p>So, MMU. It also allows you to defragment your video memory address spacewithout having to actually copy stuff around when you start running out ofvideo memory. Nice thing, that. And it makes it much easier to have multipleprocesses share the same GPU. It’s<br>definitely allowed to have one, but I’m notactually sure if it’s a requirement or not, even though it’s certainly reallynice to have (anyone care to help me out here? I’ll update the article if I getclarification on this, but tbh right now I just can’t be<br>arsed to look it up).Anyway, a MMU/virtual memory is not really something you can just add on theside (not in an architecture with caches&nbsp;&nbsp;and memory consistencyconcerns anyway), but it really isn’t specific to any particular stage – I haveto mention it somewhere,<br>so I just put it here.</p><p>There’s also a DMA engine that can copy memory around without having to involveany of our precious 3D hardware/shader cores. Usually, this can at least copybetween system memory and video memory (in both directions). It often can alsocopy from video memory<br>to video memory (and if you have to do any VRAMdefragmenting, this is a useful thing to have). It usually can’t do systemmemory to system memory copies, because this is a GPU, not a memory copyingunit – do your system memory copies on the CPU where they don’t<br>have to passthrough PCIe in both directions!</p><p>&nbsp;<img src="" alt=""></p><p><strong>Update</strong>: I’ve drawn a&nbsp;<a href="http://www.farbrausch.de/~fg/gpu/gpu_memory.jpg" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">picture</span></a>&nbsp;(link since this layoutis too narrow to put big diagrams in the text). This also shows<br>some moredetails – by now your GPU has multiple memory controllers, each of whichcontrols multiple memory banks, with a fat hub in the front. Whatever it takesto get that bandwidth. :)</p><p>Okay, checklist. We have a command buffer prepared on the CPU. We have the PCIehost interface, so the CPU can actually tell us about this, and write itsaddress to some register. We have the logic to turn that address into a loadthat will actually return data<br>– if it’s from system memory it goes throughPCIe, if we decide we’d rather have the command buffer in video memory, the KMDcan set up a DMA transfer so neither the CPU nor the shader cores on the GPUneed to actively worry about it. And then we can get the<br>data from our copy invideo memory through the memory subsystem. All paths accounted for, we’re setand finally ready to look at some commands!</p><p>**At long last, the command processor!</p><p>**Our discussion of the command processor starts, as so many things do thesedays, with a single word:</p><p>“Buffering…”</p><p>As mentioned above, both of our memory paths leading up to here arehigh-bandwidth but also high-latency. For most later bits in the GPU pipeline,the method of choice to work around this is to run lots of independent threads.But in this case, we only have a<br>single command processor that needs to chewthrough our command buffer in order (since this command buffer contains thingssuch as state changes and rendering commands that need to be executed in theright sequence). So we do the next best thing: Add a large<br>enough buffer andprefetch far enough ahead to avoid hiccups.&nbsp;</p><p>From that buffer, it goes to the actual command processing front end, which isbasically a state machine that knows how to parse commands (with ahardware-specific format). Some commands deal with 2D rendering operations –unless there’s a separate command processor<br>for 2D stuff and the 3D frontendnever even sees it. Either way, there’s still dedicated 2D hardware hidden onmodern GPUs, just as there’s a VGA chip somewhere on that die that stillsupports text mode, 4-bit/pixel bit-plane modes, smooth scrolling and all thatstuff.<br>Good luck finding any of that on the die without a microscope. Anyway,that stuff exists, but henceforth I shall not mention it again. :) Then there’scommands that actually hand some primitives to the 3D/shader pipe, woo-hoo!I’ll take about them in upcoming<br>parts. There’s also commands that go to the3D/shader pipe but never render anything, for various reasons (and in variouspipeline configurations); these are up even later.</p><p>Then there’s commands that change state. As a programmer, you think of them asjust changing a variable, and that’s basically what happens. But a GPU is amassively parallel computer, and you can’t just change a global variable in aparallel system and hope that<br>everything works out OK – if you can’t guaranteethat everything will work by virtue of some invariant you’re enforcing, there’sa bug and you will hit it eventually. There’s several popular methods, andbasically all chips use different methods for different<br>types of state.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Whenever you change a state, you require that all pendingwork that might refer to that state be finished (i.e. basically a partialpipeline flush). Historically, this is how graphics chips handled most state changes– it’s simple and not that costly<br>if you have a low number of batches, fewtriangles and a short pipeline. Alas, batch and triangle counts have gone upand pipelines have gotten long, so the cost for this type of approach has shotup. It’s still alive and kicking for stuff that’s either changed<br>infrequently(a dozen partial pipeline flushes aren’t that big a deal over the course of awhole frame) or just too expensive/difficult to implement with more specificschemes though.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You can make hardware units completely stateless. Justpass the state change command through up to the stage that cares about it; thenhave that stage append the current state to everything it sends downstream,every cycle. It’s not stored anywhere<br>– but it’s always around, so if somepipeline stage wants to look at a few bits in the state it can, because they’repassed in (and then passed on to the next stage). If your state happens to bejust a few bits, this isn’t fairly cheap and practical. If it happens<br>to be thefull set of active textures along with texture sampling state, not so much.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sometimes storing just one copy of the state and havingto flush every time that stage changes serializes things too much, but thingswould really be fine if you had two copies (or maybe four?) so yourstate-setting frontend could get a bit ahead. Say<br>you have enough registers(“slots”) to store two versions of every state, and some active job referencesslot 0. You can safely modify slot 1 without stopping that job, or otherwiseinterfering with it at all. Now you don’t need to send the whole state aroundthrough<br>the pipeline – only a single bit per command that selects whether touse slot 0 or 1. Of course, if both slot 0 and 1 are busy by the time a statechange command is encountered, you still have to wait, but you can get one stepahead. The same technique works<br>with more than two slots.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For some things like sampler or texture Shader ResourceView state, you could be setting very large numbers of them at the same time,but chances are you aren’t. You don’t want to reserve state space for 2*128active textures just because you’re keeping<br>track of 2 in-flight state sets soyou might need it. For such cases, you can use a kind of register renamingscheme – have a pool of 128 physical texture descriptors. If someone actuallyneeds 128 textures in one shader, then state changes are gonna be slow.<br>(Toughbreak). But in the more likely case of an app using less than 20 textures, youhave quite some headroom to keep multiple versions around.</p><p>This is not meant to be a comprehensive list – but the main point is thatsomething that looks as simple as changing a variable in your app (and even inthe UMD/KMD and the command buffer for that matter!) might actually need anontrivial amount of supporting<br>hardware behind it just to prevent it fromslowing things down.</p><p><strong>Synchronization</strong></p><p>Finally, the last family of commands deals with CPU/GPU and GPU/GPU synchronization.</p><p>Generally, all of these have the form “if event X happens, do Y”. I’ll dealwith the “do Y” part first – there’s two sensible options for what Y can behere: it can be a push-model notification where the GPU yells at the CPU to dosomething&nbsp;<em>right now</em>&nbsp;(“Oi!<br>CPU! I’m entering the verticalblanking interval on display 0 right now, so if you want to flip bufferswithout tearing, this would be the time to do it!”), or it can be a pull-modelthing where the GPU just memorizes that something happened and the CPU canlater<br>ask about it (“Say, GPU, what was the most recent command buffer fragmentyou started processing?” – “Let me check… sequence id 303.”). The former istypically implemented using interrupts and only used for infrequent and high-priorityevents because interrupts<br>are fairly expensive. All you need for the latter issome CPU-visible GPU registers and a way to write values into them from thecommand buffer once a certain event happens.</p><p>Say you have 16 such registers. Then you could assign currentCommandBufferSeqIdto register 0. You assign a sequence number to every command buffer you submitto the GPU (this is in the KMD), and then at the start of each command buffer,you add a “If you get<br>to this point in the command buffer, write&nbsp;&nbsp;toregister 0″. And voila, now we know which command buffer the GPU is currentlychewing on! And we know that the command processor finishes commands strictlyin sequence, so if the first command in command buffer 303<br>was executed, thatmeans all command buffers up to and including sequence id 302 are finished andcan now be reclaimed by the KMD, freed, modified, or turned into a cheesyamusement park.</p><p>We also now have an example of what X could be: “if you get here” – perhaps thesimplest example, but already useful. Other examples are “if all shaders havefinished all texture reads coming from batches before this point in the commandbuffer” (this marks safe<br>points to reclaim texture/render target memory), “ifrendering to all active render targets/UAVs has completed” (this marks pointsat which you can actually safely use them as textures), “if all operations upto this point are fully completed”, and so on.</p><p>Such operations are usually called “fences”, by the way. There’s differentmethods of picking the values you write into the status registers, but as faras I am concerned, the only sane way to do it is to use a sequential counterfor this (probably stealing some<br>of the bits for other information). Yeah, I’mreally just dropping that one piece of random information without any rationalewhatsoever here, because I think you should know. I might elaborate on it in alater blog post (though not in this series) :).</p><p>So, we got one half of it – we can now report status back from the GPU to theCPU, which allows us to do sane memory management in our drivers (notably, wecan now find out when it’s safe to actually reclaim memory used for vertexbuffers, command buffers, textures<br>and other resources). But that’s not all ofit – there’s a puzzle piece missing. What if we need to synchronize purely onthe GPU side, for example? Let’s go back to the render target example. We can’tuse that as a texture until the rendering is actually finished<br>(and some othersteps have taken place – more details on that once I get to the texturingunits). The solution is a “wait”-style instruction: “Wait until register Mcontains value N”. This can either be a compare for equality, or less-than(note you need to deal<br>with wraparounds here!), or more fancy stuff – I’m justgoing with equals for simplicity. This allows us to do the render target syncbefore we submit a batch. It also allows us to build a full GPU flushoperation: “Set register 0 to &#43;&#43;seqId if all pending jobs<br>finished” / “Waituntil register 0 contains seqId”. Done and done. GPU/GPU synchronization:solved – and until the introduction of DX11 with Compute Shaders that haveanother type of more fine-grained synchronization, this was usually the&nbsp;<em>only</em>&nbsp;synchronizationmechanism<br>you had on the GPU side. For regular rendering, you simply don’t needmore.</p><p>By the way, if you can write these registers from the CPU side, you can usethis the other way too – submit a partial command buffer including a wait for aparticular value, and then change the register from the CPU instead of the GPU.This kind of thing can be<br>used to implement D3D11-style multithreaded renderingwhere you can submit a batch that references vertex/index buffers that arestill locked on the CPU side (probably being written to by another thread). Yousimply stuff the wait just in front of the actual<br>render call, and then the CPUcan change the contents of the register once the vertex/index buffers areactually unlocked. If the GPU never got that far in the command buffer, thewait is now a no-op; if it did, it spend some (command processor) time spinninguntil<br>the data was actually there. Pretty nifty, no? Actually, you canimplement this kind of thing even without CPU-writeable status registers if youcan modify the command buffer after you submit it, as long as there’s a commandbuffer “jump” instruction. The details<br>are left to the interested reader :)</p><p>Of course, you don’t necessarily need the set register/wait register model; forGPU/GPU synchronization, you can just as simply have a “rendertarget barrier”instruction that makes sure a render target is safe to use, and a “flush everything” command. But I like<br>the set register-style model more because itkills two birds (back-reporting of in-use resources to the CPU, and GPUself-synchronization) with one well-designed stone.</p><p><img src="" alt=""></p><p><strong>Update:</strong>&nbsp;Here, I’ve drawn&nbsp;<a href="http://www.farbrausch.de/~fg/gpu/command_processor.jpg" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">a diagram</span></a>&nbsp;for you. It got a bit convoluted so I’m going to lower the amount of<br>detail in the future. The basicidea is this: The command processor has a FIFO in front, then the commanddecode logic, execution is handled by various blocks that communicate with the 2Dunit, 3D front-end (regular 3D rendering) or shader units directly (computeshaders),<br>then there’s a block that deals with sync/wait commands (which hasthe publicly visible registers I talked about), and one unit that handlescommand buffer jumps/calls (which changes the current fetch address that goesto the FIFO). And all of the units we dispatch<br>work to need to send us backcompletion events so we know when e.g. textures aren’t being used anymore andtheir memory can be reclaimed.</p><p>**Closing remarks</p><p>**Next step down is the first one doing any actual rendering work. Finally,only 3 parts into my series on GPUs, we actually start looking at some vertexdata! (No, no triangles being rasterized yet. That will take some more time).</p><p>Actually, at this stage, there’s already a fork in the pipeline; if we’rerunning compute shaders, the next step would already be … running computeshaders. But we aren’t, because compute shaders are a topic for later parts!Regular rendering pipeline first.</p><p>Small disclaimer: Again, I’m giving you the broad strokes here, going into details where it’s necessary (or interesting), but trust me, there’s a lot ofstuff that I dropped for convenience (and ease of understanding). That said, Idon’t think I left out anything<br>really important. And of course I might’vegotten some things wrong. If you find any bugs, tell me!</p><p>Until the next part…</p><p><strong>A trip through the Graphics Pipeline 2011, part 3</strong>&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; July 3, 2011&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;</p><p>At this point, we’ve sent draw calls down from our app all the way throughvarious driver layers and the command processor; now,&nbsp;<em>finally</em>&nbsp;we’reactually going to do some graphics processing on it! In this part, I’ll look atthe vertex pipeline. But before<br>we start…</p><p>**Have some Alphabet Soup!</p><p>**We’re now in the 3D pipeline proper, which in turn consists of severalstages, each of which does one particular job. I’m gonna give names to all thestages I’ll talk about – mostly sticking with the “official” D3D10/11 names forconsistency – plus the<br>corresponding acronyms. We’ll see all of theseeventually on our grand tour, but it’ll take a while (and several more parts)until we see most of them – seriously, I made a small outline of the ground Iwant to cover, and this series will keep me busy for at<br>least 2 weeks! Anyway,here goes, together with a one-sentence summary of what each stage does.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IA — Input Assembler. Reads index and vertex data.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VS — Vertex shader. Gets input vertex data, writes outprocessed vertex data for the next stage.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PA — Primitive Assembly. Reads the vertices that make upa primitive and passes them on.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HS — Hull shader; accepts patch primitives, writestransformed (or not) patch control points, inputs for the domain shader, plussome extra data that drives tessellation.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TS — Tessellator stage. Creates vertices and connectivityfor tessellated lines or triangles.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DS — Domain shader; takes shaded control points, extradata from HS and tessellated positions from TS and turns them into verticesagain.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GS — Geometry shader; inputs primitives, optionally withadjacency information, then outputs different primitives. Also the primary hubfor…</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SO — Stream-out. Writes GS output (i.e. transformed primitives)to a buffer in memory.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RS — Rasterizer. Rasterizes primitives.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PS — Pixel shader. Gets interpolated vertex data, outputspixel colors. Can also write to UAVs (unordered access views).</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OM — Output merger. Gets shaded pixels from PS, doesalpha blending and writes them back to the backbuffer.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CS — Compute shader. In its own pipeline all by itself.Only input is constant buffers&#43;thread ID; can write to buffers and UAVs.</p><p>And now that that’s out of the way, here’s a list of the various data pathsI’ll be talking about, in order: (I’ll leave out the IA, PA, RS and OM stagesin here, since for our purposes they don’t actually do anything&nbsp;<em>to</em>&nbsp;thedata, they just rearrange/reorder<br>it – i.e. they’re essentially glue)</p><p>1.&nbsp;&nbsp;&nbsp;&nbsp; VS→PS: Ye Olde ProgrammablePipeline. In D3D9, this was all you got. Still the most important path forregular rendering by far. I’ll go through this from beginning to end thendouble back to the fancier paths once I’m done.</p><p>2.&nbsp;&nbsp;&nbsp;&nbsp; VS→GS→PS: Geometry Shading(new with D3D10).</p><p>3.&nbsp;&nbsp;&nbsp;&nbsp; VS→HS→TS→DS→PS,VS→HS→TS→DS→GS→PS: Tessellation (new in D3D11).</p><p>4.&nbsp;&nbsp;&nbsp;&nbsp; VS→SO, VS→GS→SO,VS→HS→TS→DS→GS→SO: Stream-out (with and without tessellation).</p><p>5.&nbsp;&nbsp;&nbsp;&nbsp; CS: Compute. New in D3D11.</p><p>And now that you know what’s coming up, let’s get started on vertex shaders!</p><p><strong>Input Assembler stage</strong></p><p>The very first thing that happens here is loading indices from the index buffer– if it’s an indexed batch. If not, just pretend it was an identity indexbuffer (0 1 2 3 4 …) and use that as index instead. If there is an indexbuffer, its contents are read from<br>memory at this point – not directly though,the IA usually has a data cache to exploit locality of index/vertex bufferaccess. Also note that index buffer reads (in fact, all resource accesses inD3D10&#43;) are bounds checked; if you reference elements outside the<br>originalindex buffer (for example, issue a DrawIndexed with IndexCount == 6 from a5-index buffer) all out-of-bounds reads return zero. Which (in this particularcase) is completely useless, but well-defined. Similarly, you can issue aDrawIndexed with a NULL<br>index buffer set – this behaves the same way as if youhad an index buffer of size zero set, i.e. all reads are out-of-bounds andhence return zero. With D3D10&#43;, you have to work some more to get into therealm of undefined behavior. :)</p><p>Once we have the index, we have all we need to read both per-vertex andper-instance data (the current instance ID is just another counter, fairlystraightforward, at this stage anyway) from the input vertex streams. This isfairly straightforward – we have a<br>declaration of the data layout; just read itfrom the cache/memory and unpack it into the float format that our shader coreswant for input. However, this read isn’t done immediately; the hardware isrunning a cache of shaded vertices, so that if one vertex is<br>referenced bymultiple triangles (and in a fully regular closed triangle mesh, each vertexwill be referenced by about 6 tris!) it doesn’t need to be shaded every time –we just reference the shaded data that’s already there!</p><p><strong>Vertex Caching and Shading</strong>_</p><p>Note_: The contents of this section are, in part, guesswork. They’re basedon public comments made by people “in the know” about current GPUs, but thatonly gives me the “what”, not the “why”, so there’s some extrapolation here.Also, I’m simply guessing some<br>of the details here. That said, I’m not talkingcompletely out of my ass here – I’m confident that what I’m describing here isboth reasonable and works (in the general sense), I just can’t guarantee thatit’s actually that way in real HW or that I didn’t miss<br>any tricky details. :)</p><p>Anyway. For a long time (up to and including the shader model 3.0 generation ofGPUs), vertex and pixel shaders were implemented with different units that haddifferent performance trade-offs, and vertex caches were a fairly simpleaffair: usually just a FIFO<br>for a small number (think one or two dozen) ofvertices, with enough space for a worst-case number of output attributes, usingthe vertex index as a tag. As said, fairly straightforward stuff.</p><p>And then unified shaders happened. If you unify two types of shaders that usedto be different, the design is necessarily going to be a compromise. So on theone hand, you have vertex shaders, which (at that time) touched maybe up to 1million vertices a frame<br>in normal use. On the other hand you had pixelshaders, which at 1920×1200 need to touch&nbsp;<em>at least</em>&nbsp;2.3 millionpixels a frame&nbsp;<em>just to fill the whole screen once</em>&nbsp;– and a lotmore if you want to render anything interesting. So guess which of the<br>twounits ended up pulling the short straw?</p><p>Okay, so here’s the deal: instead of the vertex shader units of old that shadedmore or less one vertex at a time, you now have a huge beast of a unifiedshader unit that’s designed for maximum throughput, not latency, and hencewants large batches of work (How<br>large? Right now, the magic number seems to bebetween 16 and 64 vertices shaded in one batch).</p><p>So you need between 16-64 vertex cache misses until you can dispatch one vertexshading load, if you don’t want to shade inefficiently. But the whole FIFOthing doesn’t really play ball with this idea of batching up vertex cachemisses and shading them in one<br>go. The problem is this: if you shade a wholebatch of vertices at once, that means you can only actually start assemblingtriangles once all those vertices have finished shading. At which point you’vejust added a whole batch (let’s just say 32 here and in the<br>following) ofvertices to the end of the FIFO, which means 32 old vertices now fell out – buteach of these 32 vertices might’ve been a vertex cache hit for one of thetriangles in the current batch we’re trying to assemble! Uh oh, that doesn’twork. Clearly,<br>we can’t actually count the 32 oldest verts in the FIFO asvertex cache hits, because by the time we want to reference them they’ll begone! Also, how big do we want to make this FIFO? If we’re shading 32 verts ina batch, it needs to be at least 32 entries large,<br>but since we can’t use the32 oldest entries (since we’ll be shifting them out), that means we’lleffectively start with an empty FIFO on every batch. So, make it bigger, say 64entries? That’s pretty big. And note that every vertex cache lookup involvescomparing<br>the tag (vertex index) against all tags in the FIFO – this is fullyparallel, but it also a power hog; we’re effectively implementing a fullyassociative cache here. Also, what do we do between dispatching a shading loadof 32 vertices and receiving results –<br>just wait? This shading will take a fewhundred cycles, waiting seems like a stupid idea! Maybe have two shading loadsin flight, in parallel? But now our FIFO needs to be at least 64 entries long,and we can’t count the last 64 entries as vertex cache hits,<br>since they’ll beshifted out by the time we receive results. Also, one FIFO vs. lots of shadercores?&nbsp;<a href="http://en.wikipedia.org/wiki/Amdahl%27s_law" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Amdahl’s law</span></a>&nbsp;still holds – puttingone<br>strictly serial component in a pipeline that’s otherwise completelyparallel is a surefire way to make it the bottleneck.</p><p>This whole FIFO thing really doesn’t adapt well to this environment, so, well,just throw it out. Back to the drawing board. What do we actually want to do?Get a decently-sized batch of vertices to shade, and not shade vertices (much)more often than necessary.</p><p>So, well, keep it simple: Reserve enough buffer space for 32 vertices (=1batch), and similarly cache tag space for 32 entries. Start with an empty“cache”, i.e. all entries invalid. For every primitive in the index buffer, doa lookup on all the indices; if it’s<br>a hit in the cache, fine. If it’s a miss,allocate a slot in the current batch and add the new index to the cache tagarray. Once we don’t have enough space left to add a new primitive anymore,dispatch the whole batch for vertex shading, save the cache tag array<br>(i.e. the32 indices of the vertices we just shaded), and start setting up the nextbatch, again from an empty cache – ensuring that the batches are completelyindependent.</p><p>Each batch will keep a shader unit busy for some while (probably at least a fewhundred cycles!). But that’s no problem, because we got plenty of them – justpick a different unit to execute each batch! Presto parallelism. We’lleventually get the results back.<br>At which point we can use the saved cache tagsand the original index buffer data to assemble primitives to be sent down thepipeline (this is what “primitive assembly” does, which I’ll cover in the laterpart).</p><p>By the way, when I say “get the results back”, what does that mean? Where dothey end up? There’s two major choices: 1. specialized buffers or 2. somegeneral cache/scratchpad memory. It used to be 1), with a fixed organizationdesigned around vertex data (with<br>space for 16 float4 vectors of attributes pervertex and so on), but lately GPUs seem to be moving towards 2), i.e. “justmemory”. It’s more flexible, and has the distinct advantage that you can usethis memory for other shader stages, whereas things like specialized<br>vertexcaches are fairly useless for the pixel shading or compute pipeline, to givejust one example.</p><p>&nbsp;<img src="" alt=""></p><p><strong>Update</strong>: And here’s a&nbsp;<a href="http://www.farbrausch.de/~fg/gpu/vertex_shade.jpg" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">picture</span></a>&nbsp;of the vertex shading data flow as described so far.</p><p><strong>Shader Unit internals</strong></p><p>Short versions: It’s pretty much what you’d expect from looking at disassembledHLSL compiler output (fxc /dumpbin is your friend!). Guess what, it’s justprocessors that are&nbsp;<em>really good</em>&nbsp;at running that kind of code,and the way that kind of thing is<br>done in hardware is building something thateats something fairly close to shader bytecode, in spirit anyway. And unlikethe stuff that I’ve been talking about so far, it’s fairly well documented too– if you’re interested, just check out conference presentations<br>from AMD andNVidia or read the documentation for the CUDA/Stream SDKs.</p><p>Anyway, here’s the executive summary: fast ALU mostly built around a FMAC(Floating Multiply-ACcumulate) unit, some HW support for (at least) reciprocal,reciprocal square root, log2, exp2, sin, cos, optimized for high throughput andhigh density not low latency,<br>running a high number of threads to cover saidlatency, fairly small number of registers per thread (since you’re running somany of them!), very good at executing straight-line code, bad at branches(especially if they’re not coherent).</p><p>All that is common to pretty much all implementations. There’s somedifferences, too; AMD hardware used to stick directly with the 4-wide SIMDimplied by the HLSL/GLSL and shader bytecode (even though they seem to bemoving away from that lately), while NVidia<br>decided to rather turn the 4-waySIMD into scalar instructions a while back. Again though, all that’s on the Webalready!</p><p>What’s interesting to note though is the&nbsp;<em>differences</em>&nbsp;betweenthe various shader stages. The short version is that really are rather few ofthem; for example, all the arithmetic and logic instructions are exactly thesame across all stages. Some constructs<br>(like derivative instructions andinterpolated attributes in pixel shaders) only exist in some stages; butmostly, the differences are just what kind (and format) of data are passed inand out.</p><p>There’s one special bit related to shaders though that’s a big enough subjectto deserve a part on its own. That bit is texture sampling (and texture units).Which, it turns out, will be our topic next time! See you then.</p><p><strong>Closing remarks</strong></p><p>Again, I repeat my disclaimer from the “Vertex Caching and Shading” section:Part of that is conjecture on my part, so take it with a grain of salt. Ormaybe a pound. I don’t know.&nbsp;</p><p>I’m also not going into any detail on how scratch/cache memory is managed; thebuffer sizes depend (primarily) on the size of batches you process and thenumber of vertex output attributes you expect. Buffer sizing and management is&nbsp;_really_importantfor<br>performance, but I can’t meaningfully explain it here, nor do I want to;while interesting, this stuff is very specific to whatever hardware you’retalking about, and not really very insightful.</p><p><strong>A trip through the GraphicsPipeline 2011, part 4</strong>&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;</p><p>&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; July 4, 2011&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>Welcome back. Last part was about vertex shaders, with some coverage of GPUshader units in general. Mostly, they’re just vector processors, but they haveaccess to one resource that doesn’t exist in other vector architectures:Texture samplers. They’re an integral<br>part of the GPU pipeline and arecomplicated (and interesting!) enough to warrant their own article, so heregoes.</p><p>**Texture state</p><p>**Before we start with the actual texturing operations, let’s have a look atthe API state that drives texturing. In the D3D11 part, this is composed of 3distinct parts:</p><p>1.&nbsp;&nbsp;&nbsp;&nbsp; The sampler state. Filtermode, addressing mode, max anisotropy, stuff like that. This controls howtexture sampling is done in a general way.</p><p>2.&nbsp;&nbsp;&nbsp;&nbsp; The underlying textureresource. This boils down to a pointer to the raw texture bits in memory. Theresource also determines whether it’s a single texture or a texture array, whatmultisample format the texture has (if any), and the physical layout<br>of thetexture bits – i.e. at the resource level, it’s not yet decided how the valuesin memory are to be interpreted exactly, but their memory layout is naileddown.</p><p>3.&nbsp;&nbsp;&nbsp;&nbsp; The shader resource view (SRVfor short). This determines how the texture bits are to be interpreted by thesampler. In D3D10&#43;, the resource view links to the underlying resource, so younever specify the resource explicitly.</p><p>Most of the time, you will create a texture resource with a given format, let’ssay RGBA, 8 bits per component, and then just create a matching SRV. But youcan also create a texture as “8 bits per component, typeless” and then haveseveral different SRVs for<br>the same resource that read the underlying data indifferent formats, e.g. once as UNORM8_SRGB (unsigned 8-bit value in sRGB spacethat gets mapped to float 0..1) and once as UINT8 (unsigned 8-bit integer).&nbsp;</p><p>Creating the extra SRV seems like an annoying extra step at first, but thepoint is that this allows the API runtime to do all type checking at SRVcreation time; if you get a valid SRV back, that means the SRV and resourceformats are compatible, and no further<br>type checking needs to be done whilethat SRV exists. In other words, it’s all about API efficiency here.</p><p>Anyway, at the hardware level, what this boils down to is just a bag of stateassociated with a texture sampling operation – sampler state, texture/format touse, etc. – that needs to get kept somewhere (see&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/02/a-trip-through-the-graphics-pipeline-2011-part-2/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">part<br>2</span></a>&nbsp;for an explanation of various ways to manage statein a pipelined architecture). So again, there’s various methods, from “pipelineflush every time any state changes” to “just go completely stateless in thesampler and send the full set along with<br>every texture request”, with variousoptions inbetween. It’s nothing you need to worry about – this is the kind ofthing where HW architects whip up a cost-benefit analysis, simulate a fewworkloads and then take whichever method comes out ahead – but it’s worthrepeating:<br>as PC programmer, don’t assume the HW adheres to any particularmodel.</p><p>Don’t assume that texture switches are expensive – they might be fullypipelined with stateless texture samplers so they’re basically free. But don’tassume they’re completely free either – maybe they are not fully pipelined orthere’s a cap on the maximum number<br>of different sets of texture states in thepipeline at any given time. Unless you’re on a console with fixed hardware (oryou hand-optimize your engine for every generation of graphics HW you’retargeting), there’s just no way to tell. So when optimizing, do<br>the obviousstuff – sort by material where possible to avoid unnecessary state changes andthe like – which certainly saves you some API work at the very least, and thenleave it at that. Don’t do anything fancy based on any particular model of whatthe HW is<br>doing, because it can (and will!) change in the blink of an eyebetween HW generations.</p><p>**Anatomy of a texture request</p><p>**So, how much information do we need to send along with a texture samplerequest? It depends on the texture type and which kind of sampling instructionwe’re using. For now, let’s assume a 2D texture. What information do we need tosend if we want to do<br>a 2D texture sample with, say, up to 4x anisotropicsampling?</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The 2D texture coordinates – 2 floats, and sticking withthe D3D terminology in this series, I’m going to call them u/v and not s/t.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The partial derivatives of u and v along the screen “x”direction:&nbsp;,&nbsp;.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Similarly, we need the partial derivative in the “y”direction too:&nbsp;,&nbsp;.</p><p>So, that’s 6 floats for a fairly pedestrian 2D sampling request (of theSampleGrad variety) – probably more than you thought. The 4 gradient values areused both for mipmap selection and to choose the size and shape of theanisotropic filtering kernel. You can<br>also use texture sampling instructionsthat explicitly specify a mipmap level (in HLSL, that would be SampleLevel) –these don’t need the gradients, just a single value containing the LODparameter, but they also can’t do anisotropic filtering – the best you’ll<br>getis trilinear! Anyway, let’s stay with those 6 floats for a while. That sureseems like a lot. Do we really need to send them along with every texturerequest?</p><p>The answer is: depends. In everything but Pixel Shaders, the answer is yes, wereally have to (if we want anisotropic filtering that is). In Pixel Shaders,turns out we don’t; there’s a trick that allows Pixel Shaders to give yougradient instructions (where you<br>can compute some value and then ask thehardware “what is the approximate screen-space gradient of this value?”), andthat same trick can be employed by the texture sampler to get all the requiredpartial derivatives just from the coordinates. So for a PS 2D<br>“sample”instruction, you really only need to send the 2 coordinates which imply therest, provided you’re willing to do some more math in the sampler units.</p><p>Just for kicks: What’s the worst-case number of parameters required for asingle texture sample? In the current D3D11 pipeline, it’s a SampleGrad on aCubemap array. Let’s see the tally:</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3D texture coordinates – u, v, w: 3 floats.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cubemap array index: one int (let’s just bill that at thesame cost as a float here).</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gradient of (u,v,w) in the screen x and y directions: 6floats.</p><p>For a total of 10 values&nbsp;<em>per pixel sampled</em>&nbsp;– that’s 40 bytesif you actually store it like that. Now, you might decide that you don’t needfull 32 bits for all of this (it’s probably overkill for the array index andgradients), but it’s still a lot of<br>data to be sending around.</p><p>In fact, let’s check what kind of bandwidth we’re talking about here. Let’sassume that most of our textures are 2D (with a few cubemaps thrown in), thatmost of our texture sampling requests come from the Pixel Shader with little tono texture samples in the<br>Vertex Shader, and that the regular Sample-typerequests are the most frequent, followed by SampleLevel (all of this is prettytypical for actual rendering you see in games). That means the average numberof 32-bit floats values sent per pixel will be somewhere<br>between 2 (u&#43;v) and 3(u&#43;v&#43;w / u&#43;v&#43;lod), let’s say 2.5, or 10 bytes.</p><p>Assume a medium resolution – say, 1280×720, which is about 0.92 million pixels.How many texture samples does your average game pixel shader have? I’d say atleast 3. Let’s say we have a modest amount of overdraw, so during the 3Drendering phase, we touch each<br>pixel on the screen roughly twice. And then wefinish it off with a few texture-heavy full-screen passes to dopost-processing. That probably adds at least another 6 samples per pixel,taking into account that some of that postprocessing will be done at a lowerresolution.<br>Add it all up and we have 0.92 <em>(3</em>2 &#43; 6) = about 11 milliontexture samples per frame, which at 30 fps is about 330 million a second. At 10bytes per request, that’s 3.3 GB/s<em>just for texture request payloads</em>.Lower bound, since there’s some extra overhead<br>involved (we’ll get to that in asecond). Note that I’m <em>cough</em> erring “a bit” on the low side with all of thesenumbers :). An actual modern game on a good DX11 card will run in significantlyhigher resolution, with more complex shaders than I listed, comparable<br>amountof overdraw or even somewhat less (deferred shading/lighting to the rescue!),higher frame rate, and way more complex postprocessing – go ahead, do a quickback-of-the-envelope calculation how much texture request bandwidth adecent-quality SSAO pass in<br>quarter-resolution with bilateral upsampling takes…</p><p>Point being, this whole texture bandwidth thing is not something you can justhand-wave away. The texture samplers aren’t part of the shader cores, they’reseparate units some distance away on the chip, and shuffling multiple gigabytesper second around isn’t<br>something that just happens by itself. This is anactual architectural issue – and it’s a good thing we don’t use SampleGrad onCubemap arrays for everything :)</p><p><strong>But who asks for a single texture sample?</strong></p><p>The answer is of course:&nbsp;<em>No one</em>. Our texture requests are comingfrom shader units, which we know process somewhere between 16 and 64 pixels /vertices / control points / … at once. So our shaders won’t be sendingindividual texture samples, they’ll dispatch<br>a bunch of them at once. Thistime, I’ll use 16 as the number – simply because the 32 I chose last time isnon-square, which just seems weird when talking about 2D texture requests. So,16 texture requests at once – build that texture request payload, add somecommand<br>fields at the start so the sampler knows what to do, add some morefields so the sampler knows which texture and sampler state to use (again, seethe remarks above on state), and send that off to a texture sampler somewhere.</p><p>This will take a while.</p><p>No, seriously. Texture samplers have a seriously long pipeline (we’ll soon seewhy); a texture sampling operation takes&nbsp;_way_too long for a shaderunit to just sit idle for all that time. Again, say it with me:&nbsp;<em>throughput</em>.So what happens is that<br>on a texture sample, a shader unit will just quietlyswitch to another thread/batch and do some other work, then switch back a whilelater when the results are there. Works just fine as long as there’s enoughindependent work for the shader units to do!</p><p>**</p><p>And once the texture coordinates arrive…**</p><p>Well, there’s a bunch of computations to be done first: (In here and thefollowing, I’m assuming a simple bilinear sample; trilinear and anisotropictake some more work, see below).</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If this is a Sample or SampleBias-type request, calculatetexture coordinate gradients first.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If no explicit mip level was given, calculate the miplevel to be sampled from the gradients and add the LOD bias if specified.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each resulting sample position, apply the addressmodes (wrap / clamp / mirror etc.) to get the right position in the texture tosample from, in normalized [0,1] coordinates.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If this is a cubemap, we also need to determine whichcube face to sample from (based on the absolute values and signs of the u/v/wcoordinates), and do a division to project the coordinates onto the unit cubeso they are in the [-1,1] interval. We<br>also need to drop one of the 3coordinates (based on the cube face) and scale/bias the other 2 so they’re inthe same [0,1] normalized coordinate space we have for regular texture samples.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Next, take the [0,1] normalized coordinates and convertthem into fixed-point pixel coordinates to sample from – we need somefractional bits for the bilinear interpolation.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Finally, from the integer x/y/z and texture array index,we can now compute the address to read texels from. Hey, at this point, what’sa few more multiplies and adds among friends?</p><p>If you think it sounds bad summed up like that, let me take remind you thatthis is a simplified view. The above summary doesn’t even cover fun issues suchas texture borders or sampling cubemap edges/corners. Trust me, it may soundbad now, but if you were to<br>actually write out the code for everything thatneeds to happen here, you’d be positively horrified. Good thing we havededicated hardware to do it for us. :) Anyway, we now have a memory address toget data from. And wherever there’s memory addresses, there’s<br>a cache or twonearby.</p><p><strong>Texture cache</strong></p><p>Everyone seems to be using a two-level texture cache these days. Thesecond-level cache is a completely bog-standard cache that happens to cachememory containing texture data. The first-level cache is not quite as standard,because it’s got additional smarts.<br>It’s also smaller than you probably expect– on the order of 4-8kb per sampler. Let’s cover the size first, because ittends to come as a surprise to most people.</p><p>The thing is this: Most texture sampling is done in Pixel Shaders withmip-mapping enabled, and the mip level for sampling is specifically chosen tomake the screen pixel:texel ratio roughly 1:1 – that’s the whole point. Butthis means that, unless you happen<br>to hit the exact same location in a textureagain and again, each texture sampling operation will miss about 1 texel onaverage – the actual measured value with bilinear filtering is around 1.25misses/request (if you track pixels individually). This value stays<br>more orless unchanged for a long time even as you change texture cache size, and thendrops dramatically as soon as your texture cache is large enough to contain thewhole texture (which usually is between a few hundred kilobytes and severalmegabytes, totally<br>unrealistic sizes for a L1 cache).</p><p>Point being,&nbsp;<em>any</em>&nbsp;texture cache whatsoever is a massive win(since it drops you down from about 4 memory accesses per bilinear sample downto 1.25). But unlike with a CPU or shared memory for shader cores, there’s verylittle gain in going from say 4k<br>of cache to 16k; we’re streaming larger texturedata through the cache no matter what.</p><p>Second point: Because of the 1.25 misses/sample average, texture samplerpipelines need to be long enough to sustain a full read from memory per samplewithout stalling. Let me phrase that differently: texture sampler pipes arelong enough to not stall for a memory<br>read&nbsp;<em>even though it takes 400-800cycles</em>. That’s one seriously long pipeline right there – and it really is apipeline in the literal sense, handing data from one pipeline register to thenext for a few hundred cycles without any processing until the<br>memory read iscompleted.</p><p>So, small L1 cache, long pipeline. What about the “additional smarts”? Well,there’s compressed texture formats. The ones you see on PC – S3TC aka DXTC akaBC1-3, then BC4 and 5 which were introduced with D3D10 and are just variationson DXT, and finally BC6H<br>and 7 which were introduced with D3D11 – are allblock-based methods that encode blocks of 4×4 pixels individually. If youdecode them during texture sampling, that means you need to be able to decodeup to 4 such blocks (if your 4 bilinear sample points happen<br>to land in theworst-case configuration of straddling 4 blocks) per cycle and get a singlepixel from each. That, frankly, just sucks. So instead, the 4×4 blocks aredecoded when it’s brought into the L1 cache: in the case of BC3 (aka DXT5), youfetch one 128-bit<br>block from texture L2, and then decode that into 16 pixels inthe texture cache. And suddenly, instead of having to partially decode up to 4blocks per sample, you now only need to decode 1.25/(4*4) = about 0.08 blocksper sample, at least if your texture access<br>patterns are coherent enough to hitthe other 15 pixels you decoded alongside the one you actually asked for :).Even if you only end up using part of it before it goes out of L1 again, that’sstill a massive improvement. Nor is this technique limited to DXT<br>blocks; youcan handle most of the differences between the &gt;50 different texture formatsrequired by D3D11 in your cache fill path, which is hit about a third as oftenas the actual pixel read path – nice. For example, things like UNORM sRGBtextures can be handled<br>by converting the sRGB pixels into a 16-bitinteger/channel (or 16-bit float/channel, or even 32-bit float if you want) inthe texture cache. Filtering then operates on that, properly, in linear space.Mind that this does end up increasing the footprint of texels<br>in the L1 cache,so you might want to increase L1 texture size; not because you need to cachemore texels, but because the texels you cache are fatter. As usual, it’s atrade-off.</p><p><strong>Filtering</strong></p><p>And at this point, the actual bilinear filtering process is fairlystraightforward. Grab 4 samples from the texture cache, use the fractionalpositions to blend between them. That’s a few more of our usual standby, themultiply-accumulate unit. (Actually a lot<br>more – we’re doing this for 4channels at the same time…)</p><p>Trilinear filtering? Two bilinear samples and another linear interpolation.Just add some more multiply-accumulates to the pile.</p><p>Anisotropic filtering? Now that actually takes some extra work earlier in thepipe, roughly at the point where we originally computed the mip-level to samplefrom. What we do is look at the gradients to determine not just the area butalso the shape of a screen<br>pixel in texel space; if it’s roughly as wide as itis high, we just do a regular bilinear/trilinear sample, but if it’s elongatedin one direction, we do several samples across that line and blend the resultstogether. This generates several sample positions,<br>so we end up looping throughthe full bilinear/trilinear pipeline several times, and the actual way thesamples are placed and their relative weights are computed is a closely guardedsecret for each hardware vendor; they’ve been hacking at this problem foryears,<br>and by now both converged on something pretty damn good at reasonablehardware cost. I’m not gonna speculate what it is they’re doing; truth be told,as a graphics programmer, you just don’t need to care about the underlyinganisotropic filtering algorithm as<br>long as it’s not broken and produces eitherterrible artifacts or terrible slowdowns.</p><p>Anyway, aside from the setup and the sequencing logic to loop over the requiredsamples, this does not add a significant amount of computation to the pipe. Atthis point we have enough multiply-accumulate units to compute the weighted suminvolved in anisotropic<br>filtering without a lot of extra hardware in the actualfiltering stage. :)</p><p><strong>Texture returns</strong></p><p>And now we’re almost at the end of the texture sampler pipe. What’s the resultof all this? Up to 4 values (r, g, b, a) per texture sample requested. Unliketexture requests where there’s significant variation in the size of requests,here the most common case<br>by far is just the shader consuming all 4 values. Mindyou, sending 4 floats back is nothing to sneeze at from a bandwidth point ofview, and again you might want to shave bits in some case. If your shader issampling a 32-bit float/channel texture, you’d better<br>return 32-bit floats, butif it’s reading a 8-bit UNORM SRGB texture, 32 bit returns are just overkill,and you can save bandwidth by using a smaller format on the return path.</p><p>And that’s it – the shader unit now has its texture sampling results back andcan resume working on the batch you submitted – which concludes this part. Seeyou again in the next installment, when I talk about the work that needs to bedone before we can actually<br>start rasterizing primitives.&nbsp;<strong>Update</strong>:And here’s a&nbsp;<a href="http://www.farbrausch.de/~fg/gpu/texture_sample.jpg" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">picture</span></a>&nbsp;of the texture samplingpipeline, including an amusing<br>mistake that I’ve fixed in post like a pro!</p><p>&nbsp;</p><p>&nbsp;<img src="" alt=""></p><p>**The usual post-script</p><p>**This time, no big disclaimers. The numbers I mentioned in the bandwidthexample are honestly just made up on the spot since I couldn’t be arsed to lookup some actual figures for current games :), but other than that, what Idescribe here should be pretty<br>close to what’s on your GPU right now, eventhough I hand-waved past some of the corner cases in filtering and such (mainlybecause the details are more nauseating than they are enlightening).</p><p>As for texture L1 cache containing decompressed texture data, to the best of myknowledge this is accurate for current hardware. Some older HW would keep someformats compressed even in L1 texture cache, but because of the “1.25misses/sample for a large range<br>of cache sizes” rule, that’s not a big win andprobably not worth the complexity. I think that stuff’s all gone now.</p><p>An interesting bit are embedded/power-optimized graphics chips, e.g. PowerVR;I’ll not go into these kinds of chips much in this series since my focus hereis on the high-performance parts you see in PCs, but I have some notes aboutthem in the comments for previous<br>parts if you’re interested. Anyway, the PVRchips have their own texture compression format that’s not block-based and verytightly integrated with their filtering hardware, so I would assume that theydo keep their textures compressed even in L1 texture cache<br>(actually, I don’tknow if they even have a second cache level!). It’s an interesting method andprobably at a fairly sweet spot in terms of useful work done per area andenergy consumed. But I think the “depack to L1 cache” method gives higherthroughput overall,<br>and as I can’t mention often enough, it’s all aboutthroughput on high-end PC GPUs :)</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><strong>A trip through the GraphicsPipeline 2011, part 5</strong></p><p>&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;</p><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; July 5, 2011&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><p>After the last post about texture samplers, we’re now back in the 3D frontend.We’re done with vertex shading, so now we can start actually rendering stuff,right? Well, not quite. You see, there’s a bunch still left to do before weactually start rasterizing<br>primitives. So much so in fact that we’re not goingto see any rasterization in this post – that’ll have to wait until next time.</p><p><strong>Primitive Assembly</strong></p><p>When we left the vertex pipeline, we had just gotten a block of shaded verticesback from the shader units, with the implicit promise that this block containsan integral number of primitives – i.e., we don’t allow triangles, lines or patchesto be split across<br>multiple blocks. This is important, because it means we cantruly treat each block independently and never need to buffer more than oneblock of shader output – we can, of course, but we don’t have to.</p><p>The next step is to assemble all the vertices belonging to a single primitive(hence “primitive assembly”). If that primitive happens to be a point, thisjust reads exactly one vertex and passes it on. If it’s lines, it reads twovertices. If it’s triangles, three.<br>And so on for patches with larger numbersof control points.</p><p>In short, all that happens here is that we gather vertices. We can either dothis by reading the original index buffer and keeping a copy of our vertexindex-&gt;cache position map around (as I described), or we can store theindices for the fully expanded primitives<br>along with the shaded vertex data,which might take a bit more space for the output buffer but means we don’t haveto read the indices again here. Either way works fine.</p><p>And now we have expanded out all the vertices that make up a primitive. Inother words, we now have complete triangles, not just a bunch of vertices. Socan we rasterize them already? Not quite.</p><p><strong>Viewport culling and clipping</strong></p><p>Oh yeah, that. Yeah, I guess we’d better do that first, huh? This is one partof pipeline that really does exactly what you’d expect, pretty much the way youwould expect it too (i.e. the way it’s explained in the docs). So I’m not gonnaexplain polygon clipping<br>in general here, you can look that up in any computer graphicstextbook, although most make a terrible mess of it; if you want a goodexplanation, use Jim Blinn’s (chapter 13 of&nbsp;<a href="http://www.amazon.com/Jim-Blinns-Corner-Graphics-Pipeline/dp/1558603875" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">this<br>book</span></a>[url=]), although you probably want to pass on hisalternative [0,w] clip space these days, to avoid confusion if nothingelse.[/url]</p><p>[url=]Anyway, clipping. The short version is this: Your vertex shader returnsvertex positions on homogeneous clip space. Clip space is chosen to make theequations that describe the view frustum as simple as possible; in the case ofD3D, they are&nbsp;,&nbsp;,&nbsp;, and&nbsp;;<br>note that all the last equationreally does is exclude the homogeneous point (0,0,0,0), which is something of adegenerate case.[/url][url=]We first need to find out if the triangle ispartially or even completely outside any of these clip planes. This can be<br>donevery efficiently using[/url]<a href="http://en.wikipedia.org/wiki/Cohen%E2%80%93Sutherland" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Cohen-Sutherland</span></a>-style out-codess. You computethe clip out-code (or just clip-code) for each<br>vertex (this can be done atvertex shading time and stored along with the positions, for example). Then,for each primitive, the bitwise AND of the clip-codes will tell you all theview-frustum planes that&nbsp;<em>all</em>&nbsp;vertices in the primitive are onthe wrong<br>side of (if there’s any, that means the primitive is completelyoutside the view frustum and can be thrown away), and the bitwise OR of theclip-codes will tell you the planes that you need to clip the primitiveagainst. Given the clipcodes, all this is just<br>a few gates worth of hardware –simple stuff.</p><p>Additionally, the shaders can also generate a set of “cull distances” (atriangle will be discarded if any one cull distance for all vertices is lessthan zero), and a set of “clip distances” (which define additional clippingplanes). These get considered for<br>primitive rejection/clip testing too.</p><p>The actual clipping process, if invoked, can take one of two forms: we caneither use an actual polygon clipping algorithm (which adds extra vertices andtriangles), or we can add the clipping planes as extra edge equations to therasterizer (if that sounds like<br>gibberish to you, wait until the next partwhere I explain rasterization – it’ll ask make sense eventually). The latter ismore elegant and doesn’t require an actual polygon clipper at all, but we needto be able to handle all normalized 32-bit floating point<br>values as validvertex coordinates; there might be a trick for building a fast HW rasterizerthat does this, but it seems tricky to say the least. So I’m assuming there’san actual clipper, with all that involves (generation of extra triangles etc).This is a<br>nuisance, but it’s also very infrequent (more so than you think, I’llget to that in a second), so it’s not a big deal. Not sure if that’s specialhardware either, or if that path grabs a shader unit to do the actual clipping;depends on whether dispatching a<br>new vertex shading load at this stage isawkward or not, how big a dedicated clipping unit is, and how many of them youneed. I don’t know the answer to these questions, but at least from theperformance side of things, it doesn’t much matter: we don’t really<br>clip thatoften. That’s because we can use guard-band clipping.</p><p><strong>Guard-band clipping</strong></p><p>The name is something of a misnomer; it’s not a fancy way of doing clipping. Infact, it’s quite the opposite: a straight-forward way of not doing clipping. :)</p><p>The underlying idea is very simple: Most primitives that are partially outsidethe left, right, top and bottom clip planes don’t need to be clipped at all.Triangle rasterization on GPUs works by, in effect, scanning over the fullscreen area (or more precisely,<br>the scissor rect) and asking for every pixel:“is this pixel covered by the current triangle?” (In reality it’s a bit morecomplicated and way more efficient than that, but that’s the general idea). Andthat works just as well for triangles completely within<br>the viewport as it doesfor triangles that extend past, say, the right and top clipping planes. As longas our triangle coverage test is reliable, we don’t need to clip against theleft, right, top and bottom planes at all!</p><p>That test is usually done in integer arithmetic with some fixed precision. Andeventually, as you move say one triangle vertex further and further out, you’llget integer overflows and wrong test results. I think we can all agree that therasterizer producing<br>pixels that aren’t actually inside the triangle is, at thevery least, extremely offensive behavior and should be illegal! Which it infact is – hardware that does this is in violation of the spec.</p><p>There’s two solutions for this problem: The first is to make sure that yourtriangle tests never, ever generate the wrong results, no matter how your inputtriangle looks. If you manage that, then you don’t ever need to clip againstthe aforementioned four planes.<br>This is called “infinite guard-band” because,well, the guard-band is effectively infinite. Solution two is to clip triangleseventually, just as they’re about to go outside the safe range where therasterizer calculations can’t overflow. For example, say that<br>your rasterizerhas enough internal bits to deal with integer triangle coordinates that have&nbsp;,&nbsp;&nbsp;(note I’m using capital X and Yto denote screen-space positions; I’ll stick with this convention). You stilldo your viewport cull test (i.e. “is this triangle outside<br>the view frustum”)with the regular view planes, but only actually clip against the guard-bandclip planes which are chosen so that after the projection and viewporttransforms, the resulting coordinates are in the safe range. I guess it’s timefor an image:</p><p><img src="" alt=""></p><p>Guard-band clipping</p><p>The small white rectangle with blue outline that’s roughly in the middlerepresents our viewport, while the big salmon-colored area around it is ourguard band. It looks like a small viewport in this image, but I actually pickeda huge one so you can see anything!<br>With our -32768 .. 32767 guard-band cliprange, that viewport would be about 5500 pixels wide – yes, that’s some hugetriangles right there :). Anyway, the triangles show off some of the importantcases. The yellow triangle is the most common case – a triangle<br>that extendsoutside the viewport but not the guard band. This just gets passed straightthrough, no further processing necessary. The green triangle is fully withinthe guard band, but outside the viewport region, so it would never get here –it’s been rejected<br>above by the viewport cull. The blue triangle extendsoutside the guard-band clip region and would need to be clipped, but again it’sfully outside the viewport region and gets rejected by the viewport cull.Finally, the purple triangle extends both inside the<br>viewport and outside theguard band, and so actually needs to be clipped.</p><p>As you can see, the kinds of triangles you need to actually have to clipagainst the four side planes are pretty extreme. As said, it’s infrequent –don’t worry about it.</p><p><strong>Aside: Getting clipping right</strong></p><p>None of this should be terribly surprising; nor should it sound too difficult,at least if you’re familiar with the algorithms. But the devil’s in thedetails, always. Here’s some of the non-obvious rules the triangle clipper hasto obey in practice. If it ever<br>breaks&nbsp;<em>any</em>&nbsp;of these rules,there’s cases where it will produce cracks between adjacent triangles thatshare an edge. This isn’t allowed.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Vertex positions that are inside the view frustum must bepreserved, bit-exact, by the clipper.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Clipping an edge AB against a plane must produce the sameresults, bit-exact, as clipping the edge BA (orientation reversed) against thatplane. (This can be ensured by either making the math completely symmetric, oralways clipping an edge in the same<br>direction, say from the outside in).</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Primitives that are clipped against multiple planes mustalways clip against planes in the same order. (Either that or clip against allplanes at once)</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you use a guard band, you&nbsp;<em>must</em>&nbsp;clipagainst the guard band planes; you can’t use a guard band for some trianglesbut then clip against the original viewport planes if you actually need toclip. Again, failing to do this will cause cracks<br>– and if I remember correctlythere was actually a piece of graphics hardware in the bad old days thatshipped with this bug enshrined in silicon. Oops. :)</p><p>**</p><p>Those pesky near and far planes**</p><p>Okay, so we have a really nice quick solution for the 4 side planes, but whatabout near and far? Particularly the near plane is bothersome, since with allthe stuff that’s only slightly outside the viewport handled, that’s the planewe do most of our clipping<br>for. So what can we do? A z guard band? But howwould that work – we’re not actually rasterizing along the z axis at all! Infact, it’s just some value we interpolate over the triangle, damn!</p><p>On the plus side, though, it’s just some value we interpolate over thetriangle. And in fact the z-near test () is&nbsp;<em>really easy</em>&nbsp;to doonce you interpolate Z – it’s just the sign bit. z-far () is an extra compare though (not I’musing Z not z here, i.e.<br>these are “screen” or post-projection coordinates).But still, we’re doing Z-compares per pixel anyway (Z test!), so it’s not a bigextra expense. It depends, but doing z-clip this way is definitely an option. Andyou need to be able to skip z-near/z-far clipping<br>if you want to support thingslike NVidias ‘depth clamp’ OpenGL extension; in fact, I would argue theexistence of that extension is a pretty good hint that they’re doing this, orat least used to for a while.</p><p>So we’re down to one of the regular clip planes:&nbsp;. Can we get rid of this one too? Theanswer is yes, with a rasterization algorithm that works in homogeneouscoordinates, e.g.&nbsp;<a href="http://www.cs.unc.edu/~olano/papers/2dh-tri/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">this<br>one</span></a>. I’m not sure whetherhardware uses that one though. It’s nice an elegant, but it seems like it wouldbe hard to obey the (very strict!) D3D11 rasterization rules to the letterusing that algorithm. But maybe there’s some cool tricks that I’m not<br>aware of.Anyway, that’s about it with clipping.</p><p><strong>Projection and viewport transform</strong></p><p>Projection just takes the x, y and z coordinates and divides them by w (unlessyou’re using a homogeneous rasterizer which doesn’t actually project – but I’llignore that possibility in the following). This gives us normalized devicecoordinates, or NDCs, between<br>-1 and 1. We then apply the viewport transformwhich maps the projected x and y to pixel coordinates (which I’ll call X and Y)and the projected z into the range [0,1] (I’ll call this value Z), such that atthe z-near plane Z=0 and at the z-far plane Z=1.</p><p>At this point, we also snap pixels to fractional coordinates on the sub-pixelgrid. As of D3D11, hardware is required to have exactly 8 bits of subpixelprecision for triangle coordinates. This snapping turns some&nbsp;<em>very</em>&nbsp;thinslivers (which would otherwise<br>cause problems) into degenerate triangles (whichdon’t need to be rendered at all).</p><p><strong>Back-face and other triangle culling</strong></p><p>Once we have X and Y for all vertices, we can calculate the signed trianglearea using a cross product of the edge vectors. If the area is negative, thetriangle is wound counter-clockwise (here, negative areas correspond tocounter-clockwise because we’re now<br>in the pixel coordinate space, and in D3Dpixel space y increases downwards not upwards, so signs are inverted). If thearea is positive, it’s wound clockwise. If it’s zero, it’s degenerate anddoesn’t cover any pixels, so it can be safely culled. At this point,<br>we knowthe triangle orientation so we can do back-face culling (if enabled).</p><p>And that’s it! We’re now ready for rasterization… almost. Actually we have todo triangle setup first. But doing that requires some knowledge of howrasterization will be performed, so I’ll put that off until the next part… seeyou then!</p><p>**Final remarks</p><p>**Again, I skipped some parts and simplified others, so here’s the usualreminder that things are a bit more complicated in reality: For example, Ipretended that you just use the regular homogeneous clipping algorithm. Mostly,you do – but you can have<br>some vertex shader attributes flagged as usingscreen-space linear instead of perspective-correct interpolation. Now, theregular homogeneous clip always does perspective-correct interpolation; in thecase of screen-space linear attributes, you actually need<br>to do some extra workto make it not perspective-correct. :)</p><p>I talk about primitives some of the time, but mostly I’m just focusing ontriangles here. Points and lines aren’t hard, but let’s be honest, they’re notwhat we’re here for either. You can work out the details if you’re interested.:)</p><p>There’s tons of rasterization algorithms out there, some of which (like Olanos2DH method that I cited) allow you to skip nearly all clipping, but as Imentioned, D3D11 has very strict requirements on the triangle rasterizer sothere’s not much wiggle room for<br>HW implementations; I’m not sure if thosemethods can be tweaked to exactly follow the spec (there’s a lot of subtlepoints that I’ll cover next time). So here and in the following I’m assumingyou can’t do the ultra-sleek thing; then again, the not-quite-so-sleekapproaches<br>I’m running with have slightly less math per pixel in therasterizer, so they might win for HW implementations anyway. And of course Imight be missing the magic pixie dust right around the corner that solves allof these problems. That occurs surprisingly often<br>in graphics. If you know anawesome solution, give me a shout in the comments!</p><p>Lastly, the triangle culling I’m describing here is the bare minimum; forexample, the class of triangles that will generate zero pixels uponrasterization is much larger than just zero-area tris, and if you can find itout quickly enough (or with few enough gates),<br>you can drop the triangleimmediately and don’t need to go through triangle setup. This is the last pointwhere you can cull cheaply before going through triangle setup and at leastsome rasterization – finding other ways to early-reject tris pays offhandsomely<br>here.</p><p>&nbsp;</p><p>A trip through the Graphics Pipeline 2011, part 6</p><p>July 6, 2011</p><p>Welcome back. This time we’re actually gonna see triangles being rasterized –finally! But before we can rasterize triangles, we need to do triangle setup,and before I can discuss triangle setup, I need to explain what we’re settingthings up for; in other words,<br>let’s talk hardware-friendly trianglerasterization algorithms.</p><p><strong>How not to render a triangle</strong></p><p>First, a little heads-up to people who’ve been at this game long enough to havewritten their own optimized software texture mappers: First, you’re probablyused to thinking of triangle rasterizers as this amalgamated blob that does abunch of things at once:<br>trace the triangle shape, interpolate u and vcoordinates (or, for perspective correct mapping, u/z, v/z and 1/z), do theZ-buffer test (and for perspective correct mapping, you probably used a 1/zbuffer instead), and then do the actual texturing (plus shading),<br>all in onebig loop that’s meticulously scheduled and probably uses all availableregisters. You know the kind of thing I’m talking about, right? Yeah, forgetabout that here. This is hardware. In hardware, you package things up into nicetidy little modules that<br>are easy to design and test in isolation. In hardware,the “triangle rasterizer” is a block that tells you what (sub-)pixels atriangle covers; in some cases, it’ll also give you barycentric coordinates ofthose pixels inside the triangle. But that’s it. No u’s<br>or v’s – not even1/z’s. And certainly no texturing and shading, through with the dedicated textureand shader units that should hardly come as a surprise.</p><p>Second, if you’ve written your own triangle mappers “back in the day”, youprobably used an incremental scanline rasterizer of the kind described in ChrisHecker’s series on Perspective Texture Mapping. That happens to be a great wayto do it in sofware on processors<br>without SIMD units, but it doesn’t map wellto modern processors with fast SIMD units, and even worse to hardware – notthat it’s stopped people from trying. In particular, there’s a certain datedgame console standing in the corner trying very hard to look nonchalant<br>rightnow. The one with that triangle rasterizer that had really fast guard-bandclipping on the bottom and right edges of the screen, and not so fastguard-band clipping for the top and left edges (that, my friends, is what wecall a “tell”). Just saying.</p><p>So, what’s bad about that algorithm for hardware? First, it really rasterizestriangles scan-line by scan-line. For reasons that will become obvious once Iget to Pixel Shading, we want our rasterizer to output in groups of 2×2 pixels(so-called “quads” – not<br>to be confused with the “quad” primitive that’s beendecomposed into a pair of triangles at this stage in the pipeline). This is allkinds of awkward with the scan-line algorithm because not only do we now needto run two “instances” of it in parallel, they also<br>each start at the firstpixel covered by the triangle in their respective scan lines, which may bepretty far apart and doesn’t nicely lead to generating the 2×2 quads we’d liketo get. It’s also hard to parallelize efficiently, not symmetrical in the x andy<br>directions – which means a triangle that’s 8 pixels wide and 100 pixelsstresses very different parts of the rasterizer than a triangle that’s 100 pixelswide and 8 pixels high. Really annoying because now you have to make the “x”and “y” stepping “loops” equally<br>fast in order to avoid bottlenecks – but we doall our work on the “y” steps, the loop in “x” is trivial! As said, it’s amess.</p><p><strong>A better way</strong></p><p>A much simpler (and more hardware-friendly) way to rasterize triangles waspresented in a 1988 paper by Pineda. The general approach can be summarized in2 sentences: the signed distance to a line can be computed with a 2D dotproduct (plus an add) – just as a<br>signed distance to a plane can be computewith a 3D dot product (plus add). And the interior of a triangle can be definedas the set of all points that are on the correct side of all three edges. So…just loop over all candidate pixels and test whether they’re<br>actually insidethe triangle. That’s it. That’s the basic algorithm.</p><p>Note that when we move e.g. one pixel to the right, we add one to X and leave Ythe same. Our edge equations have the form E(X,Y) = aX &#43; bY &#43; c, with a, b, cbeing per-triangle constants, so for X&#43;1 it will be E(X&#43;1,Y) = a(X&#43;1) &#43; bY &#43; c= E(X,Y) &#43; a. In other<br>words, once you have the values of the edge equations ata given point, the values of the edge equations for adjacent pixels are just afew adds away. Also note that this is absolutely trivial to parallelize: sayyou want to rasterize 8×8 = 64 pixels at once,<br>as AMD hardware likes to do (orat least the Xbox 360 does, according to the 3rd edition of Real-timeRendering). Well, you just compute ia &#43; jb for 0 \le i, j \le 7 once for eachtriangle (and edge) and keep that in registers; then, to rasterize a 8×8 blockof<br>pixels, you just compute the 3 edge equation for the top-left corner, fireoff 8×8 parallel adds of the constants we’ve just computed, and then test theresulting sign bits to see whether each of the 8×8 pixels is inside or outsidethat edge. Do that for 3 edges,<br>and presto, one 8×8 block of a trianglerasterized in a truly embarrassingly parallel fashion, and with nothing morecomplicated than a bunch of integer adders! And by the way, this is why there’ssnapping to a fixed-point grid in the previous part – so we can<br>use integermath here. Integer adders are much, much simpler than any floating-point mathunit. And of course we can choose the width of the adders just right to supportthe viewport sizes we want, with sufficient subpixel precision, and probably a2x-4x factor<br>on top of that so we get a decently-sized guard band.</p><p>By the way, there’s another thorny bit here, which is fill rules; you need tohave tie-breaking rules to ensure that for any pair of triangles sharing anedge, no pixel near that edge will ever be skipped or rasterized twice. D3D andOpenGL both use the so-called<br>“top-left” fill rule; the details are explainedin the respective manuals. I won’t talk about it here except to note that withthis kind of integer rasterizer, it boils down to subtracting 1 from theconstant term on some edges during triangle setup. That makes<br>it guaranteedwatertight, no fuss at all – compare with the kind of contortions Chris has togo through in his article to make this work properly! Sometimes things justcome together beautifully.</p><p>We have a problem though: How do we find out which 8×8 blocks of pixels to testagainst? Pineda mentions two strategies: 1) just scanning over the wholebounding box of the triangle, or 2) a smarter scheme that stops to “turnaround” once it notices that it didn’t<br>hit any triangle samples anymore. Well,that’s just fine if you’re testing one pixel at a time. But we’re doing 8×8pixels now! Doing 64 parallel adds only to find out at the very end thatexactly none of them hit any pixels whatsoever is a lot of wasted work.<br>So…don’t do that!</p><p><strong>What we need around here is more hierarchy</strong></p><p>What I’ve just described is what the “fine” rasterizer does (the one thatactually outputs sample coverage). Now, to avoid wasted work at the pixellevel, what we do is add another rasterizer in front of it that doesn’trasterize the triangle into pixels, but<br>“tiles” – our 8×8 blocks (This paper byMcCormack and McNamara has some details, as does Greene’s “Hierarchical PolygonTiling with Coverage Masks” that takes the idea to its logical conclusion).Rasterizing edge equations into covered tiles works very similarly<br>torasterizing pixels; what we do is compute lower and upper bounds for the edgeequations over full tiles; since the edge equations are linear, such extremaoccur on the boundary of the tile – in fact, it’s enough to loop at the 4corner points, and from the<br>signs of the ‘a’ and ‘b’ terms in the edgeequation, we can determine which corner. Bottom line, it’s really not much moreexpensive than what we already discussed, and needs exactly the same machinery– a few parallel integer adders. As a bonus, if we evaluate<br>the edge equationsat one corner of the tile anyway, we might as well just pass that through tothe fine rasterizer: it needs one reference value per 8×8 block, remember? Verynice.</p><p>So what we do now is run a “coarse” rasterizer first that tells us which tilesmight be covered by the triangle. This rasterizer can be made smaller (8×8 atthis level really seems like overkill!), and it doesn’t need to be as fast(because it’s only run for each<br>8×8 block). In other words, at this level, thecost of discovering empty blocks is correspondingly lower.</p><p>We can think this idea further, as in Greene’s paper or Mike Abrash’sdescription of Rasterization on Larrabee, and do a full hierarchicalrasterizer. But with a hardware rasterizer, there’s little to no point: itactually increases the amount of work done for<br>small triangles (unless you canskip levels of the hierarchy, but that’s not how you design HW dataflows!), andif you have a triangle that’s large enough to actually produce significantrasterization work, the architecture I describe should already be fast enoughto<br>generate pixel locations faster than the shader units can consume them.</p><p>In fact, the actual problem here isn’t big triangles in the first place; theyare easy to deal with efficiently for pretty much any algorithm (certainlyincluding scan-line rasterizers). The problem is small triangles! Even if youhave a bunch of tiny triangles<br>that generate 0 or 1 visible pixels, you stillneed to go through triangle setup (that I still haven’t described, but we’regetting close), at least one step of coarse rasterization, and then at leastone fine rasterization step for an 8×8 block. With tiny triangles,<br>it’s easy toget either triangle setup or coarse rasterization bound.</p><p>One thing to note is that with this kind of algorithm, slivers (long, very thintriangles) are seriously bad news – you need to traverse tons of tiles and onlyget very few covered pixels for each of them. So, well, they’re slow. Avoidthem when you can.</p><p><strong>So what does triangle setup do?</strong></p><p>Well, now that I’ve described what the rasterization algorithm is, we just needto look what per-edge constants we used throughout; that’s exactly what we needto set up during triangle setup.</p><p>In our case, the list is this:</p><p>&nbsp; &nbsp; The edge equations – a, b, c for all 3 triangle edges.</p><p>&nbsp; &nbsp; Some of the derived values, like the ia &#43; jb for 0 \le i, j \le 7that I mentioned; note that you wouldn’t actually store a full 8×8 matrix ofthese in hardware, certainly not if you’re gonna add another value to itanyway. The best way to do this is in HW<br>probably to just compute the ia andjb, use a Carry-save adder (aka 3:2 reducer, I wrote about them before) toreduce the ia &#43; jb &#43; c expression to a single sum, and then finish that offwith a regular adder. Or something similar, anyway.</p><p>&nbsp; &nbsp; Which reference corner of the tiles to use to get the upper/lowerbounds of the edge equations for coarse rasterizer.</p><p>&nbsp; &nbsp; The initial value of the edge equations at the first referencepoint for the coarse rasterizer (adjusted for fill rule).</p><p>…so that’s what triangle setup computes. It boils down to several large integermultiplies for the edge equations and their initial values, a few smallermultiplies for the step values, and some cheap combinatorial logic for therest.</p><p><strong>Other rasterization issues and pixel output</strong></p><p>One thing I didn’t mention so far is the scissor rect. That’s just ascreen-aligned rectangle that masks pixels; no pixel outside that rect will begenerated by the rasterizer. This is fairly easy to implement – the coarserasterizer can just reject tiles that<br>don’t overlap the scissor rect outright,and the fine rasterizer ANDs all generated coverage masks with the “rasterized”scissor rectangle (where “rasterization” here boils down to a one integercompare per row and column and some bitwise ANDs). Simple stuff,<br>moving on.</p><p>Another issue is multisample antialiasing. What changes is now you have to testmore samples per pixel – as of DX11, HW needs to support at least 8x MSAA. Notethat the sample locations inside each pixel aren’t on a regular grid (which isbadly behaved for near-horizontal<br>or near-vertical edges), but dispersed togive good results across a wide range of multiple edge orientations. Theseirregular sample locations are a total pain to deal with in a scanlinerasterizer (another reason not to use them!) but very easy to support in<br>aPineda-style algorithm: it boils down to computing a few more per-edge offsetsin triangle setup and multiple additions/sign tests per pixel instead of justone.</p><p>For, say 4x MSAA, you can do two things in an 8×8 rasterizer: you can treateach sample as a distinct “pixel”, which means your effective tile size is now4×4 actual screen pixels after the MSAA resolve and each block of 2×2 locationsin the fine rast grid now<br>corresponds to one pixel after resolve, or you canstick with 8×8 actual pixels and just run through it four times. 8×8 seems abit large to me, so I’m assuming that AMD does the former. Other MSAA levelswork analogously.</p><p>Anyway, we now have a fine rasterizer that gives us locations of 8×8 blocks plusa coverage mask in each block. Great, but it’s just half of the story – currenthardware also does early Z and hierarchical Z testing (if possible) beforerunning pixel shaders, and<br>the Z processing is interwoven with actualrasterization. But for didactic reasons it seemed better to split this up; soin the next part, I’ll be talking about the various types of Z processing, Zcompression, and some more triangle setup – so far we’ve just<br>covered setup forrasterization, but there’s also various interpolated quantities we want for Zand pixel shading, and they need to be set up too! Until then.</p><p><strong>Caveats</strong></p><p>I’ve linked to a few rasterization algorithms that I think are representativeof various approaches (they also happen to be all on the Web). There’s a lot more.I didn’t even try to give you a comprehensive introduction into the subjecthere; that would be a (lengthy!)<br>serious of posts on its own – and rather dullafter a fashion, I fear.</p><p>Another implicit assumption in this article (I’ve stated this multiple times,but this is one of the places to remind you) is that we’re on high-end PChardware; a lot of parts, particularly in the mobile/embedded range, areso-called tile renderers, which partition<br>the screen into tiles and render eachof them individually. These are not the same as the 8×8 tiles for rasterizationI used throughout this article. Tiled renderes need at least another“ultra-coarse” rasterization stage that runs early and finds out which of<br>the(large) tiles are covered by each triangle; this stage is usually called“binning”. Tiled renderers work differently and have different designparameters than the “sort-last” architectures (that’s the official name) Idescribe here. When I’m done with the<br>D3D11 pipeline (and that’s still a waysoff!) I might throw in a post or two on tiled renderers (if there’s interest),but right now I’m just ignoring them, so be advised that e.g. the PowerVR chipsyou so often find in smartphones handle some of this differently.</p><p>The 8×8 blocking (other block sizes have the same problem) means that trianglessmaller than a certain size, or with inconvenient aspect ratios, take a lotmore rasterization work than you would think, and get crappy utilization duringthe process. I’d love to<br>be able to tell you that there’s a magic algorithmthat’s easy to parallelize and good with slivers and the like, but if there isI don’t know it, and since there’s still regular reminders by the HW vendorsthat slivers are bad, apparently neither do they. So<br>for the time being, thisjust seems to be a fact of life with HW rasterization. Maybe someone will comeup with a great solution for this eventually.</p><p>The “edge function lower bound” thing I described for coarse rast works fine,but generates false positives in certain cases (false positives in the sensethat it asks for fine rasterization in blocks that don’t actually cover anypixels). There’s tricks to reduce<br>this, but again, detecting some of the rarercases is trickier / more expensive than just rasterizing the occasional fine blockthat doesn’t have any pixels lit. Another trade-off.</p><p>Finally the blocks used during rasterization are often snapped on a grid (whythat would help will become clearer in the next part). If that’s the case, evena triangle that just covers 2 pixels might straddle 2 tiles and make yourasterize two 8×8 blocks. More<br>inefficiency.</p><p>The point is this: Yes, all this is fairly simple and elegant, but it’s notperfect, and actual rasterization for actual triangles is nowhere neartheoretical peak rasterization rates (which always assume that all of the fineblocks are completely filled). Keep<br>that in mind.</p><p>&nbsp;</p><p><span style="color:#999">本帖最后由</span><span style="color:#999"> iceworld</span><br><span style="color:#999">于</span><span style="color:#999"> 2012-2-23 02:30</span><span style="color:#999">编辑</span></p><p>A trip through the Graphics Pipeline 2011, part 7</p><p>July 8, 2011</p><p>In this installment, I’ll be talking about the (early) Z pipeline and how itinteracts with rasterization. Like the last part, the text won’t proceed inactual pipeline order; again, I’ll describe the underlying algorithms first,and then fill in the pipeline<br>stages (in reverse order, because that’s theeasiest way to explain it) after the fact.</p><p><strong>Interpolated values</strong></p><p>Z is interpolated across the triangle, as are all the attributes output by thevertex shader. So let me take a minute to explain how that works. At this pointI originally had a section on how the math behind interpolation is derived, andwhy perspective interpolation<br>works the way it works. I struggled with that forhours, because I was trying to limit it to maybe one or two paragraphs (sinceit’s an aside), and what I can say now is that if I want to explain itproperly, I need more space than that, and at least one or two<br>pictures; apicture may say more than thousand words, but a nice diagram takes me about aslong to prepare as a thousand words of text, so that’s not necessarily a winfrom my perspective :). Anyway, this is something of a tangent anyway, so I’madding it to my<br>pile of “graphics-related things to write up properly at somepoint”. For now, I’m giving you the executive summary:</p><p>Just linearly interpolating attributes (colors, texture coordinates etc.)across the screen-space triangle does not produce the right results (unless theinterpolation mode is one of the “no perspective” ones, in which case ignorewhat I just wrote). However,<br>say we want to interpolate a 2D texture coordinatepair (s,t). It turns out you do get the right results if you linearlyinterpolate \frac{1}{w}, \frac{s}{w} and \frac{t}{w} in screen-space (w here isthe homogeneous clip-space w from the vertex position), then<br>per-pixel take thereciprocal of \frac{1}{w} to get w, and finally multiply the other twointerpolated fractions by w to get s and t. The actual linear interpolationboils down to setting up a plane equation and then plugging the screen-spacecoordinates in. And<br>if you’re writing a software perspective texture mapper,that’s the end of it. But if you’re interpolating more than two values, abetter approach is to compute (using perspective interpolation) barycentriccoordinates – let’s call them \lambda_0 and \lambda_1<br>– for the current pixelin the original clip-space triangle, after which you can interpolate the actualvertex attributes using regular linear interpolation without having to multiplyeverything by w afterwards.</p><p>So how much work does that add to triangle setup? Setting up the \frac{\lambda_0}{w}and \frac{\lambda_1}{w} for the triangle requires 4 reciprocals, the trianglearea (which we already computed for back-face culling!), and a fewsubtractions, multiplies and adds.<br>Setting up the vertex attributes forinterpolation is really cheap with the barycentric approach – two subtractionsper attribute (if you don’t use barycentric, you get some more multiply-addaction here). Follow me? Probably not, unless you’ve implemented this<br>before.Sorry about that – but it’s fairly safe to ignore all this if you don’tunderstand it.</p><p>Let’s get back to why we’re here: the one value we want to interpolate rightnow is Z, and because we computed Z as \frac{z}{w} at the vertex level as partof projection (see previous part), so it’s already divided by w and we can justinterpolate it linearly<br>in screen space. Nice. What we end up with is a planeequation for Z = aX &#43; bY &#43; c that we can just plug X and Y into to get a value.So, here’s the punchline of my furious hand-waving in the last few paragraphs:Interpolating Z at any given point boils down<br>to two multiply-adds. (Startingto see why GPUs have fast multiply-accumulate units? This stuff is absolutelyeverywhere!).</p><p><strong>Early Z/Stencil</strong></p><p>Now, if you believe the place that graphics APIs traditionally put Z/Stencilprocessing into – right before alpha blend, way at the bottom of the pixelpipeline – you might be confused a bit. Why am I even discussing Z at the pointin the pipeline where we are<br>right now? We haven’t even started shading pixels!The answer is simple: the Z and stencil tests reject pixels. Potentially themajority of them. You really, really don’t want to completely shade a detailedmesh with complicated materials, to then throw away<br>95% of the work you justdid because that mesh happens to be mostly hidden behind a wall. That’s just areally stupid waste of bandwidth, processing power and energy. And in mostcases, it’s completely unnecessary: most shaders don’t do anything that wouldinfluence<br>the results of the Z test, or the values written back to theZ/stencil buffers.</p><p>So what GPUs actually do when they can is called “early Z” (as opposed to lateZ, which is actually at the late stage in the pipeline that traditional APImodels generally display it at). This does exactly what it sounds like – executethe Z/stencil tests and<br>writes early, right after the triangle has beenrasterized, and before we start sending off pixels to the shaders. That way, wenotice all the rejected pixels early, without wasting a lot of computation onthem. However, we can’t always do this: the pixel shader<br>may ignore theinterpolated depth value, and instead provide its own depth to be written tothe Z-buffer (e.g. depth sprites); or it might use discard, alpha test, oralpha-to-coverage, all of which “kill” pixels/samples during pixel shaderexecution and mean<br>that we can’t update the Z-buffer or stencil buffer earlybecause we might be updating depth values for samples that later get discardedin the shader!</p><p>So GPUs actually have two copies of the Z/stencil logic; one right after therasterizer and in front of the pixel shader (which does early Z) and one afterthe shader (which does late Z). Note that we can still, in principle, do thedepth testing in the early-Z<br>stage even if the shader uses some of thesample-killing mechanism. It’s only writes that we have to be careful with. Theonly case that really precludes us from doing any early Z-testing at all iswhen we write the output depth in the pixel shader – in that<br>case the early Zunit simply has nothing to work with.</p><p>Traditionally, APIs just pretended none of this early-out logic existed;Z/Stencil was in a late stage in the original API model, and any optimizationssuch as early-Z had to be done in a way that was 100% functionally consistentwith that model; i.e. drivers<br>had to detect when early-Z was applicable, andcould only turn it on when there were no observable differences. By now APIshave closed that gap; as of DX11, shaders can be declared as “force early-Z”,which means they run with full early-Z processing even when<br>the shader usesprimitives that aren’t necessarily “safe” for early-Z, and shaders that writedepth can declare that the interpolated Z value is conservative (i.e. early Zreject can still happen).</p><p><strong>Z/stencil writes: the full truth</strong></p><p>Okay, wait. As I’ve described it, we now have two parts in the pipeline – earlyZ and late Z – that can both write to the Z/stencil buffers. For any givenshader/render state combination that we look at, this will work – in the steadystate. But that’s not how<br>it works in practice. What actually happens is thatwe render a few hundred to a few thousand batches per frame, switching shadersand render state regularly. Most of these shaders will allow early Z, but somewon’t. Switching from a shader that does early Z<br>to one that does late Z is noproblem. But going back from late Z to early Z is, if early Z does any writes:early Z is, well, earlier in the pipeline than early Z – that’s the wholepoint! So we may start early-Z processing for one shader, merrily writing to<br>thedepth buffer while there’s still stuff down in the pipeline for our old shaderthat’s running late-Z and may be trying to write the same location at the sametime – classic race condition. So how do we fix this? There’s a bunch ofoptions:</p><p>&nbsp; &nbsp; Once you go from early-Z to late-Z processing within a frame (orat least a sequence of operations for the same render target), you stay atlate-Z until the next point where you flush the pipeline anyway. This works butpotentially wastes lots of shader cycles<br>while early-Z is unnecessarily off.</p><p>&nbsp; &nbsp; Trigger a (pixel) pipeline flush when going from a late-Z shaderto an early-Z shader – also works, also not exactly subtle. This time, we don’twaste shader cycles (or memory bandwidth) but stall instead – not much of animprovement.</p><p>&nbsp; &nbsp; But in practice, having Z-writes in two places is just bad news.Another option is to not ever write Z in the early-Z phase; always do theZ-writes in late-Z. Note that you need to be careful to make conservativeZ-testing decisions during early Z if you do<br>this! This avoids the racecondition but means the early Z-test results may be stale because the Z-writefor the currently-dispatched pixels won’t happen until a while later.</p><p>&nbsp; &nbsp; Use a separate unit that keeps track of Z-writes for us andenforces the correct ordering; both early-Z and late-Z must go through thisunit.</p><p>All of these methods work, and all have their own advantages and drawbacks.Again I’m not sure what current hardware does in these cases, but I have strongreason to believe that it’s one of the last two options. In particular, we’llmeet a functional unit later<br>down the road (and the pipeline) that would be agood place to implement the last option.</p><p>But we’re still doing all this testing per pixel. Can’t we do better?</p><p><strong>Hierarchical Z/Stencil</strong></p><p>The idea here is that we can use our tile trick from rasterization again, andtry to Z-reject whole tiles at a time, before we even descend down to the pixellevel! What we do here is a strictly conservative test; it may tell us that“there might be pixels that<br>pass the Z/stencil-test in this tile” when thereare none, but it will never claim that all pixels are rejected when in factthey weren’t.</p><p>Assume here that we’re using “less”, “less-equal”, or “equal” as Z-comparemode. Then we need to store the maximum Z-value we’ve written for that tile,per tile. When rasterizing a triangle, we calculate the minimum Z-value theactive triangle is going to write<br>to the current tile (one easy conservativeapproximation is to take the min of the interpolated Z-values at the fourcorners of the current tile). If our triangle minimum-Z is larger than thestored maximum-Z for the current tile, the triangle is guaranteed to<br>becompletely occluded. That means we now need to track maximum-Z per-tile, andkeep that value up to date as we write new pixels – though again, it’s fine ifthat information isn’t completely up to date; since our Z-test is of the “less”variety, values in the<br>Z buffer will only get smaller over time. If we use aper-tile maximum-Z that’s a bit out of date, it just means we’ll get slightlyworse early rejection rates than we could; it doesn’t cause any other problems.</p><p>The same thing works (with min/max and compare directions swapped) if we’reusing one of the “greater”, “greater-equal” or “equal” Z-tests. What we can’teasily do is change from one of the “less”-based tests to a “greater”-basedtests in the middle of the frame,<br>because that would make the information we’vebeen tracking useless (for less-based tests we need maximum-Z per tile, forgreater-based tests we need minimum-Z per tile). We’d need to loop over thewhole depth buffer to recompute min/max for all tiles, but what<br>GPUs actuallydo is turn hierarchical-Z off once you do this (up until the next Clear). So:don’t do that.</p><p>Similar to the hierarchical-Z logic I’ve described, current GPUs also havehierarchical stencil processing. However, unlike hierarchical-Z, I haven’t seenmuch in the way of published literature on the subject (meaning, I haven’t runinto it – there might be papers<br>on it, but I’m not aware of them); as a gameconsole developer you get access to low-level GPU docs which include adescription of the underlying algorithms, but frankly, I’m definitely notcomfortable writing about something here where really the only good sources<br>Ihave are various GPU docs that came with a thick stack of NDAs. Instead I’lljust nebulously note that there’s magic pixie dust that can do certain kinds ofstencil testing very efficiently under controlled circumstances, and leave youto ponder what that might<br>be and how it might work, in the unlikely case thatyou deeply care about this – presumably because your father was killed by ahierarchical stencil unit and you’re now collecting information on its weakpoints for your revenge, or something like that.</p><p><strong>Putting it all together</strong></p><p>Okay, we now have all the algorithms and theory we need – let’s see how we cantake our new set of toys and wire it up with what we already have!</p><p>First off, we now need to do some extra triangle setup for Z/attributeinterpolation. Not much to be done about it – more work for triangle setup;that’s how it goes. After that’s coarse rasterization, which I’ve discussed inthe previous part.</p><p>Then there’s hierarchical Z (I’m assuming less-style comparisons here). We wantto run this between coarse and fine rasterization. First, we need the logic tocompute the minimum Z estimates for each tile. We also need to store theper-tile maximum Zs, which don’t<br>need to be exact: we can shave bits as long aswe always round up! As usual, there’s a trade-off here between space used andearly-rejection efficiency. In theory, you could put the Z-max info intoregular memory. In practice, I don’t think anyone does this,<br>because you wantto make the hierarchical-Z decision without a ton of extra latency. The otheroption is to put dedicated memory for hierarchical Z onto the chip – usually asSRAM, the kind of memory you also make caches out of. For 24-bit Z, youprobably need<br>something like 10-14 bits per tile to store a reasonable-accuracyZ-max in a compact encoding. Assuming 8×8 tiles, that means less than 1MBit(128k) of SRAM to support resolutions up to 2048×2048 – sounds like a plausibleorder of magnitude to me. Note that these<br>things are fixed size and shared forthe whole chip; if you do a context switch, you lose. If you allocate the wrongdepth buffers to this memory, you can’t use hierarchical Z on the depth buffersthat actually matter, and you lose. That’s just how it goes. This<br>kind ofthings is why hardware vendors regularly tell you to create your most importantrender targets and depth buffers first; they have a limited supply of this typeof memory (there’s more like it, as you’ll see), and when it runs out, you’reout of luck. Note<br>they don’t necessarily need to do this all-or-nothing; forexample, if you have a really large depth buffer, you might only gethierarchical Z in the top left 2048×1536 pixels, because that’s how much fitsinto the Z-max memory. It’s not ideal, but still much<br>better than disablinghierarchical-Z outright.</p><p>And by the way, “Real-Time Rendering” mentions at this point that “it is likelythat GPUs are using hierarchical Z-buffers with more than two levels”. I doubtthis is true, for the same reason that I doubt they use a multilevelhierarchical rasterizer: adding<br>more levels makes the easy cases (largetriangles) even faster while adding latency and useless work for smalltriangles: if you’re drawing a triangle that fits inside a single 8×8 tile, anycoarser hierarchy level is pure overhead, because even at the 8×8 level,<br>you’djust do one test to trivial-reject the triangle (or not). And again, forhardware, it’s not that big a performance issue; as long as you’re notconsuming extra bandwidth or other scarce resources, doing more compute workthan strictly necessary isn’t a big<br>problem, as long as it’s within reasonablelimits,</p><p>Hierarchical stencil is also there and should also happen prior to fine rast,most likely in parallel with hierarchical Z. We’ve established that this runson air, love and magic pixie dust, so it doesn’t need any actual hardware andis probably always exactly<br>right in its predictions. Ahem. Moving on.</p><p>After that is fine rasterization, followed in turn by early Z. And for early Z,there’s two more important points I need to make.</p><p><strong>Revenge of the API order</strong></p><p>For the past few parts, I’ve been playing fast and loose with the order thatprimitives are submitted in. So far, it didn’t matter; not for vertex shading,nor primitive assembly, triangle setup or rasterization. But Z is different.For Z-compare modes like “less”<br>or “lessequal”, it’s very important what orderthe pixels arrive in; if we mess with that, we risk changing the results andintroducing nondeterministic behavior. More importantly, as per the spec, we’refree to execute operations in any order so long as it isn’t<br>visible to the app;well, as I just said, for Z processing, order is important, so we need to makesure that triangles arrive at Z processing in the right order (this goes forboth early and late Z).</p><p>What we do in cases like this is go back in the pipeline and look for areasonable spot to sort things into order again. In our current path, the bestcandidate location seems to be primitive assembly; so when we start assemblingprimitives from shaded vertex<br>blocks, we make sure to assemble them strictly inthe original order as submitted by the app to the API. This means we mightstall a bit more (if the PA buffer holds an output vertex block, but it’s notthe correct one, we need to wait and can’t start setting<br>up primitives yet),but that’s the price of correctness.</p><p><strong>Memory bandwidth and Z compression</strong></p><p>The second big point is that Z/Stencil is a serious bandwidth hog. This has acouple of reasons. For one, this is the one thing we really run for all samplesgenerated by the rasterizer (assuming Z/Stencil isn’t off, of course). Shaders,blending etc. all benefit<br>from the early rejection we do; but even Z-rejectedpixels do a Z-buffer read first (unless they were killed by hierarchical Z). That’sjust how it works. The other big reason is that, when multisampling is enabled,the Z/stencil buffer is per sample; so 4x MSAA<br>means 4x the memory bandwidthcost of Z? For something that takes a substantial amount of memory bandwidtheven at no MSAA, that’s seriously bad news.</p><p>So what GPUs do is Z compression. There’s various approaches, but the generalidea is always the same: assuming reasonably-sized triangles, we expect a lotof tiles to just contain one or maybe two triangles. If that happens, theninstead of storing Z-values for<br>the whole tile, we just store the planeequation of the triangle that filled up this tile. That plane equation is(hopefully) smaller than the actual Z data. Without MSAA, one tile covers 8×8actual pixels, so triangles need to be relatively big to cover a full<br>tile; butwith 4x MSAA, a tile effectively shrinks to 4×4 pixels, and covering full tilesgets easier. There’s also extensions that can support 2 triangles etc., but forreasonably-sized tiles, you can’t go much larger than 2-3 tris and stillactually save bandwidth:<br>the extra plane equations and coverage masks aren’tfree!</p><p>Anyway, point is: this compression, when it works, is fully lossless, but it’snot applicable to all tiles. So we need some extra space to denote whether atile is compressed or not. We could store this in regular memory, but thatwould mean we now need to wait<br>two full memory round-trips latencies to do aZ-read. That’s bad. So again, we add some dedicated SRAM that allows us tostore a few (1-3) bits per tile. At its simplest, it’s just a “compressed” or“not compressed” flag, but you can get fancy and add multiple<br>compression modesand such. A nice side effect of Z-compression is that it allows us to do fastZ-clears: e.g. when clearing to Z=1, we just set all tiles to “compressed” andstore the plane equation for a constant Z=1 triangle.</p><p>All of the Z-compression thing, much like texture compression in the texturesamplers, can be folded into memory access/caching logic, and made completelytransparent to everyone else. If you don’t want to send the plane equations (oradd the interpolator logic)<br>to the Z memory access block, it can just inferthem from the Z data and use some integer delta-coding scheme. This kind ofapproach usually needs extra bits per sample to actually allow losslessreconstruction, but it can lead to simpler data paths and nicer<br>interfacebetween units, which hardware guys love.</p><p>And that’s it for today! Next up: Pixel shading and what happens around it.</p><p><strong>Postscript</strong></p><p>As I said earlier, the topic of setting up interpolated attributes wouldactually make for a nice article on its own. I’m skipping that for now – mightdecide to fill this gap later, who knows.</p><p>Z processing has been in the 3D pipeline for ages, and a serious bandwidthissue for most of the time; people have thought long and hard about thisproblem, and there’s a zillion tricks that go into doing “production-quality”Z-buffering for GPUs, some big, some<br>small. Again, I’m just scratching thesurface here; I tried to limit myself to the bits that are useful to know for agraphics programmer. That’s why I don’t spend much time on the details ofhierarchical Z computations or Z compression and the like; all of this<br>is veryspecific on hardware details that change slightly in every generation, andultimately, mostly there’s just no practical way you get to exploit any of thisusefully: If a given Z-compression scheme works well for your scene, that’ssome memory bandwidth<br>you can spend on other things. If not, what are you gonnado? Change your geometry and camera position so that Z-compression is moreefficient? Not very likely. To a hardware designer, these are all algorithms tobe improved on in every generation, but to a programmer,<br>they’re just facts oflife to deal with.</p><p>This time, I’m not going into much detail on how memory accesses work in thisstage of the pipeline. That’s intentional. There’s a key to high-throughputpixel shading and other per-pixel or per-sample processing, but it’s later inthe pipeline, and we’re not<br>there yet. Everything will be revealed in due time:)</p><p>&nbsp;</p><p>&nbsp;</p><p>A trip through the GraphicsPipeline 2011, part 8</p><p>July 10, 2011</p><p>In this part, I’ll be dealing with the first half of pixel processing: dispatchand actual pixel shading. In fact, this is really what most graphics programmerthink about when talking about pixel processing; the alpha blend and late Zstages we’ll encounter in<br>the next part seem like little more than anafterthought. In hardware, the story is a bit more complicated, as we’ll see –there’s a reason I’m splitting pixel processing into two parts. But I’m gettingahead of myself. At the point where we’re entering this<br>stage, the coordinatesof pixels (or, actually, quads) to shade, plus associated coverage masks,arrive from the rasterizer/early-Z unit – with triangle in the exact same orderas submitted by the application, as I pointed out last time. What we need to dohere<br>is to take that linear, sequential stream of work and farm it out tohundreds of shader units, then once the results are back, we need to merge itback into one linear stream of memory updates.</p><p>That’s a textbook example of fork/join-parallelism. This part deals with thefork phase, where we go wide; the next part will explain the join phase, wherewe merge the hundreds of streams back into one. But first, I have a few morewords to say about rasterization,<br>because what I just told you about therebeing just one stream of quads coming in isn’t quite true.</p><p><strong>Going wide during rasterization</strong></p><p>To my defense, what I told you used to be true for quite a long time, but it’sa serial part of the pipeline, and once you throw in excess of 300 shader unitsat a problem, serial parts of the pipeline have the tendency to becomebottlenecks. So GPU architects<br>started using multiple rasterizers; as of 2010,NVidia employs four rasterizers and AMD uses two. As a side note, the NVpresentation also has a few notes on the requirement to keep stuff in APIorder. In particular, you really do need to sort primitives back<br>into orderprior to rasterization/early-Z, like I mentioned last time; doing it justbefore alpha blend (as you might be inclined to do) doesn’t work.</p><p>The work distribution between rasterizers is based on the tiles we’ve alreadyseen for early-Z and coarse rasterization. The frame buffer is divided intotile-sized regions, and each region is assigned to one of the rasterizers.After setup, the bounding box of<br>the triangle is consulted to find out whichtriangles to hand over to which rasterizers; large triangles will always besent to all rasterizers, but smaller ones can hit as little as one tile andwill only be sent to the rasterizer that owns it.</p><p>The beauty of this scheme is that it only requires changes to the workdistribution and the coarse rasterizers (which traverse tiles); everything thatonly sees individual tiles or quads (that is, the pipeline from hierarchical Zdown) doesn’t need to be modified.<br>The problem is that you’re now dividing jobsbased on screen locations; this can lead to a severe load imbalance between therasterizers (think a few hundred tiny triangles all inside a single tile) thatyou can’t really do anything about. But the nice thing<br>is that everything thatadds ordering constraints to the pipeline (Z-test/write order, blend order)comes attached to specific frame-buffer locations, so screen-space subdivisionworks without breaking API order – if this wasn’t the case, tiled rendererswouldn’t<br>work.</p><p><strong>You need to go wider!</strong></p><p>Okay, so we don’t get just one linear stream of quad coordinates plus coveragemasks in, but between two and four. We still need to farm them out to hundredsof shader units. It’s time for another dispatch unit! Which first means anotherbuffer. But how big are<br>the batches we send off to the shaders? Here I go withNVidia figures again, simply because they mention this number in public whitepapers; AMD probably also states that information somewhere, but I’m notfamiliar with their terminology for it so I couldn’t<br>do a direct search for it.Anyway, for NVidia, the unit of dispatch to shader units is 32 threads, whichthey call a “Warp”. Each quad has 4 pixels (each of which in turn can behandled as one thread), so for each shading batch we issue, we need to grab 8incoming<br>quads from the rasterizer before we can send off a batch to the shaderunits (we might send less in case there’s a shader switch or pipeline flush).</p><p>Also, this is a good point to explain why we’re dealing with quads of 2×2pixels and not individual pixels. The big reason is derivatives. Texturesamplers depend on screen-space derivatives of texture coordinates to do theirmip-map selection and filtering (as<br>we saw back in part 4); and, as of shadermodel 3.0 and later, the same machinery is directly available to pixel shadersin the form of derivative instructions. In a quad, each pixel has both ahorizontal and vertical neighbor within the same quad; this can be<br>used toestimate the derivatives of parameters in the x and y directions using finitedifferencing (it boils down to a few subtractions). This gives you a very cheapway to get derivatives at the cost of always having to shade groups of 2×2pixels at once. This<br>is no problem in the interior of large triangles, butmeans that between 25-75% of the shading work for quads generated for triangleedges is wasted. That’s because all pixels in a quad, even the masked ones, getshaded. This is necessary to produce correct derivatives<br>for the pixels in thequad that are visible. The invisible but still-shaded pixels are called “helperpixels”. Here’s an illustration for a small triangle:</p><p>The triangle intersects 4 quads, but only generates visible pixels in 3 ofthem. Furthermore, in each of the 3 quads, only one pixel is actually covered(the sampling points for each pixel region are depicted as black circles) – thepixels that are filled are<br>depicted in red. The remaining pixels in eachpartially-covered quad are helper pixels, and drawn with a lighter color. Thisillustration should make it clear that for small triangles, a large fraction ofthe total number of pixels shaded are helper pixels, which<br>has attracted someresearch attention on how to merge quads of adjacent triangles. However, whileclever, such optimizations are not permissible by current API rules, andcurrent hardware doesn’t do them. Of course, if the HW vendors at some pointdecide that<br>wasted shading work on quads is a significant enough problem toforce the issue, this will likely change.</p><p><strong>Attribute interpolation</strong></p><p>Another unique feature of pixel shaders is attribute interpolation – all other shadertypes, both the ones we’ve seen so far (VS) and the ones we’re still to talkabout (GS, HS, DS, CS) get inputs directly from a preceding shader stage ormemory, but pixel shaders<br>have an additional interpolation step in front ofthem. I’ve already talked a bit about this in the previous part when discussingZ, which was the first interpolated attribute we saw.</p><p>Other interpolated attributes work much the same way; a plane equation for themis computed during triangle setup (GPUs may choose to defer this computationsomewhat, e.g. until it’s known that at least one tile of the triangle passedthe hierarchical Z-test,<br>but that shall not concern us here), and then duringpixel shading, there’s a separate unit that performs attribute interpolationusing the pixel positions of the quads and the plane equations we justcomputed.</p><p>Update: Marco Salvi points out (in the comments below) that while there used tobe dedicated interpolators, by now the trend is towards just having them returnthe barycentric coordinates to plug into the plane equations. The actualevaluation (two multiply-adds<br>per attribute) can be done in the shader unit.</p><p>All of this shouldn’t be surprising, but there’s a few extra interpolationtypes to discuss. First, there’s “constant” interpolators, which are(surprise!) constant across the primitive and take the value for each vertexattribute from the “leading vertex” (which<br>vertex that is is determined duringprimitive setup). Hardware may either have a fast-path for this or just set upa corresponding plane equation; either way works fine.</p><p>Then there’s no-perspective interpolation. This will usually set up the planeequations differently; the plane equations for perspective-correctinterpolation are set up either for X, Y-based interpolation by dividing theattribute values at each vertex by the<br>corresponding w, or for barycentricinterpolation by building the triangle edge vectors. Non-perspectiveinterpolated attributes, however, are cheapest to evaluate when their planeequation is set up for X, Y-based interpolation without dividing the values ateach<br>vertex by the corresponding w.</p><p><strong>“Centroid” interpolation is tricky</strong></p><p>Next, we have “centroid” interpolation. This is a flag, not a separate mode; itcan be combined both with the perspective and no-perspective modes (but notwith constant interpolation, because it would be pointless). It’s also terriblynamed and a no-op unless<br>multisampling is enabled. With multisampling ob, it’sa somewhat hacky solution to a real problem. The issue is that withmultisampling, we’re evaluating triangle coverage at multiple sample points inthe rasterizer, but we’re only doing the actual shading once<br>per pixel.Attributes such as texture coordinates will be interpolated at the pixel centerposition, as if the whole pixel was covered by the primitive. This can lead toproblems in situations such as this:</p><p>MSAA sample problem</p><p>Here, we have a pixel that’s partially covered by a primitive; the four smallcircles depict the 4 sampling points (this is the default 4x MSAA pattern)while the big circle in the middle depicts the pixel center. Note that the bigcircle is outside the primitive,<br>and any “interpolated” value for it willactually be linear extrapolation; this is a problem if the app uses textureatlases, for example. Depending on the triangle size, the value at the pixelcenter can be very far off indeed. Centroid sampling solves this<br>problem. Theoriginal explanation was that the GPU takes all of the samples covered by theprimitive, computes their centroid, and samples at that position (hence thename). This is usually followed by the addition that this is just a conceptualmodel, and GPUs<br>are free to do it differently, so long as the point they pickfor sampling is within the primitive.</p><p>If you think it somewhat unlikely that the hardware actually counts the coveredsamples, sums them up, then divides by the count, then join the club. Here’swhat actually happens:</p><p>&nbsp; &nbsp; If all sample points cover the primitive, interpolation is doneas usual, i.e. at the pixel center (which happens to be the centroid of allsample positions for all reasonable sampling patterns).</p><p>&nbsp; &nbsp; If not all sample points cover the triangle, the hardware picksone of the sample points that do and evaluates there. All covered sample pointsare (by definition) inside the primitive so this works.</p><p>That picking used to be arbitrary (i.e. left to the hardware); I believe by nowDX11 actually prescribes exactly how it’s done, but this more a matter ofgetting consistent results between different pieces of hardware than it issomething that API users will actually<br>care about. As said, it’s a bit hacky.It also tends to mess up derivative calculations for quads that have partiallycovered pixels – tough luck. What can I say, it may be industrial-strength ducttape, but it’s still duct tape.</p><p>Finally (new in DX11!) there’s “pull-model” attribute interpolation. Regularattribute interpolation is done automatically before the pixel shader starts;pull-model interpolation adds actual instructions that do the interpolation tothe pixel shader. This allows<br>the shader to compute its own position to samplevalues at, or to only interpolate attributes in some branches but not inothers. What it boils down to is the pixel shader being able to send additionalrequests to the interpolation unit while the shader is running.</p><p><strong>The actual shader body</strong></p><p>Again, the general shader principles are well-explained in the APIdocumentation, so I’m not going to talk about how individual instructions work;generally, the answer is “as you would expect them to”. There are however someinteresting bits about pixel shader<br>execution that are worth talking about.</p><p>The first one is: texture sampling! Wait, didn’t I wax on about texturesamplers for quite some time in part 4 already? Yes, but that was the texturesampler side of things – and if you remember, there was that one bit abouttexture cache misses being so frequent<br>that samplers are usually designed tosustain at least one miss to main memory per request (which is 16-32 pixels, remember!)without stalling. That’s a lot of cycles – hundreds of them. And it would be atremendous waste of perfectly good ALUs to keep them idle<br>while all this isgoing on.</p><p>So what shader units actually do is switch to a different batch after they’veissued a texture sample; then when that batch issues a texture sample (orcompletes), it switches back to one of the previous batches and checks if thetexture samples are there yet.<br>As long as each shader unit has a few batches itcan work on at any given time, this makes good use of available resources. Itdoes increase latency for completion of individual batches though – again, alatency-vs-throughput trade-off. By now you should know<br>which side wins onGPUs: Throughput! Always. One thing to note here is that keeping multiplebatches (or “Warps” on NVidia hardware, or “Wavefronts” for AMD) running at thesame time requires more registers. If a shader needs a lot of registers, ashader unit<br>can keep less warps around; and if there are less of them, the chancethat at some point you’ll run out of runnable batches that aren’t waiting ontexture results is higher. If there’s no runnable batches, you’re out of luckand have to stall until one of them<br>gets its results back. That’s unfortunate,but there’s limited hardware resources for this kind of thing – if you’re outof memory, you’re out of memory, period.</p><p>Another point I haven’t talked about yet: Dynamic branches in shaders (i.e.loops and conditionals). In shader units, work on all elements of each batch usuallyproceeds in lockstep. All “threads” run the same code, at the same time. Thatmeans that ifs are a<br>bit tricky: If any of the threads want to execute the“then”-branch of an if, all of them have to – even though most of them may endup ignoring the results using a technique called predication, because theydidn’t want to descend down there in the first place..<br>Similarly for the “else”branch. This works great if conditionals tend to be coherent across elements,and not so great if they’re more or less random. Worst case, you’ll alwaysexecute both branches of every if. Ouch. Loops work similarly – as long as atleast<br>one thread wants to keep running a loop, all of the threads in thatbatch/Warp/Wavefront will.</p><p>Another pixel shader specific is the discard instruction. A pixel shader candecide to “kill” the current pixel, which means it won’t get written. Again, ifall pixels inside a batch get discarded, the shader unit can stop and go toanother batch; but if there’s<br>at least one thread left standing, the rest willbe dragged along. DX11 adds more fine-grained control here by way of writingthe output pixel coverage from the pixel shader (this is always ANDed with theoriginal triangle/Z-test coverage, to make sure that a<br>shader can’t writeoutside its primitive, for sanity). This allows the shader to discardindividual samples instead of whole pixels; it can be used to implementAlpha-to-Coverage with a custom dithering algorithm in the shader, for example.</p><p>Pixel shaders can also write the output depth (this feature has been around forquite some time now). In my experience, this is an excellent way to shoot downearly-Z, hierarchical Z and Z compression and in general get the slowest pathpossible. By now, you know<br>enough about how these things work to see why. :)</p><p>Pixel shaders produce several outputs – in general, one 4-component vector foreach render target, of which there can be (currently) up to 8. The shader thensends the results on down the pipeline towards what D3D calls the “OutputMerger”. This’ll be our topic<br>next time.</p><p>But before I sign off, there’s one final thing that pixel shaders can dostarting with D3D11: they can write to Unordered Access Views (UAVs) –something which only compute and pixel shaders can do. Generally speaking, UAVstake the place of render targets during<br>compute shader execution; but unlikerender targets, the shader can determine the position to write to itself, andthere’s no implicit API order guarantee (hence the “unordered access” part ofthe name). For now, I’ll only mention that this functionality exists<br>– I’lltalk more about it when I get to Compute Shaders.</p><p>Update: In the comments, Steve gave me a heads-up about the correct AMDterminology (the first version of the post didn’t have the “Wavefronts” namebecause I couldn’t remember it) and also posted a link to this greatpresentation by Kayvon Fatahalian that explains<br>shader execution on GPUs, witha lot more pretty pictures that I can be bothered to make :). You should reallycheck it out if you’re interested in how shader cores work.</p><p>And… that’s it! No big list of caveats this time. If there’s something missinghere, it’s because I’ve genuinely forgotten about it, not because I decided itwas too arcane or specific to write up here. Feel free to point out omissionsin the comments and I’ll<br>see what I can do.</p><p>&nbsp;</p><p>&nbsp;</p><p>A trip through the GraphicsPipeline 2011, part 9</p><p>July 12, 2011</p><p>Welcome back! This post deals with the second half of pixel processing, the“join phase”. The previous phase was all about taking a small number of inputstreams and turning them into lots of independent tasks for the shader units.Now we need to fold that large<br>number of independent computations back into one(correctly ordered) stream of memory operations. As I already did in the posts onrasterization and early Z, I’ll first give a quick description of what needs tobe done on a general level, and then I’ll go into<br>how this is mapped tohardware.</p><p><strong>Merging pixels again: blend and late Z</strong></p><p>At the bottom of the pipeline (in what D3D calls the “Output Merger” stage), wehave late Z/stencil processing and blending. These two operations are bothrelatively simple computationally, and they both update the render target(s) /depth buffer respectively.<br>“Update” operation here means they’re of the read-modify-writevariety. Because all of this happens for every quad that makes it this farthrough the pipeline, it’s also bandwidth-intensive. Finally, it’sorder-sensitive (both blending and Z processing need to<br>happen in API order),so we need to make sure to sort processed quads into order first.</p><p>I’ve already explained Z-processing, and blending is one of these things thatwork pretty much as you’d expect; it’s a fixed-function block that performs amultiply, a multiply-add and maybe some subtractions first, per render target.This block is kept deliberately<br>simple; it’s separate from the shader units soit needs its own ALU, and we’d really prefer for it to be as small as possible:we want to spend our chip area (and power budget) on ALUs in the shader units,where they benefit every code that runs on the GPU, not<br>on a fixed-functionunit that’s only used at the end of the pixel pipeline. Also, we need it tohave a short, predictable latency: this part of the pipeline needs to processdata in-order to be correct. This limits our options as far as tradingthroughput for<br>latency is concerned; we can still process quads that don’toverlap in parallel, but if we e.g. draw lots of small triangles, we’ll havemultiple quads coming in for every screen location, and we’d better be able towrite them out as quickly as they come, or<br>else all our massively parallelpixel processing was for nought.</p><p><strong>Meet the ROPs</strong></p><p>ROPs are the hardware units that handle this part of the pipeline (as you cantell by the plural, there’s more than one). The acronym, depending on who youasks, stands for “Render OutPut unit”, “Raster Operations Pipeline”, or “RasterOperations Processor”. The<br>actual name is fairly archaic – it derives from thedays of pure 2D hardware acceleration, with hardware whose main purpose was todo fast Bit blits. The classic 2D ROP design has three inputs – the current(destination) pixel value in the frame buffer, the source<br>data, and a maskinput – then computes some function of the 3 values and writes the results backto the frame buffer. Note this is before true color displays: the image datawas usually in bit plane format and the function was some binary logicfunction. Then<br>at some point bit planes died out (in favor of “chunky”representations that keep the bits for a pixel together), true color became thenorm, the on-off mask was replaced with an alpha channel and the bitwiseoperations with blends, but the name stuck. So even<br>now in 2011, when about thelast remnant of that original architecture is the “logic op” in OpenGL, westill call them ROPs.</p><p>So what do we need to do, in hardware, for blend/late Z? A simple plan:</p><p>&nbsp; &nbsp; Read original render target/depth buffer contents from memory –memory access, long latency. Might also involve depth buffer and render targetdecompression! (I’ll explain render target compression later)</p><p>&nbsp; &nbsp; Sort incoming shaded quads into the right (API) order. This takessome buffering so we don’t immediately stall when quads don’t finish in theright order (think loops/branches, discard, and variable texture fetchlatency). Note we only need to sort based on<br>primitive ID here – two quads fromthe same primitive can never overlap, and if they don’t overlap they don’t needto be sorted!</p><p>&nbsp; &nbsp; Perform the actual blend/late Z/stencil operation. This is math –maybe a few dozen cycles worth, even with deeply pipelined units.</p><p>&nbsp; &nbsp; Write the results back to memory again, compressing etc. alongthe way – long latency again, though this time we’re not waiting for results soit’s less of a problem at this end.</p><p>So, build the late-Z/blending unit, add some compression logic, wire it up tomemory on one side and do some buffering of shaded quads on the other side andwe’re done, right?</p><p>Well, in theory anyway.</p><p>Except we need to cover the long latencies somehow. And all this happens forevery single pixel (well, quad, actually). So we need to worry about memorybandwidth too… memory bandwidth? Wasn’t there something about memory bandwidth?Watch closely now as I pull<br>a bunny out of a hat after I put it there way backin part 2 (uh oh, that was more than a week ago – hope that critter is still OKin there…).</p><p><strong>Memory bandwidth redux: DRAM pages</strong></p><p>In part 2, I described the 2D layout of DRAM, and how it’s faster to staywithin a single row because changing the active row takes time – so for idealbandwidth you want to stay in the same row between accesses. Well, the thingis, single DRAM rows are kinda<br>large. Individual DRAM chips go up into theGigabit range in size these days, and while they’re not necessarily square (infact a 2:1 aspect ratio seems to be preferred), you can still do a roughcalculation of how many rows and columns there would be; for 512<br>Megabit(=64MB), we’d expect something like 16384×32768, i.e. a single row is about 32kbits or 4k bytes (or maybe 2k, or 8k, but somewhere in that ballpark – you getthe idea). That’s a rather inconvenient size to be making memory transactionsin.</p><p>Hence, a compromise: the page. A DRAM page is some more conveniently sizedslice of a row (by now, usually 256 or 512 bits) that’s commonly transferred ina single burst. Let’s take 512 bits (64 bytes) for now. At 32 bits per pixel –the standard for depth buffers<br>and still fairly common for render targetsalthough rendering workloads are definitely shifting towards 64 bit/pixelformats – that’s enough memory to fit data for 16 pixels in. Hey, that’s funny– we’re usually shading pixels in groups of 16 to 64! (NV is a<br>bit closer tothe smaller end, AMD favors the larger counts). In fact, the 8×8 tile size I’vebeen quoting in the rasterizer / early Z parts comes from AMD; I wouldn’t besurprised if NV did coarse traversal (and hierarchical Z, which they dub“Z-cull”) on 4×4<br>tiles, though a quick web search turned up nothing to eitherconfirm this or rule it out. Either way, the plot thickens. Could it be thatwe’re trying to traverse pixels in an order that gives good DRAM pagecoherency? You bet we are. Note that this has implications<br>for internal rendertarget layout too: we want to make sure pixels are stored such that a singleDRAM page actually has a useful shape; for shading purposes, a 4×4 or 8×2 pixelDRAM page is a lot more useful than a 16×1 pixel one (remember – quads). Whichis why<br>render targets usually don’t have a fully linear layout in memory.</p><p>That gives us yet another reason to shade pixels in groups, and also yetanother reason to do a two-level traversal. But can we milk this some more? Youbet we can: we still have the memory latency to cover. Usual disclaimer: Thisis one of the places where I<br>don’t have detailed information on what GPUsactually do, so what I’m describing here is a guess, not a fact. Anyway, assoon as we’ve rasterized a tile, we know whether it generates any pixels ornot. At that point, we can select a ROP to handle our quads for<br>that tile, andqueue a command to fetch the associated frame buffer data into a buffer. By thepoint we get shaded quads back from the shader units, that data should bethere, and we can start blending without delay (of course, if blending is offor identity,<br>we can skip this load altogether). Similarly for Z data – if werun early Z before the pixel shader, we might need to allocate a ROP and fetchdepth/stencil data earlier, maybe as soon as a tile has passes the coarse Ztest. If we run late Z, we can just prefetch<br>the depth buffer data at the sametime we grab the framebuffer pixels (unless Z is off completely, that is).</p><p>All of this is early enough to avoid latency stalls for all but the fastestpixel shaders (which are usually memory bandwidth-bound anyway). There’s alsothe issue of pixel shaders that output to multiple render targets, but thatdepends on how exactly that feature<br>is implemented. You could run the shadermultiple times (not efficient but easiest if you have fixed-size output buffers),or you could run all the render targets through the same ROP (but up to 8rendertargets with up to 128 bits/pixels – that’s a lot of buffer<br>space we’retalking), or you could allocate one ROP per output render target.</p><p>An of course, if we have these buffers in the ROPs anyway, we might as welltreat them as a small cache (i.e. keep them around for a while). This wouldhelp if you’re drawing lots of small triangles – as long as they’re spatiallylocalized, anyway. Again, I’m<br>not sure if GPUs actually do this, but it seemslike a reasonable thing to do (you’d probably want to flush these bufferssomething like once per batch or so though, to avoid thesynchronization/coherency issues that full write-back caches bring).</p><p>Okay, that explains the memory side of things, and the computational part we’vealready covered. Next up: Compression!</p><p><strong>Depth buffer and color buffer compression</strong></p><p>I already explained the basic workings of this in part 7 while talking about Z;in fact, I don’t have much to add about depth buffer compression here. But allthe bandwidth issues I mentioned there exist for color values too; it’s not sobad for regular rendering<br>(unless the Pixel Shaders output pixels fast enoughto hit memory bandwidth limits), but it is a serious issue for MSAA, where wesuddenly store somewhere between 2 and 8 samples per pixel. Like Z, we wantsome lossless compression scheme to save bandwidth in<br>common cases. Unlike Z,plane equations per tile are not a good fit to textured pixel data.</p><p>However, that’s no problem, because actually, MSAA pixel data is even easier tooptimize for: Remember that pixel shaders only run once per pixel, not persample – unless you’re using sample-frequency shading anyway, but that’s aD3D11 feature and not commonly<br>used (yet?). Hence, for all pixels that arefully covered by a single primitive, the 2-8 samples stored will usually be thesame. And that’s the idea behind the common color buffer compression schemes:Write a flag bit (either per pixel, or per quad, or on an<br>even larger granularity)that denotes whether for all the pixels in a compression block, all theper-sample colors are in fact the same. And if that’s the case, we only need tostore the color once per pixel after all. This is fairly simple to detectduring write-back,<br>and again (much like depth compression), it requires sometag bits that we can store in a small on-chip SRAM. If there’s an edge crossingthe pixels, we need the full bandwidth, but if the triangles aren’t too small(and they’re basically never all small), we<br>can save a good deal of bandwidthon at least part of the frame. And again, we can use the same machinery toaccelerate clears.</p><p>On the subject of clears and compression, there’s another thing to mention:Some GPUs have “hierarchical Z”-like mechanisms that store, for a large blockof pixels (a rasterizer tile, maybe even larger) that the block was recentlycleared. Then you only need to<br>store one color value for the whole tile (orlarger block) in memory. This gives you very fast color clears for some buffers(again, you need some tag bits for this!). However, as soon as any pixel withnon-clear color is written to the tile (or larger block),<br>the “this was justcleared” flag needs to be… well, cleared. But we do save a lot of memorybandwidth on the clear itself and the first time a tile is read from memory.</p><p>And that’s it for our first rendering data path: just Vertex and Pixel Shaders(the most common path). In the next part, I’ll talk about Geometry Shaders andhow that pipeline looks. But before I conclude this post, I have a small bonustopic that fits into this<br>section.</p><p><strong>Aside: Why no fully programmable blend?</strong></p><p>Everyone who writes rendering code wonders about this at some point – theregular blend pipeline a serious pain to work with sometimes. So why can’t weget fully programmable blend? We have fully programmable shading, after all!Well, we now have the necessary<br>framework to look into this properly. There’stwo main proposals for this that I’ve seen – let’s look at the both in turn:</p><p>&nbsp; &nbsp; Blend in Pixel Shader – i.e. Pixel Shader reads framebuffer,computes blend equation, writes new output value.</p><p>&nbsp; &nbsp; Programmable Blend Unit – “Blend Shaders”, with subset of fullshader instruction set if necessary. Happen in separate stage after PS.</p><p><strong>1. Blend in Pixel Shader</strong></p><p>This seems like a no-brainer: after all, we have loads and texture samples inshaders already, right? So why not just allow a read to the current rendertarget? Turns out that unconstrained reads are a really bad idea, because itmeans that every pixel being shaded<br>could (potentially) influence every otherpixel being shaded. So what if I reference a pixel in the quad over to theleft? Well, a shader for that quad could be running this instant. Or I could besampling half of my current quad and half of another quads that’s<br>currentlyactive – what do I do now? What exactly would be the correct results in thatregard, never mind that we’d probably have to shade all quads sequentially toreliably get them? No, that’s a can of worms. Unconstrained reads from theframe buffer in Pixel<br>Shaders are out. But what if we get a special rendertarget read instruction that samples one of the active render targets at thecurrent location? Now, that’s a lot better – now we only need to worry aboutwrites to the location of the current quad, which is<br>a way more tractableproblem.</p><p>However, it still introduces ordering constraints; we have to check all quadsgenerated by the rasterizer vs. the quads currently being pixel-shaded. If aquad just generated by the rasterizer wants to write to a sample that’ll bewritten by one of the Pixel Shaders<br>that are currently in flight, we need towait until that PS is completed before we can dispatch the new quad. Thisdoesn’t sound too bad, but how do we track this? We could just have a “thissample is currently being shaded” bit flag… so how many of these bits<br>do weneed? At 1920×1080 with 8x MSAA, about 2MB worth of them (that’s bytes notbits) – and that memory is global, shared and determines the rate at which wecan issue new quads (since we need to mark a quad as busy before we can issueit). Worse, with the hierarchical<br>Z etc. tag bits, they were just a hint; if weran out of them, we could still render, albeit more slowly. But this memory isnot optional. We can’t guarantee correctness unless we’re really tracking everysample! What if we just tracked the “busy” state per pixel<br>(or even quad), andany write to a pixel would block all other such writes? That would work, but itwould massively harm our MSAA performance: If we track per sample, we can shadeadjacent, non-overlapping triangles in parallel, no problem. But if we trackper<br>pixel (or at lower granularity), we effectively serialize all the edgequads. And what happens to our fill rate for e.g. particle systems with lots ofoverdraw? With the pipeline I described, these render (more or less) as fast asthe ROPs can merge the incoming<br>pixels into the store buffers. But if we needto avoid conflicts, we really end up shading the individual overlappingparticles in order. This isn’t good news for our shader units that are designedto trade latency for throughput, not at all.</p><p>Okay, so this whole tracking thing is a problem. What if we just force shadingto execute in order? That is, keep the whole thing pipelined and all shadersrunning in lockstep; now we don’t need tracking because pixels will finish inthe same order we put them<br>into the pipeline! But the problem here is that weneed to make sure the shaders in a batch actually always take the exact sametime, which has unfortunate consequences: You always have to wait theworst-case delay time for every texture sample, need to always<br>execute bothsides of every branch (someone might at some point need the then/else branches,and we need everything to take the same time!), always runs all loops throughfor the same number of iterations, can’t stop shading on discard… no, thatdoesn’t sound<br>like a winner either.</p><p>Okay, time to face the music: Pixel Shader blend in the architecture I’vedescribed comes with a bunch of seriously tricky problems. So what about thesecond approach?</p><p><strong>2. “Blend Shaders”</strong></p><p>I’ll say it right now: This can be made to work, but…</p><p>Let’s just say it has its own problems. For once, we now need another full ALU&#43; instruction decoder/sequencer etc. in the ROPs. This is not a small change –not in design effort, nor in area, nor in power. Second, as I mentioned nearthe start of this post, our<br>regular “just go wide” tactics don’t work so wellfor blend, because this is a place where we might well get a bunch of quadshitting the same pixels in a row and need to process them in order, so we wantlow latency. That’s a very different design point than<br>our regular unifiedshader units – so we can’t use them for this (it also means texturesampling/memory access in Blend Shaders is a big no, but I doubt that shocksanyone at this point). Third, pure serial execution is out at this point – toolow throughput.<br>So we need to pipeline it. But to pipeline it, we need to knowhow long the pipeline is! For a regular blend unit, it’s a fixed length, soit’s easy. A blend shader would probably be the same. In fact, due to thedesign constraints, you’re unlikely to get a blend<br>shader – more like a blendregister combiner, really, completely with a (presumably relatively low) upperlimit on the number of instructions, as determined by the length of thepipeline.</p><p>Point being, the serial execution here really constrains us to designs that arestill relatively low-level; nowhere near the fully programmable shader unitswe’ve come to love. A nicer blend unit with some extra blend modes, you candefinitely get; a more open<br>register combiner-style design, possibly, thoughneither the API guys nor the hardware guys will like it much (the API becauseit’s a fixed function block, the hardware guys because it’s big and needs a bigALU&#43;control logic where they’d rather not have it).<br>Fully programmable, withbranches, loops, etc. – not going to happen. At that point you might as wellbite the bullet and do what it takes to get the “Blend in Pixel Shader”scenario to work properly.</p><p>…and that’s it for this post! See you next time.</p><p>&nbsp;</p><p>&nbsp;</p><p>A trip through the GraphicsPipeline 2011, part 10</p><p>July 20, 2011</p><p>Welcome back. Last time, we dove into bottom end of the pixel pipeline. Thistime, we’ll switch back to the middle of the pipeline to look at what isprobably the most visible addition that came with D3D10: Geometry Shaders. Butfirst, some more words on how I<br>decompose the graphics pipeline in this series,and how that’s different from the view the APIs will present to you.</p><p><strong>There’s multiple pipelines / anatomy of a pipeline stage</strong></p><p>This goes back to part 3, but it’s important enough to repeat it: if you lookin, for example, the D3D10 documentation, you’ll find a diagram of the “D3D10pipeline” that includes all stages that might be active. The “D3D10 pipeline”includes Geometry Shading,<br>even if you don’t have a Geometry shader set, andthe same for Stream-Out. In the purely functional model of D3D10, the GeometryShading stage is always there; if you don’t set a Geometry Shader, it justhappens to be very simple (and boring): data is just passed<br>through unmodifiedto the next pipeline stage(s) (Rasterization/Stream-Out).</p><p>That’s the right way to specify the API, but it’s the wrong way to think aboutit in this series, where we’re concerned with how that functional model isactually implemented in hardware. So how do the two shader stages we’ve seen sofar look? For VS, we went<br>through the Input Assembler, which prepared a blockof vertices for shading, then dispatched that batch to a shader unit (whichchews on it for a while), and then some time later we get the results back,write them into a buffer (for Primitive Assembly), make<br>sure they’re in theright order, then send them down to the next pipeline stage (Culling/Clippingetc.). For PS, we receive to-be-shaded quads from the rasterizer, batch themup, buffer them for a while until a shader unit is free to accept a new batch,dispatch<br>a batch to a shader unit (which chews on it for a while), and then sometime later we get the results back, write them into a buffer (for the ROPs),make sure they’re in the right order, then do blend/late Z and send the resultson to memory. Sounds kind of familiar,<br>doesn’t it?</p><p>In fact, this is how it always looks when we want to get something done by theshader units: we need a buffer in the front, then some dispatching logic (whichis in fact pretty universal for all shader types and can be shared), then we gowide and run a bunch<br>of shaders in parallel, and finally we need another bufferand a unit that sorts the results (which we received potentially out-of-orderfrom the shader units) back into API order.</p><p>We’ve seen shader units (and shader execution) and we’ve seen dispatch; and infact, now that we’ve seen Pixel Shaders (which have some peculiarities likederivative computation, helper pixels, discard and attribute interpolation),we’re not gonna see any big<br>additions to shader unit functionality until we getto Compute Shaders, with their specialized buffer types and atomics. So for thenext few parts, I won’t be talking about the shader units; what’s reallydifferent about the various shader types is the shape<br>and interpretation ofdata that goes in and comes out. The shader parts that don’t deal with IO(arithmetic, texture sampling) stay the same, so I won’t be talking about them.</p><p><strong>The Shape of Tris to Shade</strong></p><p>So let’s have a look at how our IO buffers for Geometry Shaders look. Let’sstart with input. Well, that’s reasonably easy – it’s just what we wrote fromthe Vertex Shader! Or well, not quite; the Geometry Shader looks at primitives,not individual vertices, so<br>what we really need is the output from PrimitiveAssembly (PA). Note that there’s multiple ways to deal with this; PA couldexpand primitives out (duplicating vertices if they’re referenced multipletimes), or it could just hand us one block of vertices (I’ll<br>stick with the 32vertices I used earlier) with an associated small “index buffer” (since we’reindexing into a block of 32 vertices, we just need 5 bits per index). Eitherway works fine; the former is the natural input format for the clip/cull Idiscussed after<br>PA, but the latter needs far less buffer space when running GS,so I’ll use that model here.</p><p>One reason you need to worry about amount of buffer space with GS is that itcan work on some pretty large primitives, because it doesn’t just support plainlines or triangles (2 and 3 vertices per primitive respectively), but alsolines/triangles with adjacency<br>information (4/6 vertices per primitive). AndD3D11 adds input primitives that are much fatter still – a GS can also consumespatches with up to 32 control points as input. Duplicating the vertices of e.g.a 16-control point patch, which could each have up to<br>16 vector attributes (32with D3D11)? That’d be some serious memory waste. So I’m assumingnon-duplicated, indexed vertices for this path. Which makes the input for abatch of primitives: the VS output, plus a (relatively small) index buffer.</p><p>Now, the geometry shader runs per primitive. For vertex shaders, we needed togather a batch of vertices, and we chose our batch size with a simple greedyalgorithm that tries to pack as many vertices into a batch as possible withoutsplitting a primitive across<br>multiple batches – fair enough. And for pixelshading, we get plenty of quads from the rasterizer and pack them all intobatches. Geometry Shaders are a bit more inconvenient – our input block isguaranteed to contain at least one full primitive, and possibly<br>several – butother than that, the number of primitives in that block completely depends onthe vertex cache hit rate. If it’s high and we’re using triangles, we might getsomething like 40-43; if we’re using triangles with adjacency information wecould have<br>as little as 5 if we’re unlucky.</p><p>Of course, we could try to collect primitives from several input blocks here,but that’s kind of awkward too. Now we need to keep multiple input blocks andindex buffers around for a single GS batch, and if a single batch can refer tomultiple index buffers that<br>means each primitive in that batch now needs toknow where to get the indices and vertex data from – more storage requirements,more management, more overhead. Also ugly. And of course even with two inputblocks you’re still at crappy utilization if you hit two<br>input batches with lowvertex cache hit rate. You can support more input blocks, but that eats away atmemory – and remember, you need space for the output geometry too (I’ll get tothat in a bit).</p><p>So this is our first snag: with VS, we could basically pick our target batchsize, and we chose to not always generate full batches so as to make our livesin PA (and here in the GS, and later in the HS too) a bit easier. With PS, wealways shade quads, and even<br>fairly small tris usually hit multiple quads so weget an okay ratio of number of quads to number of tris. But with GS, we don’thave full control over either ends of the pipeline (since we’re in themiddle!), and we need multiple input vertices per primitive<br>(as opposed tomultiple quads per one input triangle), so buffering up a lot of input isexpensive (both in terms of memory and in the amount of management overhead weget).</p><p>At this stage, you can basically pick how many input blocks you’re willing tomerge to get one block of primitives to geometry shade; that number is going tobe low because of the memory requirements (I’d be very surprised to see morethan 4), and depending on<br>how important you judge GS to be, you might even pick1, i.e. don’t merge across input blocks at all and live with crappy utilizationon GS shading blocks/Warps/Wavefronts! That’s not great with triangles andreally bad with the primitives that have even more<br>vertices, but not much of anissue when your main use case for GS in practice is expanding points to quads(point sprites) and maybe rendering the occasional cube shadow map (using theViewport Array Index/Rendertarget Index – I’ll get to that in a bit).</p><p><strong>GS output: no rose garden over here, either</strong></p><p>So how’s it looking on the output side? Again, this is more complicated thanthe plain VS data flow. Much more complicated in fact; while a VS only outputsone thing (shaded vertices) with a 1:1 correspondence between unshaded andshaded vertices, a GS outputs<br>a variable number of vertices (up to a maximumthat’s specified at compile time), and as of D3D11 it can also have multipleoutput streams – however, a maximum of one stream can be sent on down the restof the pipeline, which is the path I’m talking about now.<br>The other destinationfor GS data (Stream-Out) will be covered in the next part.</p><p>A GS produces variable-sized output, but it needs to run with bounded memoryrequirements (among other things, the amount of memory available for buffersdetermines how many primitives can be Geometry Shaded in parallel), which iswhy the maximum number of output<br>vertices is fixed at compile-time. This(together with the number of written output attributes) determines how muchbuffer space is allocated, and thus indirectly the maximum number of parallelGS invocations; if that number is too low, latency can’t be fully<br>hidden, andthe GS will stall for some percentage of the time.</p><p>Also note that the GS inputs primitives (e.g. points, lines, triangles orpatches, optionally with adjacency information), but outputs vertices – eventhough we send primitives down to the rasterizer! If the output primitive typeis points, this is trivial. For<br>lines and triangles however, we need toreassemble those vertices back into primitives again. This is handled by makingthe output vertices form a line or triangle strip, respectively. This handleswhat are perhaps the 3 most important cases well: single lines,<br>triangles, orquads. It’s not so convenient if the GS tries to do some actual extrusion orgenerate otherwise “complicated” geometry, which often needs several “restartstrip” markers (which boils down to a single bit per vertex that denoteswhether the current<br>strip is continued or a new strip is started). So why thelimitation? At the API level, it seems fairly arbitrary – why can’t the GS justoutput a vertex list together with a small index buffer?</p><p>The answer boils down to two words: Primitive Assembly. This is what we’redoing here – taking a number of vertices and assembling them into a fullprimitive to send down the pipeline. But we already use that functional blockin this data path, just in front of<br>the GS. So for GS, we need a secondprimitive assembly stage, which we’d like to keep simple, and assemblingtriangle strips is very simple indeed: a triangle is always 3 vertices from theoutput buffer in sequential order, with only a bit of glue logic to keep<br>trackof the current winding order. In other words, strips are not significantly morecomplex to support than what is arguably the simplest primitive of all(non-indexed lines/triangles), but they still save output buffer space (andhence give us more potential<br>for parallelism) for typical primitives likequads.</p><p><strong>API order again</strong></p><p>There’s a few problems here, however: in the regular vertex shading path, weknow exactly how many primitives there are in a batch and where they are, evenbefore the shaded vertices arrive at the PA buffer – all this is fixed from thepoint where we set up the<br>batches to shade. If we, for example, have multipleunits for cull/clip/triangle setup, they can all start in parallel; they knowwhere to get their vertex data from, and they can know ahead of time which“sequence number” their triangle will have so it can all<br>be put into order.</p><p>For GS, we don’t generally know how many primitives we’re gonna generate beforewe get the outputs back – in fact, we might not have produced any! But we stillneed to respect API order: it’s first all primitives generated from GSinvocation 0, then all primitives<br>from invocation 1, and so on, through to theend of the batch (and of course the batches need to be processed in order too,same as with VS). So for GS, once we get results back, we first need to scanover the output data to determine the locations where complete<br>primitivesstart. Only then can we start doing cull, clip and triangle setup (potentiallyin parallel). More extra work!</p><p><strong>VPAI and RTAI</strong></p><p>These are two features added with GS that don’t actually affect Geometry Shaderexecution, but do have some effect on the processing further downstream, so Ithought I’d mention them here: The Viewport Array Index (here, VPAI for short)and Render-target Array<br>Index (RTAI). RTAI first, since it’s a bit easier toexplain: as you hopefully know, D3D10 adds support for texture arrays. Well,the RTAI gives you render-to-texture-array support: you set a texture array asrender target, and then in the GS you can select per-primitive<br>to which arrayindex the primitive should go. Note that because the GS is writing vertices notprimitives, we need to pick a single vertex that selects the RTAI (and alsoVPAI) per primitive; this is always the “leading vertex”, i.e. the firstspecified vertex<br>that belongs to a primitive. One example use case for RTAI isrendering cubemaps in one pass: the GS decides per primitive to which of thecube faces it should be sent (potentially several of them). VPAI is anorthogonal feature which allows you to set multiple<br>viewports and scissor rects(up to 15), and then decide per primitive which viewport to use. This can beused to render multiple cascades in a Cascaded Shadow Map in a single pass, forexample, and it can also be combined with RTAI.</p><p>As said, both features don’t affect GS processing significantly – they’re justextra data that gets tacked onto the primitive and then used later: the VPAIgets consumed during the viewport transform, while the RTAI makes it all theway down to the pixel pipeline.</p><p><strong>Summary so far</strong></p><p>Okay, so there’s some amount of trouble on the input end – we don’t fully getto pick our input data format, so we need extra buffering on the input data,and even then we have a variable amount of input primitives which we’re notnecessarily going to be able<br>to partition into nice big batches. And on theoutput end, we’re again assembling a variable number of primitives, don’tnecessarily know which GS will produce how many primitives in advance (thoughfor some GSs we’ll be able to determine this statically from<br>the compiled code,for example because all vertex emits are outside of flow control or insideloops with a known iteration count and no early-outs), and have to spend sometime parsing the output before we can send it on to triangle setup.</p><p>If that sounds more involved than what we had in the VS-only case, that’sbecause it is. This is why I mentioned above that it’s a mistake to think ofthe GS as something that always runs – even a very simple GS that does nothingexcept pass the current triangle<br>through goes through two more bufferingstages, an extra round of primitive assembly, and might execute on the shaderunits with poor utilization. All of this has a cost, and it tends to add up: Ichecked it when D3D10 hardware was fairly new, and on both AMD<br>and NVidiahardware, even a pure pass-through GS was between 3x and 7x slower than no GSat all (in a geometry-limited scenario, that is). I haven’t re-run thisexperiment on more recent hardware; I would assume that it’s gotten better bynow (this was the first<br>generation to implement GS, and features don’t usuallyhave good performance in the first GPU generation that implements them), butthe point still stands: just sending something through the GS pipe, even ifnothing at all happens there, has a very visible cost.</p><p>And it doesn’t help that GSs produce primitives as strips, sequentially; for aVertex Shader, we get one invocation per vertex, which reads one vertex andwrites one vertex (nice). For a GS, though, we might end up having only a batchof 11 GSs running (because<br>there wasn’t enough primitives in the input buffer),with each of them running fairly long and producing something like 8 outputvertices. That’s a long time to be running at low utilization! (Remember weneed somewhere between 16 and 64 independent jobs per<br>batch we dispatch to theshader units). It’s even more annoying if the GS mainly consists of a loop –for example, in the “render to cube map” case I mentioned for RTAI, we loopover the 6 faces in a cube, check if a triangle is visible on that face, andoutput<br>a triangle if that’s the case. The computations for the 6 faces arereally independent; if possible, we’d like to run them in parallel!</p><p><strong>Bonus: GS Instancing</strong></p><p>Well, enter GS Instancing, another feature new in D3D11 – poorly documented,sadly (and I’m not sure if there’s any good examples for it in the SDK). It’sfairly simple to explain, though: for each input primitive, the GS gets run notjust once but multiple times<br>(this is a static count selected at compile time).It’s basically equivalent to wrapping the whole shader in a</p><p>for (int i = 0; i &lt; N; i&#43;&#43;)</p><p>{</p><p>&nbsp; &nbsp; // …</p><p>}</p><p>block, only the loop is handled outside the shader by actually generatingmultiple GS invocations per input primitive, which helps us get larger batchsizes and thus better utilization. The i is exported to the shader as asystem-generated value (in D3D11, with<br>Semantic SV_GSInstanceID). So if youhave a GS like that, just get rid of the outer loop, add a [instances(N)]declaration and declare i as input with the right semantic and it’ll probablyrun faster for very little work on your part – the magic of giving moreindependent<br>jobs to a massively parallel machine!</p><p>Anyway, that’s it on Geometry Shaders. I’ve skipped Stream-Out, but this postis already long enough, and besides SO is a big enough topic (and independentenough of GS!) to warrant its own post. Next post, to be more precise. Untilthen!</p><p>&nbsp;</p><p>A trip through the GraphicsPipeline 2011, part 11</p><p>August 14, 2011</p><p>Welcome back! This time, the focus is going to be on Stream-Out (SO). This is afacility for storing the Output of the Geometry Shader stage to memory, insteadof sending it down the rest of the pipeline. This can be used to e.g. cacheskinned vertex data, or<br>as a sort of poor man’s Compute Shader on D3D10-levelhardware using the D3D10 API (note that with D3D11, you can just use CS 4.0,even on D3D10 hardware). And just like the GS Instancing I mentioned last time,some of this is very poorly described in the API<br>docs, so I’ll have a fewcomments about API usage even though it’s technically out of the intended scopeof this series.</p><p><strong>Vertex Shader Stream-Out (i.e. SO with NULL GS)</strong></p><p>This is one of the features that’s not properly explained in the D3D10 (orD3D11, for that matter) docs; in fact, it’s not mentioned there at all exceptfor a small throwaway remark in “Getting Started with the Stream-Output Stage(Direct3D 10)”. You’re supposed<br>to figure it out from the examples – whichthemselves don’t exactly go out of their way to make it clear what’s going on.That’s a pity – VS Stream-Out is easier than GS SO, and has some pretty usefulapplications by itself (e.g. caching skinned vertices).</p><p>So here’s how it’s done in D3D10 and 11: You simply pass Vertex Shader bytecode(instead of GS bytecode) to CreateGeometryShaderWithStreamOutput. Yes, the docsmention something about “Size of the compiled geometry shader” here – ignoreit. What you get back is<br>a Geometry Shader object that you can then pass toGSSetShader. This is, in effect, a NULL Geometry Shader – it doesn’t actuallygo through GS processing. It’s just some wrapper (more like duct tape really)to make it fit into the API model, where all rendering<br>passes through the GSstage and SO comes right after GS – though as I’ve explained last time, actualHW tends to skip the GS stage completely when there’s no GS set.</p><p>So the shaded vertices get assembled into primitives as before, but instead ofgetting sent down the rest of the pipeline as already described, they getforwarded to Stream-Out, where they arrive – as always – in a buffer. Whatexactly happens with them then depends<br>on the Stream-Out declaration (which ispassed at creation time). In the Stream-Out declaration, the app gets tospecify where it wants each output vector to end up in the Stream-Out targets(or SO targets for short). If the SO declaration “matches” the Vertex<br>ShaderOutput Declaration (i.e. the same attributes in the same order), data from theinput buffers can be streamed more or less unprocessed into memory. If itdoesn’t match the declaration exactly – it might skip some attributes writtenby the shader, or write<br>them in a different order – either way, there’s someextra reordering involved. This might involve a dedicated reordering unit(which basically implements a gather-type operation from the SO input buffers),or it might involve generating lots of small memory<br>writes instead of largeburst writes, or something similar. Either way, it’s extra effort and generallyslower; the details of what exactly triggers a slow path depend on the hardwarespecifics, but really, it doesn’t matter that much. If you want optimal SOperformance,<br>just make sure the SO declaration and Output declarations agree.</p><p>Another point is that SO usually doesn’t have access to a very high-performancepath to the memory subsystem. Unlike e.g. the ROPs, SO isn’t really (yet?) afull citizen in current GPU designs, so it often only has access to one memorychannel or something of<br>the sort. That’s something to keep in mind if you’reproducing a lot of data via SO. This is compounded by SO outputs always beingfull floats, so there’s no way to conserve bandwidth by using one of the packedvertex data types.</p><p>Final remark on VS SO: As I mentioned earlier, SO operates on assembledprimitives, not individual vertices. Note that Primitive Assembly discardsadjacency information if it makes it that far down the pipeline, and since thishappens before SO, vertices corresponding<br>to adjacency info won’t make it intoSO buffers either. SO working on primitives not individual vertices is relevantfor use cases like instancing a single skinned mesh (in a single pose) severaltimes. If you were to draw your triangle mesh as you usually would<br>and then useSO on that, this results in a data explosion – you get 3 unpacked, unsharedvertices per input primitive. This works, but isn’t exactly an efficient use ofbandwidth, both on the SO and the later vertex input side. Instead, you shoulddraw your triangle<br>mesh as a (non-indexed) point list in the first pass,thereby shading each vertex exactly once. The SO buffer then ends up in 1:1correspondence to your original vertex buffer, only with skinned instead of non-skinnedvertices. You can then use that vertex buffer<br>with your original primitivetopology and index buffer.</p><p><strong>Geometry Shader SO: Multiple streams</strong></p><p>This basically works like SO with a NULL GS, except there’s a Geometry Shaderinvolved, which adds some new capabilities (and complications). In the VS case,we just had one output stream (note that streams are a D3D11&#43; feature – theydon’t exist on D3D10-level<br>HW). That stream could be sent to SO or not, and itcould also be sent to down the pipeline to viewport/clip/cull or not, butthat’s it. But Geometry Shaders allow multiple streams, which makes outputrouting a bit more difficult.</p><p>Basically, every GS can write to (as of D3D11) up to 4 streams. Each stream maybe sent on to SO targets – yes, plural: a single stream can write to multipleSO targets, but a single SO target can receive values from only one stream,i.e. this is a one-to-many<br>relationship, not a fully general many-to-many one.The presence of streams has some implications for SO buffering – instead of asingle input buffer like I described in the NULL GS case, we now may havemultiple input buffers, one per stream. In addition to<br>SO targets, up to onestream may be sent down the pipe – i.e. the regular rendering pipeline and SOmay be used simultaneously.</p><p>As in the NULL GS case, SO works on primitives, not individual vertices – thatis, the strips you output in the GS get expanded out to full lines or trianglesbefore they get into SO.</p><p><strong>Tracking output size</strong></p><p>There’s another issue here: we don’t necessarily know how much output data isgoing to be produced from SO. For GS, this comes about because each GSinvocation may produce a variable number of output primitives; but even in thesimpler VS case, as soon as indexed<br>primitives are involved, the app might slipsome “primitive cut” indices in there that influence how many primitivesactually get written. This is a problem if we then want to draw from that SObuffer later, because we don’t know how many vertices are actually<br>in there! Wedo have an upper bound – the maximum capacity of the buffer as created – butthat’s it. Now, this could be resolved using some kind of query mechanism, butonce you think it through, that seems fairly backwards: at the point we’reusing the SO buffer<br>for drawing, we obviously do know how many primitives weactually wrote – the SO unit needs to keep track of its current outputposition, after all! If we employed some query mechanism, we would end uptransporting that single 32-bit value back over the bus to<br>the driver, whichpasses it on to the API, which passes it on to the app – which then immediatelydispatches another draw, going through all the layers again in the oppositedirection.</p><p>So that’s now how it’s solved. Instead, there’s DrawAuto. The idea is verysimple – the GPU already knows how many valid vertices it actually wrote to theoutput buffer; the SO unit keeps track of that while it’s writing, and thefinal counter is also kept in<br>memory (along with the buffer) since the app mayrender to a SO buffer in multiple passes. This counter is then used forDrawAuto, instead of having the app submit an explicit count itself –simplifying things considerably and avoiding the costly round-trip completely.Note<br>that this query mechanism does exist – both for checking the number ofvertices written and to determine whether an overflow occurred. But it’s not onthe critical path for rendering from SO buffers, which makes things a lotsimpler for driver developers.</p><p>And that’s it for SO, really. Not really a lot of HW info in this one, and notreally a super-interesting topic from a pipeline perspective, which is why ittook me so long to finish; sorry about that. Next up is Tessellation – thisshould be a lot quicker, since<br>it’s a fun topic :)</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>A trip through the Graphics Pipeline 2011, part 12</p><p>September 6, 2011</p><p>Welcome back! This time, we’ll look into what is perhaps the “poster boy”feature introduced with the D3D11 / Shader 5.x hardware generation:Tessellation. This one is interesting both because it’s a fun topic, andbecause it marks the first time in a long while<br>that a significant user-visiblecomponent has been added to the graphics pipeline that’s not programmable.</p><p>Unlike Geometry Shaders, which are conceptually quite easy (it’s just a shaderthat sees whole primitives as opposed to individual vertices), the topic of“Tessellation” requires some more explanation. There’s tons of ways totessellate geometry – to name just<br>the most popular ones, there’s SplinePatches in dozens of flavors, various types of Subdivision Surfaces, andDisplacement Mapping – so from the bullet point “Tessellation” alone it’s notat all obvious what services the GPU provides us with, and how they areimplemented.</p><p>To describe how hardware tessellation works, it’s probably easiest to start inthe middle – with the actual primitive tessellation step, and the variousrequirements that apply to it. I’ll get to the new shader types (Hull Shadersand Domain Shaders in D3D11 parlance,<br>Tessellation Control Shader andTessellation Evaluation Shader in OpenGL 4.0 lingo) later.</p><p><strong>Tessellation – not quite like you’d expect</strong></p><p>Tessellation as implemented by Shader 5.x class HW is of the “patch-based”variety. Patch types in the CG literature are mostly named by what kind offunction is used to construct the tessellated points from the control points(B-spline patches, Bézier triangles,<br>etc.). But we’ll ignore that part for now,since it’s handled in the new shader types. The actual fixed-functiontessellation unit deals only with the topology of the output mesh (i.e. howmany vertices there are and how they’re connected to each other); and<br>it turnsout that from this perspective, there’s basically only two different types ofpatches: quad-based patches, which are defined on a parameter domain with twoorthogonal coordinate axes (which I’ll call u and v here, both are in [0,1])and usually constructed<br>as a tensor product of two one-parameter basisfunctions, and triangle-based patches, which use a redundant representationwith three coordinates (u, v, w) based on barycentric coordinates (i.e. u, v, w\ge 0, u &#43; v &#43; w = 1). In D3D11 parlance, these are the<br>“quad” and “tri”domains, respectively. There’s also an “isoline” domain which instead of a 2Dsurface produces one or multiple 1D curves; I’ll treat it the same way as I didlines and point primitives throughout this series: I acknowledge its existencebut won’t<br>go into further detail.</p><p>Tessellated primitives can be drawn naturally in their respective domaincoordinate systems. For quads, the obvious choice of drawing the domain is as aunit square, so that’s what I’ll use; for triangles, I’ll use an equilateraltriangle to visualize things.<br>Here’s the coordinate systems I’ll be using inthis post with both the vertices and edges labeled:</p><p>Anyway, both triangles and quads have what I would consider a “natural” way totessellate them, depicted below. But it turns out that’s not actually the meshtopology you get.</p><p>Here’s the&nbsp;<em>actual</em>&nbsp;meshes that the tessellator will produce forthe given input parameters:</p><p>For quads, this is (roughly) what we’re expecting – except for some flippeddiagonals, which I’ll get to in a minute. But the triangle is a completelydifferent beast. It’s got a very different topology from the “natural”tessellation I showed above, including<br>a different number of vertices (12instead of 10). Clearly, there’s something funny going on here – and thatsomething happens to be related to the way transitions between differenttessellation levels are handled.</p><p><strong>Making ends meet</strong>The elephant in the room is handling transitions betweenpatches. Tessellating a single triangle (or quad) is easy, but we want to beable to determine tessellation factors per-patch, because we only want to spendtriangles where<br>we need them – and not waste tons of triangles on some distant(and possibly backface-culled) parts of the mesh. Additionally, we want to beable to do this quickly and ideally without extra memory usage; that means aglobal fix-up post-pass or something of that<br>caliber is out of the question.</p><p>The solution – which you’ve already encountered if you’ve written a Hull orDomain shader – is to make all of the actual tessellation work purely local andpush the burden of ensuring watertightness for the resulting mesh down to theshaders. This is a topic all<br>by itself and requires, among other things,&nbsp;<a href="http://www.ludicon.com/castano/blog/2010/09/precise/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">great care in the Domain Shader code</span></a>; I’ll skip all the detailsabout expression<br>evaluation in shaders and stick with the basics. The basicmechanism is that each patch has multiple tessellation factors (TFs), which arecomputed in the Hull Shader: one or two for the actual inside of the patch,plus one for each edge. The TFs for the inside<br>of the patch can be chosenfreely; but if two patches share an edge, they’d better compute the exact sameTFs along that edge, or there will be cracks. The hardware doesn’t care – itwill process each patch by itself. If you do everything correctly, you’ll get<br>anice watertight mesh, otherwise – well, that’s your problem. All the HW needsto make sure is that it’s&nbsp;<em>possible</em>&nbsp;to get watertight meshes,preferably with reasonable efficiency. That by itself turns out to be tricky insome places; I’ll get to that<br>later.</p><p>So, here are some new reference patches – this time with different TFs alongeach edge so we can see how that works:</p><p>I’ve colored the areas influenced by the different edge tessellation factors;the uncolored center part in the middle only depends on the inside TFs. Inthese images, the u=0 (yellow) edge has a TF of 2, the v=0 (green) edge has aTF of 3, the u=1 / w=0 (pink)<br>edge has a TF of 4, and the v=1 (quad only, cyan)edge has a TF of 5 – exactly the number of vertices along the correspondingouter edge. As should be obvious from these two images, the basic buildingblock for topology is just a nice way to stitch two subdivided<br>edges withdifferent number of vertices to each other. The details of this are somewhattricky, but not particularly interesting, so I won’t go into it.</p><p>As for the inside TFs, quads are fairly easy: The quad above has an inside TFof 3 along u and 4 along v. The geometry is basically that of a regular grid ofthat size, except with the first and last rows/columns replaced by the respectivestitching triangles<br>(if any edge has a TF of 1, the resulting mesh will havethe same structure as if the inside TFs for u/v were both 2, even if they’resmaller than that). Triangles are a bit more complicated. Odd TFs we’ve alreadyseen – for a TF of&nbsp;, they produce a mesh consisting<br>of&nbsp;&nbsp;concentric rings, the innermostof which is a single triangle. For even TFs, we get&nbsp;concentric rings with a center vertexinstead of a center triangle. Below is an image of the simplest even case,&nbsp;, which consists just of edge stitchesplus the center vertex.</p><p>Finally, when triangulating quads, the diagonal is generally chosen to pointaway from the center of the patch (in the domain coordinate space), with aconsistent tie-breaking rule. This is simply to ensure maximum rotationalsymmetry of the resulting meshes –<br>if there’s extra degrees of freedom, mightas well use them!</p><p><strong>Fractional tessellation factors and overall pipeline flow</strong>So far, I’veonly talked about integer TFs. In two of the so-called “partitioning types”,namely “Integer” and “Pow2″, that’s all the Tessellator sees. If the shadergenerates a non-integer<br>(or, respectively, non-power-of-2) TF, it will simplyget rounded up to the next acceptable value. More interesting are the remainingtwo partitioning types: Fractional-odd and Fractional-even tessellation.Instead of jumping from tessellation factor to tessellation<br>factor (which wouldcause visible pops), new vertices start out at the same position as an existingvertex in the mesh and then gradually move to their new positions as the TFincreases.</p><p>For example, with fractional-odd tessellation, if you were to use an inner TFof 3.001 for the above triangle, the resulting mesh would look very much likethe mesh for a TF of 3 – but topologically, it’d be the same as if the TF was5, i.e. it’s a patch with<br>3 concentric rings, even though the middle ring isvery narrow. Then as the TF gets closer to 5, the middle ring expands until itis eventually at its final position for TF 5. Once you raise the TF past 5, themesh will be topologically the same as is the TF<br>was 7, but again with a numberof almost-degenerate triangles in the middle, and so forth. Fractional-eventessellation uses the same principle, just with even TFs.</p><p>The output of the tessellator then consists of two things: First, the positionsof the tessellated vertices in domain coordinates, and second, thecorresponding connectivity information – basically an index buffer.</p><p>Now, with the basic function of the fixed-function tessellator unit explained,let’s step back and see what we need to do to actually churn out primitives:First, we need to input a bunch of input control points comprising a patch intothe Hull Shader. The HS<br>then computes output control points and “patchconstants” (both of which get passed down to the Domain Shader), plus allTessellation Factors (which are essentially just more patch constants). Then werun the fixed-function tessellator, which gives us a bunch<br>of Domain Positionsto run the Domain Shader at, plus the associated indices. After we’ve run theDS, we then do another round of primitive assembly, and then send theprimitives either down to the GS pipeline (if it’s active) or Viewport transform,Clip and Cull<br>(if not).</p><p>So let’s look a bit into the HS stage.</p><p><strong>Hull Shader execution</strong>Like&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/20/a-trip-through-the-graphics-pipeline-2011-part-10/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Geometry Shaders</span></a>, Hull Shaders work on full<br>(patch) primitives as input –with all the input buffering headaches that causes. How much of a headacheentirely depends on the type of input patch. If the patch type is somethinglike a cubic Bézier patch, we need 4×4 = 16 input points&nbsp;<em>per patch</em>&nbsp;andmight<br>just produce a single quad of output (or even none at all, if the patchis culled); clearly, that’s a somewhat awkward amount of data to work with, anddoesn’t lend itself to very efficient shading. On the other hand, iftessellation takes plain triangles as<br>input (which a lot of people do), inputbuffering is pretty tame and not likely to be a source of problems orbottlenecks.</p><p>More importantly, unlike Geometry Shaders (which run for every primitive), HullShaders don’t run all that often – they run once&nbsp;<em>per patch</em>, and aslong as there’s any actual tessellation going on (even at modest TFs), we haveway less patches than we<br>have output triangles. In other words, even when HSinput is somewhat inefficient, it’s less of an issue than in the GS case simplybecause we don’t hit it that often.</p><p>The other nice attribute of Hull Shaders is that, unlike Geometry Shaders, theydon’t have a variable amount of output data; they produce a fixed amount ofcontrol points, each which a fixed amount of associated attributes, plus afixed amount of patch constants.<br>All of this is statically known at compiletime; no dynamic run-time buffer management necessary. If we Hull Shade 16hulls at a time, we know exactly where the data for each hull will end upbefore we even start executing the shader. That’s definitely an advantage<br>overGeometry Shaders; for lots of Geometry Shaders, it’s possible to knowstatically how many output vertices will be generated (for example because allthe control flow leading to emit / cut instructions can be statically evaluatedat compile time), and for<br>all of them, there’s a guaranteed maximum number ofoutput vertices, but for HS, we have a guaranteed fixed amount of output data,no additional analysis required. In short, there’s no problems with outputbuffer management, other than the fact that, again depending<br>on the primitivetype, we might need lots of output buffer space which limits the amount ofparallelism we can achieve (due to memory/register constraints).</p><p>Finally, Hull Shaders are somewhat special in the way they are compiled inD3D11; all other shader types basically consist of one block of code (with somesubroutines maybe), but Hull Shaders are generated factored into multiplephases, each of which can consist<br>of multiple (independent) threads ofexecution. The details are mainly of interest to driver and shader compilerprogrammers, but suffice it to say that your average HS comes packaged in aform that exposes lots of latent parallelism, if there is any. It certainlyseems<br>like Microsoft was really keen to avoid the bottlenecks that plagueGeometry Shaders this time around.</p><p>Anyway, Hull Shaders produce a bunch of output per patch; most of it is justkept around until the corresponding Domain Shaders run, except for the TFs,which get sent to the tessellator unit. If any of the TFs are less than orequal to zero (or NaN), the patch<br>is culled, and the corresponding controlpoints and patch constants silently get thrown away. Otherwise, the Tessellator(which implements the functionality described above) kicks in, reads thejust-shaded patches, and starts churning out domain point positions<br>andtriangle indices, and we need to get ready for DS execution.</p><p><strong>Domain Shaders</strong>Just like for&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/03/a-trip-through-the-graphics-pipeline-2011-part-3/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Vertex Shading</span></a>&nbsp;way back, we want to gather<br>multiple domainvertices into one batch that we shade together and then pass on the PA. Thefixed-function tessellator can take care of this: “just” handle it along withproducing vertex positions and indices (I put the “just” in quotes here becausethis does<br>involve some amount of bookkeeping).</p><p>In terms of input and output, Domain Shaders are very simple indeed: the onlyinput they get that actually varies per vertex is the domain point u and vcoordinates (w, when used, doesn’t need to be computed or passed in by thetesselator; since&nbsp;, it can be computed<br>as&nbsp;). Everything else is either patchconstants, control points (all of which are the same across a patch) orconstant buffers. And output is basically the same as for Vertex Shaders.</p><p>In short, once we get to the DS, life is good; the data flow is almost assimple as for VS, which is a path we know how to run efficiently. This isperhaps the biggest advantage of the D3D11 tessellation pipeline over GeometryShaders: the actual triangle amplification<br>doesn’t happen in a shader, where wewaste precious ALU cycles and need to keep buffer space for a worst-caseestimate of vertices, but in a localized element (the tessellator) that isbasically a state machine, gets very little input (a few TFs) and produces<br>verycompact output (effectively an index buffer, plus a 2D coordinate per outputvertex). Because of this, we need way less memory for buffering, and can keepour Shader Units busy with actual shading work instead of housekeeping.</p><p>And that’s it for this post – next up: Compute Shaders, aka the final part inmy original outline for this series! Until then.</p><p><strong>Final remarks</strong>As usual, I cut a few corners. There’s the “isoline” patchtype, which I didn’t go into at all (if there’s any demand for this, I canwrite it up). The Tessellator has all kinds of symmetry and precisionrequirements; as far as vertex<br>domain positions are concerned, you canbasically expect bit-exact results between the different HW vendors, becausethe D3D11 spec really nails this bit down. What’s intentionally not nailed downis the order in which vertices or triangles are produced – an<br>implementationcan do what it wants there, provided it does so consistently (i.e. the sameinput has to produce the same output, always). There’s a bunch of subtleconstraints that go into this too – for example, all domain positions writtenby the Tessellator<br>need to have both u and 1-u (and also v and 1-v) exactlyrepresentable as float; there’s a bunch of necessary conditions like this sothat Domain Shaders can then produce watertight meshes (this rule in particularis important so that a shared edge AB between<br>two patches, which is AB to onepatch and BA to the other, can get tessellated the same way for both patches).</p><p>Writing Domain Shaders so they actually can’t produce cracks is tricky andrequires great care; I intentionally sidestep the topic because it’s outsidethe scope of this series. Another much more trivial issue that I didn’t mentionis the winding order of triangles<br>generated by the Tessellator (answer: it’s upto the App – both clockwise and counterclockwise are supported).</p><p>The description of Input/Output buffering for Hull and Domain shaders issomewhat terse, but it’s very similar to stages we’ve already seen, so I’drather keep it short and avoid extra clutter; re-read the posts on VertexShaders and Geometry Shaders if this was<br>too fast.</p><p>Finally, because the Tesselation pipeline can feed into the GS, there’s thequestion of whether it can generate adjacency information. For the “inside” ofpatches this would be conceivable (just more indices for the Tessellator unitto write), but it gets ugly<br>fast once you reach patch edges, since cross-patchadjacency needs exactly the kind of global “mesh awareness” that theTessellation pipeline design tries so hard to avoid. So, long story short, no,the tessellator will not produce adjacency information for the<br>GS, just plaintriangles.</p><p><strong>A trip through the GraphicsPipeline 2011, part 13</strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; October 9, 2011&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; Welcome back to what’s going to be the last “official” part of this series– I’ll do more GPU-related posts in the future, but this series is long enoughalready.<br>We’ve been touring all the regular parts of the graphics pipeline,down to different levels of detail. Which leaves one major new featureintroduced in DX11 out: Compute Shaders. So that’s gonna be my topic this timearound.</p><p><strong>Execution environment</strong>For this series, the emphasis has been on overalldataflow at the architectural level, not shader execution (which is explainedwell elsewhere). For the stages so far, that meant focusing on the input pipedinto and output<br>produced by each stage; the way the internals work was usuallydictated by the shape of the data. Compute shaders are different – they’rerunning by themselves, not as part of the graphics pipeline, so the surfacearea of their interface is much smaller.</p><p>In fact, on the input side, there’s not really any buffers for input data atall. The only input Compute Shaders get, aside from API state such as the boundConstant Buffers and resources, is their thread index. There’s a tremendouspotential for confusion here,<br>so here’s the most important thing to keep inmind: a “thread” is the atomic unit of dispatch in the CS environment, and it’sa substantially different beast from the threads provided by the OS that youprobably associate with the term. CS threads have their<br>own identity andregisters, but they don’t have their own Program Counter (Instruction Pointer)or stack, nor are they scheduled individually.</p><p>In fact, “threads” in CS take the place that individual vertices had during&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/03/a-trip-through-the-graphics-pipeline-2011-part-3/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Vertex Shading</span></a>,<br>or individual pixels during&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/10/a-trip-through-the-graphics-pipeline-2011-part-8/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">Pixel Shading</span></a>. And they get treated the same way: assemble<br>a bunch ofthem (usually, somewhere between 16 and 64) into a “Warp” or “Wavefront” andlet them run the same code in lockstep. CS threads don’t get scheduled – Warpsand Wavefronts do (I’ll stick with “Warp” for the rest of this article; mentallysubstitute “Wavefront”<br>for AMD). To hide latency, we don’t switch to adifferent “thread” (in CS parlance), but to a different Warp, i.e. a differentbundle of threads. Single threads inside a Warp can’t take branchesindividually; if at least one thread in such a bundle wants to execute<br>acertain piece of code, it gets processed by all the threads in the bundle –even if most threads then end up throwing the results away. In short, CS“threads” are more like SIMD lanes than like the threads you see elsewhere inprogramming; keep that in mind.</p><p>That explains the “thread” and “warp” levels. Above that is the “thread group”level, which deals with – who would’ve thought? – groups of threads. The sizeof a thread group is specified during shader compilation. In DX11, a threadgroup can contain anywhere<br>between 1 and 1024 threads, and the thread groupsize is specified not as a single number but as a 3-tuple giving thread x, y,and z coordinates. This numbering scheme is mostly for the convenience of shadercode that addresses 2D or 3D resources, though it also<br>allows for traversaloptimizations. At the macro level, CS execution is dispatched in multiples ofthread groups; thread group IDs in D3D11 again use 3D group IDs, same as threadIDs, and for pretty much the same reasons.</p><p>Thread IDs – which can be passed in in various forms, depending on what theshader prefers – are the only input to Compute Shaders that’s not the same forall threads; quite different from the other shader types we’ve seen before. Thisis just the tip of the iceberg,<br>though.</p><p><strong>Thread Groups</strong>The above description makes it sound like thread groups area fairly arbitrary middle level in this hierarchy. However, there’s oneimportant bit missing that makes thread groups very special indeed: ThreadGroup Shared Memory (TGSM).<br>On DX11 level hardware, compute shaders have accessto 32k of TGSM, which is basically a scratchpad for communication betweenthreads in the same group. This is the primary (and fastest) way by whichdifferent CS threads can communicate.</p><p>So how is this implemented in hardware? It’s quite simple: all threads (well,Warps really) within a thread group get executed by the same shader unit. Theshader unit then simply has at least 32k (usually a bit more) of local memory.And because all grouped threads<br>share the same shader unit (and hence the sameset of ALUs etc.), there’s no need to include complicated arbitration orsynchronization mechanisms for shared memory access: only one Warp can accessmemory in any given cycle, because only one Warp gets to issue<br>instructions inany cycle! Now, of course this process will usually be pipelined, but thatdoesn’t change the basic invariant: per shader unit, we have exactly one pieceof TGSM; accessing TGSM might require multiple pipeline stages, but actualreads from (or<br>writes to) TGSM will only happen inside one pipeline stage, andthe memory accesses during that cycle all come from within the same Warp.</p><p>However, this is not yet enough for actual shared-memory communication. Theproblem is simple: The above invariant guarantees that there’s only one set ofaccesses to TGSM per cycle even when we don’t add any interlocks to preventconcurrent access. This is nice<br>since it makes the hardware simpler and faster.It does not guarantee that memory accesses happen in any particular order fromthe perspective of the shader program, however, since Warps can be scheduledmore or less randomly; it all depends on who is runnable<br>(not waiting formemory access / texture read completion) at certain points in time. Somewhatmore subtle, precisely because the whole process is pipelined, it might takesome cycles for writes to TGSM to become “visible” to reads; this happens whenthe actual<br>read and write operations to TGSM occur in different pipeline stages(or different phases of the same stage). So we still need some kind ofsynchronization mechanism. Enter barriers. There’s different types of barriers,but they’re composed of just three fundamental<br>components:</p><p>1.&nbsp;&nbsp;&nbsp;&nbsp; <em>Group Synchronization</em>. A Group SynchronizationBarrier forces all threads inside the current group to reach the barrier beforeany of them may consume past it. Once a Warp reaches such a barrier, it will beflagged as non-runnable, same as if<br>it was waiting for a memory or textureaccess to complete. Once the last Warp reaches the barrier, the remaining Warpswill be reactivated. This all happens at the Warp scheduling level; it addsadditional scheduling constraints, which may cause stalls, but there’s<br>no needfor atomic memory transactions or anything like that; other than lostutilization at the micro level, this is a reasonably cheap operation.</p><p>2.&nbsp;&nbsp;&nbsp;&nbsp; <em>Group Memory Barriers</em>. Since all threads within agroup run on the same shader unit, this basically amounts to a pipeline flush,to ensure that all pending shared memory operations are completed. There’s noneed to synchronize with resources<br>external to the current shader unit, whichmeans it’s again reasonably cheap.</p><p>3.&nbsp;&nbsp;&nbsp;&nbsp; <em>Device Memory Barriers</em>. This blocks all threadswithin a group until all memory accesses have completed – either direct orindirect (e.g. via texture samples). As explained earlier in this series,memory accesses and texture samples on GPUs<br>have long latencies – think morethan 600, and often above 1000 cycles – so this kind of barrier will reallyhurt.</p><p>DX11 offers different types ofbarriers that combine several of the above components into one atomic unit; thesemantics should be obvious.</p><p><strong>Unordered Access Views</strong>We’ve now dealt with CS input and learned a bitabout CS execution. But where do we put our output data? The answer has theunwieldy name “unordered access views”, or UAVs for short. An UAV seemssomewhat similar to render<br>targets in Pixel Shaders (and UAVs can in fact beused in addition to render targets in Pixel Shaders), but there’s some veryimportant semantic differences:</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Most importantly, as the same suggests, access to UAVs is“unordered”, in the sense that the API does not guarantee accesses to becomevisible in any particular order. When rendering primitives, quads are guaranteedto be Z-tested, blended and written<br>back in API order (as discussed in detailin&nbsp;<a href="http://fgiesen.wordpress.com/2011/07/12/a-trip-through-the-graphics-pipeline-2011-part-9/" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">part 9 of this series</span></a>), or at least produce<br>the same results as if theywere – which takes substantial effort. UAVs make no such effort – UAV accesseshappen immediately as they’re encountered in the shader, which may be verydifferent from API order. They’re not<em>completely</em>&nbsp;unordered, though;while<br>there’s no guaranteed order of operations within an API call, the API anddriver will still collaborate to make sure that perceived sequential orderingis preserved across API calls. Thus, if you have a complex Compute Shader (orPixel Shader) writing to an UAV<br>immediately followed by a second (simpler) CSthat reads from the same underlying resource, the second CS will see thefinished results, never some partially-written output.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UAVs support random access. A Pixel Shader can only writeto one location per render target – its corresponding pixel. The same PixelShader can write to arbitrary locations in whatever UAVs it has bound.</p><p>·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UAVs support atomic operations. In the classic PixelPipeline, there’s no need; we guarantee there’s never any collisions anyway.But with the free-form execution provided by UAVs, different threads might betrying to access a piece of memory at the<br>same time, and we needsynchronization mechanisms to deal with this.</p><p>So from a “CPU programmer”‘s point of view, UAVscorrespond to regular RAM in a shared-memory multiprocessing system; they’rewindows into memory. More interesting is the issue of atomic operations; thisis one area where current GPUs diverge considerably from<br>CPU designs.</p><p><strong>Atomics</strong>In current CPUs, most of the magic for shared memory processingis handled by the memory hierarchy (i.e. caches). To write to a piece ofmemory, the active core must first assert exclusive ownership of thecorresponding cache line. This<br>is accomplished using what’s called a “cachecoherency protocol”, usually&nbsp;<a href="http://en.wikipedia.org/wiki/MESI_protocol" rel="external nofollow noopener noreferrer" target="_blank"><span style="color:#369">MESI</span></a>&nbsp;and descendants. Thedetails are tangential to this article;<br>what matters is that because writing tomemory entails acquiring exclusive ownership, there’s never a risk of two coressimultaneously trying to write to the some location. In such a model, atomicoperations can be implemented by holding exclusive ownership for<br>the durationof the operation; if we had exclusive ownership for the whole time, there’s nochance that someone else was trying to write to the same location while we wereperforming the atomic operation. Again, the actual details of this get hairypretty fast<br>(especially as soon as things like paging, interrupts andexceptions get involved), but the 30000-feet-view will suffice for the purposesof this article.</p><p>In this type of model, atomic operations are performed using the regular CoreALUs and load/store units, and most of the “interesting” work happens in thecaches. The advantage is that atomic operations are (more or less) regularmemory accesses, albeit with some<br>extra requirements. There’s a couple ofproblems, though: most importantly, the standard implementation of cachecoherency, “snooping”, requires that all agents in the protocol talk to eachother, which has serious scalability issues. There are ways around thisrestriction<br>(mainly using so-called Directory-based Coherency protocols), butthey add additional complexity and latency to memory accesses. Another issue isthat all locks and memory transactions really happen at the cache line level;if two unrelated but frequently-updated<br>variables share the same cache line, itcan end up “ping-ponging” between multiple cores, causing tons of coherencytransactions (and associated slowdown). This problem is called “false sharing”.Software can avoid it by making sure unrelated fields don’t fall<br>into the samecache line; but on GPUs, neither the cache line size nor the memory layoutduring execution is known or controlled by the application, so this problemwould be more serious.</p><p>Current GPUs avoid this problem by structuring their memory hierarchydifferently. Instead of handling atomic operations inside the shader units(which again raises the “who owns which memory” issue), there’s dedicatedatomic units that directly talk to a shared<br>lowest-level cache hierarchy.There’s only one such cache, so the issue of coherency doesn’t come up; eitherthe cache line is present in the cache (which means it’s current) or it isn’t(which means the copy in memory is current). Atomic operations consist of<br>firstbringing the respective memory location into the cache (if it isn’t therealready), then performing the required read-modify-write operation directly onthe cache contents using a dedicated integer ALU on the atomic units. While anatomic unit is busy on<br>a memory location, all other accesses to that locationwill stall. Since there’s multiple atomic units, it’s necessary to make surethey never try to access the same memory location at the same time; one easyway to accomplish this is to make each atomic unit<br>“own” a certain set ofaddresses (statically – not dynamically as with cache line ownership). This isdone by computing the index of the responsible atomic unit as some hashfunction of the memory address to be accessed. (Note that I can’t confirm thisis how<br>current GPUs do; I’ve found little detail on how the atomic units workin official docs).</p><p>If a shader unit wants to perform an atomic operation to a given memoryaddress, it first needs to determine which atomic unit is responsible, waituntil it is ready to accept new commands, and then submit the operation (andpotentially wait until it is finished<br>if the result of the atomic operation isrequired). The atomic unit might only be processing one command at a time, orit might have a small FIFO of outstanding requests; and of course there’s allkinds of allocation and queuing details to get right so that atomic<br>operationprocessing is reasonably fair so that shader units will always make progress.Again, I won’t go into further detail here.</p><p>One final remark is that, of course, outstanding atomic operations count as“device memory” accesses, same as memory/texture reads and UAV writes; shaderunits need to keep track of their outstanding atomic operations and make surethey’re finished when they hit<br>device memory access barriers.</p><p><strong>Structured buffers and append/consume buffers</strong>Unless I missed something,these two buffer types are the last CS-related features I haven’t talked aboutyet. And, well, from a hardware perspective, there’s not that much to talkabout, really. Structured<br>buffers are more of a hint to the driver-internalshader compiler than anything else; they give the driver some hint as to howthey’re going to be used – namely, they consist of elements with a fixed stridethat are likely going to be accessed together – but<br>they still compile down toregular memory accesses in the end. The structured buffer part may bias thedriver’s decision of their position and layout in memory, but it does not addany fundamentally new functionality to the model.</p><p>Append/consume buffers are similar; they could be implemented using theexisting atomic instructions. In fact, they kind of are, except theappend/consume pointers aren’t at an explicit location in the resource, they’reside-band data outside the resource that<br>are accessed using special atomic instructions.(And similarly to structured buffers, the fact that their usage is declared asappend/consume buffer allows the driver to pick their location in memoryappropriately).</p><p><strong>Wrap-up.</strong>And… that’s it. No more previews for the next part, this seriesis done :), though that doesn’t mean I’m done with it. I have somerestructuring and partial rewriting to do – these blog posts are raw andunproofed, and I intend to go over<br>them and turn it into a single document. Inthe meantime, I’ll be writing about other stuff here. I’ll try to incorporatethe feedback I got so far – if there’s any other questions, corrections orcomments, now’s the time to tell me! I don’t want to nail down<br>the ETA for thefinal cleaned-up version of this series, but I’ll try to get it down wellbefore the end of the year. We’ll see. Until then, thanks for reading!</p><p>&nbsp;</p><p>&nbsp;</p><p><br>本文地址 <a href="http://yjaelex.github.io/2015/09/29/转-转-A-trip-through-the-Graphics-Pipeline/">http://yjaelex.github.io/2015/09/29/转-转-A-trip-through-the-Graphics-Pipeline/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;转一个国外牛人的文章；深入浅出的介绍了现代GPU的方方面面，从软件到硬件都有。理解了这些对理解新一代图形API大有好处！&lt;img src=&quot;http://static.blog.csdn.net/xheditor/xheditor_emot/default/smile.gif&quot; alt=&quot;微笑&quot;&gt;&lt;/p&gt;&lt;p&gt;原文&amp;nbsp;&lt;a href=&quot;https://fgiesen.wordpress.com/2011/07/09/a-trip-through-the-graphics-pipeline-2011-index/&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;A trip through the Graphics Pipeline 2011: Index&lt;/a&gt;&lt;/p&gt;&lt;p&gt;PDF下载：&lt;a href=&quot;http://download.csdn.net/detail/qwertyu1234/9041011&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;http://download.csdn.net/detail/qwertyu1234/9041011&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[原]AMD Mantle API 学习笔记 -- Mantle简介</title>
    <link href="http://yjaelex.github.io/2015/09/27/%E5%8E%9F-AMD-Mantle-API-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Mantle%E7%AE%80%E4%BB%8B/"/>
    <id>http://yjaelex.github.io/2015/09/27/原-AMD-Mantle-API-学习笔记-Mantle简介/</id>
    <published>2015-09-27T02:30:56.000Z</published>
    <updated>2016-12-29T08:40:21.200Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><p>最近一段时间准备学习新的下一代graphics API，DX12和Vulkan。发现目前公开的资料不多，特别是Vulkan，kronos的速度也真慢，正式的Spec还没有出来。倒是LunarG出了个SDK和其在intel平台上实现的Vulkan driver，只是笔者一直没找到公开的代码。</p><p>其实，无论DX12还是Vulkan都是来自AMD的Mantle，而AMD也已经放出了公开的<a href="http://www.amd.com/Documents/Mantle-Programming-Guide-and-API-Reference.pdf" title="Mantle API" rel="external nofollow noopener noreferrer" target="_blank">Mantle API Refernce Doc</a>. 这份文档写的很不错，条理清晰很有价值；笔者建议有兴趣的朋友一定要拜读一下。于是笔者决定先从新标准的鼻祖mantle入手学习，结合笔者多年图形学工作的经验，希望能对下一代API能有个叫深刻的认识。在此也把自己学习体会和大家分享一下，当做读书笔记吧：）</p><a id="more"></a><h2 id="Mantle-简介"><a href="#Mantle-简介" class="headerlink" title="- Mantle 简介"></a>- Mantle 简介</h2><p>mantle设计的理念和以往的API（D3D、OpenGL）完全不同；它是一个对GPU的更低级别的抽象，暴露了很多低级接口。比如mantle可以让game开发者直接操作Command Buffer；可以自己管理video memory。这中更低级的编程模型可以降低CPU的负担，从而得到更好的性能；这使得PC上的game编程更像game console了。</p><p>Mantle大大降低了驱动的复杂度，让很多原本在驱动中做的工作移到了应用程序里。这样的好处是应用程序可以根据自身特性，利用Mantle暴露的低级别的系统接口函数，实现符合自己的效率更高的优化。相对于D3D，OpenGL，Mantle对CPU的利用效率更高；在对CPU要求高的游戏里，Performance一般大大好于D3D和OpenGL。</p><h2 id="Mantel-系统架构"><a href="#Mantel-系统架构" class="headerlink" title="Mantel 系统架构"></a>Mantel 系统架构</h2><p><img src="http://img.blog.csdn.net/20150909173352344" alt="Mantle软件架构图"></p><p>整个架构主要由两个模块组成：Loader和ICD驱动。</p><ol><li>Loader有点像OpenGL里面的LibGL或者OpenGL ES里的LibEGL；其主要任务是枚举系统中所有的GPU并加载该GPU对应的Vendor提供的ICD驱动。Loader应该是完全硬件无关的，它的实现是依赖于具体操作系统的。</li><li>ICD驱动是硬件供应商提供的一个动态库；它可以作用于一组GPUs（比如系统中所有AMD的GPU应该共享一个ICD；而Intel的GPU会用另一个）。</li><li>Shader编译库。不是必须的；其主要功能是把高级Shader语言（类C++），编译成一个Mantle可以识别的中间语言（ASM）。</li></ol><p>这里ICD又可以大致分为三个主要模块：</p><ul><li>Validation Layer（VL）。这是一个用于调试的一层，主要是用于验证应用程序API跳用的合法性。Mantle设计理念是简单快速；所以它默认只包含很少的错误检测。在Game开发阶段，我们可以打开VL来帮助调试程序。</li><li>窗口系统。这个和OpenGL一样。OGL标准只是定义了一组图形API；你可以用它来做离屏渲染。为了要将结果显示在屏幕上，我们就必须调用一组窗口系统函数（比如SwapBuffer）。这个窗口系统的借口在不同OS上有不同标准：Windows是wgl，LNX X-Window是GLX，Android上是EGL。Manle里面Core部分也是不知道具体窗口系统的；必须通过extension（扩展）的方式来显示渲染结果。</li><li>ICD的Core部分主要是将应用程序发送来的Cmd Buffers翻译成硬件相关的Cmds，并且和OS协作来把这些Cmds送给GPU去执行。</li></ul><h2 id="运行模型（EXECUTION-MODEL）"><a href="#运行模型（EXECUTION-MODEL）" class="headerlink" title="运行模型（EXECUTION MODEL）"></a>运行模型（EXECUTION MODEL）</h2><p>Mantle的模型和以往D3D或OpenGL完全不同，见下图。这里有几个关键概念：GPU的Engine，cmd buffer和cmd buffer queues。</p><ul><li>Engine。现在的GPU上其实有很多不同功能的可以执行的engine；它们都是并行的，也就是说不同engines之间可能需要同步。目前主要有Graphics queue（图形渲染），DMA（video memory之间复制数据）和Compute（通用计算）。</li><li>Cmd Buffer。是一段Video memory，其内容是GPU可以执行的命令（如设置寄存器或Draw，等等）。Mantle里应用程序可以直接操作cmd buffer。</li><li>Cmd Buffer queue。是一个队列（FIFO或者Ring，和具体实现有关）。队列中的每一项都指向一个Cmd Buffer。Queue和Engine有关。<br><img src="http://img.blog.csdn.net/20150909182025091" alt="EXECUTION MODEL"><br><strong>所以Mantle的基本编程模型是Apps主动直接地创建并填写Cmd Buffers（可以是多线程的）；在适当时候将Cmd Buffer submit到对应的Queue中去。当然在组建cmd buffer时会用到各种State Objects，许多resources（VB/IB，textures）以及resource descriptors。</strong></li></ul><p><br>本文地址 <a href="http://yjaelex.github.io/2015/09/27/原-AMD-Mantle-API-学习笔记-Mantle简介/">http://yjaelex.github.io/2015/09/27/原-AMD-Mantle-API-学习笔记-Mantle简介/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;最近一段时间准备学习新的下一代graphics API，DX12和Vulkan。发现目前公开的资料不多，特别是Vulkan，kronos的速度也真慢，正式的Spec还没有出来。倒是LunarG出了个SDK和其在intel平台上实现的Vulkan driver，只是笔者一直没找到公开的代码。&lt;/p&gt;&lt;p&gt;其实，无论DX12还是Vulkan都是来自AMD的Mantle，而AMD也已经放出了公开的&lt;a href=&quot;http://www.amd.com/Documents/Mantle-Programming-Guide-and-API-Reference.pdf&quot; title=&quot;Mantle API&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Mantle API Refernce Doc&lt;/a&gt;. 这份文档写的很不错，条理清晰很有价值；笔者建议有兴趣的朋友一定要拜读一下。于是笔者决定先从新标准的鼻祖mantle入手学习，结合笔者多年图形学工作的经验，希望能对下一代API能有个叫深刻的认识。在此也把自己学习体会和大家分享一下，当做读书笔记吧：）&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[转]A look at the PowerVR graphics architecture: Tile-based rendering</title>
    <link href="http://yjaelex.github.io/2015/09/15/%E8%BD%AC-A-look-at-the-PowerVR-graphics-architecture-Tile-based-rendering/"/>
    <id>http://yjaelex.github.io/2015/09/15/转-A-look-at-the-PowerVR-graphics-architecture-Tile-based-rendering/</id>
    <published>2015-09-15T08:21:23.000Z</published>
    <updated>2016-12-29T08:39:26.111Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><p>转一篇Imagination论坛上的牛人写的关于Tile-Based Rendering的文章。有空的话可以翻译下<img src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/smile.gif" alt="微笑"></p><a id="more"></a><p>#<br>A look at the PowerVR graphics architecture: Tile-based rendering</p><p>I’m fond of telling the story about why I joined Imagination. It goes along the lines of: despite offers to go work on graphics in much sunnier climes, I took the job working on PowerVR Graphics here in distinctly un-sunny Britain because I was really interested<br>in how&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><a href="http://www.imgtec.com/powervr/powervr-architecture.asp" rel="external nofollow noopener noreferrer" target="_blank">Tile-Based<br>Deferred Rendering (TBDR)</a></span></span>&nbsp;could work in practice. My graphics career to-date had been mostly focused on the conceptually simpler&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0">Immediate<br>Mode Renderers (IMRs)</span></span>&nbsp;of the day – mostly GeForces and Radeons.</p><p>And no offence to the folks who designed said GeForces and Radeons – a few of whom I am friends with and many more I know quite well, but the front-end architecture of a modern discrete IMR GPU isn’t the most exciting thing in the world. Designed around having<br>plenty of dedicated bandwidth, those GPUs go about the job of painting pixels in a reasonably inefficient way, but one that’s conceptually simple and manifests itself in silicon in a similarly simple way, which makes it relatively easy for the GPU architect<br>to design, spec and have built by the hardware team.</p><p><a href="http://blog.imgtec.com/wp-content/uploads/2013/05/IMR-Pipeline.jpg" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://blog.imgtec.com/wp-content/uploads/2013/05/IMR-Pipeline.jpg" alt="IMR Pipeline"></a>Immediate<br>Mode Rendering at work</p><p>With an IMR you send work to the GPU and it gets drawn straight away. There’s little connection to what else has already been drawn, or will be drawn in the future. You send triangles, you shade them. You rasterise them into pixels, you shade those. You send<br>the rendered pixels to the screen. Triangles in, pixels out, job done! But, crucially the job is done with no context of what’s already happened, or what might happen in the future.</p><p>PowerVR GPUs are about as different as they come in that respect, and it’s that which made me take the job here, to figure out how PowerVR’s architects, hardware teams and software folks had made TBDR actually work in real products. My instinct was that TBDRs<br>would be too complex to build so that they’d work well and actually provide a benefit. I had a chance to figure it out and five years later I’m still here, helping figure out how we’ll evolve it in the future, along with the rest of the GPU’s microarchitecture.</p><p>As far as the graphics programmer is concerned, PowerVR still looks like&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0">triangles in, pixels out, job done</span>. But under the hood something<br>much more exciting is happening. And while the exciting part put me in this chair so I could write about it 5 years later, crucially it’s also that other good E word: efficient!</p><p>####<br>It always starts with the classic TBDR vs. IMR debate</p><p>To help understand why, let’s keep talking about IMRs. One of the biggest things stopping a desktop-class IMR scaling down to fit the power, performance and area budgets of modern embedded application processors is bandwidth. It’s such a scarce resource, even<br>in high-end processors – mostly because of power, area, wiring and packaging limitations, among other things – that you&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0">really</span></span>&nbsp;need<br>to use it as efficiently as possible.</p><p>IMRs don’t do that very well, especially when pixel shading. Remember that there are usually a great many more pixels being rendered than the vertices used to build triangles. On top of that, with an IMR pixels are often still shaded despite never being visible<br>on the screen, and that costs large amounts of precious bandwidth and power. Here’s why.</p><p>Textures for those pixels need to be sampled, and those pixels need to be written out to memory – and often read back in and written out again! – before being drawn on the screen. While all modern IMRs have means in hardware to try and avoid some of that redundant<br>work, say one building in the background being completely obscured by one drawn closer to you, there are things the application developer can do to effectively disable those mechanisms, such as always drawing the building in the background first.</p><p>In our architecture it doesn’t really matter how the application developer draws what’s on the screen. There are exceptions for non-opaque geometry, which the developer still needs to manage, but otherwise we’re submission order independent. That capability<br>is something we’ve had in our hardware since before we were ever an IP company and still made&nbsp;<a href="http://www.imgtec.com/news/detail.asp?ID=58" rel="external nofollow noopener noreferrer" target="_blank">our<br>own standalone PC and console GPUs</a>. You could draw the building in the background first, then the one in the foreground on top, and we’ll never perform pixel shading for the first one, unlike an IMR.</p><p>We effectively sort all of the opaque geometry in the GPU, regardless of how and when it was submitted by the application, to figure out the top-most triangles. Sure, if a developer perfectly sorts their geometry then an IMR can get much closer to our efficiency,<br>but that’s not the common case by any means.</p><p><a href="http://blog.imgtec.com/wp-content/uploads/2013/05/TBDR-Pipeline.jpg" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://blog.imgtec.com/wp-content/uploads/2013/05/TBDR-Pipeline.jpg" alt="TBDR Pipeline"></a>PowerVR<br>TBDRs</p><p>Think again about all the work that’s saving, especially for modern content: for&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0">every<br>pixel</span></span>shaded there are going to be a non-trivial amount of texture lookups for various things, dozens and sometimes hundreds of ALU cycles spent to run computation on that texture data in order to apply the right effects, which often means writing<br>the pixel out to an intermediate surface to be read back in again in a further rendering pass, and then the pixel needs to be stored in memory at the end of shading, so it can be displayed on screen.</p><p>And that’s just one optimisation that we have. So even though we’ve avoided processing completely occluded geometry, there’s still bandwidth saving work we can do at the pixel processing stage. Because we split the screen up into tiles, where we figure out<br>all of the geometry that contributes to the tile so we only process what we need to, and we know exactly how big the tile is (currently 32×32 pixels, but it’s been smaller and even non-square in prior designs), we can build enough on-chip storage to process<br>a few of those tiles at a time, without having to use storage in external memory again until we’ve finished and want to write the final pixels out.</p><p><a href="http://blog.imgtec.com/wp-content/uploads/2013/10/TBDR-architecture.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://blog.imgtec.com/wp-content/uploads/2013/10/TBDR-architecture.png" alt="TBDR architecture"></a>PowerVR<br>GPUs split the screen into tiles</p><p>There are secondary benefits to working on screen, region at a time; benefits that other GPUs take advantage of too: because it’s highly likely that a pixel on the screen will share some data with its immediate neighbours, it’s likely that when we move on to<br>processing the neighbouring pixels that we’ve fetched the data into cache and don’t have to wait for another set of external memory accesses, saving bandwidth again. It’s a classic exploitation of spatial locality that’s present in a lot of modern 3D rendering.</p><p>So that’s the top-level view of the biggest benefits of a TBDR versus an IMR in terms of processing and (especially) bandwidth efficiency. But how does it actually work in hardware? If you’re not too hardware inclined you can stop here! If you go no further,<br>you’ll still have understood the big top-level benefits of how we go about making best use of the available and very precious bandwidth, throughout rendering on in embedded, low-power systems.</p><p>####<br>How TBDR works in hardware</p><p>For those interested in how things happen in the hardware, let’s talk about the tiler in context of&nbsp;<a href="http://blog.imgtec.com/powervr/powervr-rogue-designing-an-optimal-architecture-for-graphics-and-gpu-compute" rel="external nofollow noopener noreferrer" target="_blank">a<br>modern Rogue GPU</a>.</p><p>A 3D graphics application starts by telling us where its geometry is in memory, so we ask the GPU to fetch it and perform vertex shading. We have blocks in our GPUs that are responsible for the non-programmable steps of each kind of task type, called&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0">the<br>data masters</span></span>. They do a bunch of different things on behalf of&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0">the<br>Universal Shading Cluster or USC</span></span>&nbsp;(our shading core) to do the fixed function bits of any workload, including fetch data from memory. So because we’re vertex shading, it’s&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0">the<br>vertex data master (VDM)</span></span>&nbsp;that gets involved at this point, to fetch the vertex data from memory based on information provided by the driver.</p><p><a href="http://blog.imgtec.com/wp-content/uploads/2014/10/PowerVR-Series7-Series7XT-architecture.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://blog.imgtec.com/wp-content/uploads/2014/10/PowerVR-Series7-Series7XT-architecture.png" alt="PowerVR Series7 - Series7XT architecture"></a>PowerVR<br>Series7XT is the latest family of Rogue GPUs</p><p>The data could be stored in memory as lines, triangles or points. It could be indexed or non-indexed. There are associated shader programs and accompanying data for those programs. The VDM fetches whatever’s needed, using another couple of internally-programmable<br>blocks to help, and emits it all to the USC for vertex shading. The USC runs the shader program and the output vertices are stored on-chip.</p><p>They’re then consumed by hardware that performs primitive assembly, certain kinds of culling, and then clipping. If the geometry is back-facing or can be determined to be completely off the screen, it’s culled. All of the remaining on-screen front-facing geometry<br>is sent to be clipped. There’s a fast path here for geometry that doesn’t intersect a clip plane, to let it make onwards progress with no extra processing bar the intersection test. If the geometry intersects with a plane, the clipper generates new geometry<br>so that passed-on vertices are fully on-screen (even though they might be right at the edges). The clipper can do some other cool things, but they’re not too relevant for a big picture explanation like this.</p><p>Then we’re on to where a lot of the magic happens, compared to IMRs. We’re obviously aiming to run computation in multiple phases in the hardware, to maximise efficiency and occupancy: one front-end phase to figure out what’s going on with shaded geometry and<br>bin it into tiles, then one phase to consume that binned data, rasterise it and pass it on for pixel shading and final write-out. To keep things as efficient as possible that intermediate acceleration structure between the two main phases has to be as optimal<br>as we can make it.</p><p>Clearly it’s a bandwidth cost to create it and read it back, one which our competitors like to pick on when it comes to a competitive advantage they have over us. And it’s true; an IMR doesn’t have to deal with it. But given the bandwidth savings we have in<br>our processing model, creating that acceleration structure – which we call&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0">the<br>Parameter Buffer (PB)</span></span>&nbsp;– before feeding it to our trick rasteriser, we still end up with a huge bandwidth advantage in typical rendering situations, especially complex game-like scenes.</p><p>So how do we generate the PB? The clipper outputs a stream of primitives and render target IDs into memory, grouped by render target. Think of it as a container around collections of related geometry. The relationship is critical: we don’t want to read it later<br>and not consume a majority of the data that’s inside, since that’d be wasteful. The output at this stage is the main data structure stored in the PB. We then compress the memory, and in the general case it always compresses very well, so we save quite a lot<br>of PB creation bandwidth just from that step alone.</p><p>####<br>The concept of tiling</p><p>Now for the bit that most people sort of understand about our hardware architecture: tiling. The tiling engine has one main job: output some data that marks out a tiled region, some associated state, and a set of pointers to the geometry that contributes to<br>that region. We also store masks just in case a primitive doesn’t actually contribute to the region, but is stored in memory anyway. That lets us save some bandwidth and processing for that geometry, because it doesn’t contribute to the tile.</p><p>We call the resulting data structure a primitive list. If you’ve ever consumed any of&nbsp;<a href="http://community.imgtec.com/developers/powervr/documentation/" rel="external nofollow noopener noreferrer" target="_blank">our<br>developer documentation</a>, you’ll have seen mention of primitive lists as the intermediate data structure between the front-end phase and the pixel processing phase. Next, some more magic that’s specific to the PowerVR way of doing things.</p><p>Imagine you were tasked with building this bit of the architecture yourself, where you had to determine what regions need to be rasterised for a given set of geometries. There’s one obvious algorithm you could choose: the bounding box. Draw a box around the<br>triangle that covers its extents, and whatever tiles that box touches are the ones you rasterise for that triangle. That falls down pretty quickly though, efficiency wise.</p><p>Imagine a fairly long and thin triangle drawn across the screen in any orientation. You can quickly picture that the bounding box for that triangle is going to lie over tiles that the triangle doesn’t actually touch. So when you rasterise, you’re going to generate<br>work for your shading core, but where nothing is actually going to happen in terms of a contribution to the screen.</p><p>Instead, we have an algorithm baked into the hardware which we call&nbsp;<span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0"><span style="margin:0;padding:0;border:0;outline:0;vertical-align:baseline;background:0 0">perfect<br>tiling</span></span>. It works as you’d expect: we only generate tile lists where the geometry actually covers some area in the tile. It’s one of the most optimised and most efficient parts of the design. The perfect tiling engine generates that perfect list<br>of tiles for a given set of geometry.</p><p><a href="http://blog.imgtec.com/wp-content/uploads/2015/03/PowerVR-TBDR-perfect-tiling.png" rel="external nofollow noopener noreferrer" target="_blank"><img src="http://blog.imgtec.com/wp-content/uploads/2015/03/PowerVR-TBDR-perfect-tiling.png" alt="PowerVR TBDR - perfect tiling"></a>PowerVR<br>perfect tiling vs. bounding box or hierarchical tiling</p><p>That tile information plus the primitive lists are packed into the PB as efficiently as we can, and that’s conceptually pretty much it. In reality there’s a heck of a lot that still happens here in the hardware at the back-end phase of tiling, to fill the PB<br>and organise and marshal the actual memory accesses for the external memory writes, but in terms of functionality to wrap your head around, we’re pretty much done.</p><p>That front-end hardware architecture for us is really where a really big chuck of the efficiency gains can be found in a modern PowerVR GPU, compared to some of our competition. Surprisingly to those who find out, it’s part of the hardware architecture that’s<br>not actually that different, at least at the top-level, between Rogue and SGX. While we completely redesigned the shader core for Rogue, the front-end architecture actually bears a strong resemblance to the one you’ll find in&nbsp;<a href="http://blog.imgtec.com/powervr/understanding-powervr-sgx-mobiles-leading-gpu" rel="external nofollow noopener noreferrer" target="_blank">the<br>later generation SGX GPU IPs</a>. It works and it works very well.</p><p>And now that I’m done explaining the tiling part of our TBDR, it’s a good excuse to stop! I’ll come back to the deferred rendering part in a future blog post, so stay tuned.</p><p><br>本文地址 <a href="http://yjaelex.github.io/2015/09/15/转-A-look-at-the-PowerVR-graphics-architecture-Tile-based-rendering/">http://yjaelex.github.io/2015/09/15/转-A-look-at-the-PowerVR-graphics-architecture-Tile-based-rendering/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;转一篇Imagination论坛上的牛人写的关于Tile-Based Rendering的文章。有空的话可以翻译下&lt;img src=&quot;http://static.blog.csdn.net/xheditor/xheditor_emot/default/smile.gif&quot; alt=&quot;微笑&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>[原]AMD Mantle API 学习笔记 -- Mantle初始化</title>
    <link href="http://yjaelex.github.io/2015/09/09/%E5%8E%9F-AMD-Mantle-API-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Mantle%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://yjaelex.github.io/2015/09/09/原-AMD-Mantle-API-学习笔记-Mantle初始化/</id>
    <published>2015-09-09T13:50:24.000Z</published>
    <updated>2016-12-29T08:40:38.570Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --><p>本系列文章是笔者研究mantle的一些心得；其目的是为了学习新的图形API标准Vulkan。因为Vulkan还没有正式发布，而它事实上是基于mantle的，所以研究mantle可以让我们对新一代图形标准（Vulkan和D3D12）有一个提前认识。在Vulkan正式发布后，笔者也会写一系列的文章来介绍Vulkan。事实上，mantle的API函数都是以gr开头的；而Vulkan很多API只是简单的替换为vk开头而已。这进一步说明学习mantle的价值。</p><p>要学习一个新的API，最好就是用它来写个简单的demo。国外有位牛人已经写了个mantle版的Hello World：<a href="https://medium.com/@Overv/implementing-hello-triangle-in-mantle-4302450fbcd2" rel="external nofollow noopener noreferrer" target="_blank">Implementing Hello Triangle with Mantle</a>。这个例子的代码在：<a href="https://github.com/Overv/MantleHelloTriangle" rel="external nofollow noopener noreferrer" target="_blank">MantleHelloTriangle</a>。笔者接下来的文章都是基于这个例子，试着把mantle的一系列基本概念剖析一下。</p><a id="more"></a><hr><h2 id="初始化Mantle"><a href="#初始化Mantle" class="headerlink" title="初始化Mantle"></a>初始化Mantle</h2><p>使用mantle首先要初始化；主要是调用“grInitAndEnumerateGpus()”来获取系统中所有GPU的handle，然后调用”grGetGpuInfo()”得到每个物理GPU的属性。另外还可以获取<code>GPU_PERFORMANCE</code>的特性，这样Apps可以根据这些信息来选择使用哪个GPU。</p><pre><code>GR_CHAR appName[] = &quot;HelloWorld&quot;;
GR_APPLICATION_INFO appInfo = {};
appInfo.pAppName = appName;
appInfo.pEngineName = appName;
appInfo.apiVersion = GR_API_VERSION;

// initialize and enumerate all Mantle enabled GPUs
GR_RESULT result = grInitAndEnumerateGpus(&amp;appInfo, NULL, &amp;m_gpuCount, &amp;m_gpus[0]);
MANTLE_ASSERT(result);

if(result == GR_SUCCESS)
{
    // retrieve the GPU information for all GPUs
    for(GR_UINT32 gpu = 0; gpu &lt; m_gpuCount; ++gpu)
    {
        GR_SIZE gpuInfoSize = 0;
        GR_PHYSICAL_GPU_PROPERTIES gpuInfo = {};
        // first query the size of the gpuInfo
        result = grGetGpuInfo(m_gpus[gpu], GR_INFO_TYPE_PHYSICAL_GPU_PROPERTIES, &amp;gpuInfoSize, NULL);
        MANTLE_ASSERT(result);
        // retrieve the GPU physical properties
        result = grGetGpuInfo(m_gpus[gpu], GR_INFO_TYPE_PHYSICAL_GPU_PROPERTIES, &amp;gpuInfoSize, &amp;gpuInfo);
        MANTLE_ASSERT(result);
    }
}
</code></pre><h2 id="Device和Queue"><a href="#Device和Queue" class="headerlink" title="Device和Queue"></a>Device和Queue</h2><p>选取GPU后就可以在此GPU上创建Device了。Device代表了一个在某GPU上运行的上下文（execution context）；这个概念与D3D里的ID3DDevice对象类似。</p><pre><code>if(result == GR_SUCCESS)
{
    // use the WSI_WINDOWS extension
    // required to output results in a window on Windows
    const GR_CHAR *extensions[] =
    {
        &quot;GR_WSI_WINDOWS&quot;
    };

    // 先检查extension是否支持
    for(int ext = 0; ext &lt; sizeof(extensions) / sizeof(extensions[0]); ++ext)
    {
        result = grGetExtensionSupport(gpus[0], extensions[ext]);
    }

    // only use one universal queue
    GR_DEVICE_QUEUE_CREATE_INFO dqinfo = {};
    dqinfo.queueCount = 1;
    dqinfo.queueType = GR_QUEUE_UNIVERSAL;

    //device creation info
    GR_DEVICE_CREATE_INFO info = {};
    info.queueRecordCount = 1;
    info.pRequestedQueues = &amp;dqinfo;
    info.extensionCount = sizeof(extensions) / sizeof(GR_CHAR *);
    info.ppEnabledExtensionNames = extensions;

    // 正式版本默认无需任何validation
    // 调试时可以打开validation并且设置为break on error模式
#ifdef _DEBUG
    info.flags = GR_DEVICE_CREATE_VALIDATION;
    info.maxValidationLevel = GR_VALIDATION_LEVEL_4;
    GR_BOOL grTrue = GR_TRUE;
    result = grDbgSetGlobalOption(GR_DBG_OPTION_BREAK_ON_ERROR, sizeof(grTrue), &amp;grTrue);
#else
    info.maxValidationLevel = GR_VALIDATION_LEVEL_0;
#endif
    // create the device
    result = grCreateDevice(gpus[0], &amp;info, &amp;device);
}
</code></pre><p>在这里我们在默认gpu0上创建了一个device；其主要需要的参数是要支持的extension，需要的queue以及validation的级别。<br>在此我们有必要理一下物理显卡（adaptor），GPU，Screen/Display，Device和Queue之间的关系；这种基本概念之间的联系在以后的标准Vulkan里也是适用的。<br><img src="http://img.blog.csdn.net/20150913173840443" alt="Mantle-Display-Screen"><br>这里我们只用到了一个extension：WSI_WINDOWS。这是为了获取相应的Display对象，从而将渲染结果Swap到屏幕上去显示。<br>有了Device后，我们就可以得到Queue了。Mantle的设计模式是应用程序生成并填写Cmd Buffers；然后再将Cmd Buffers submit到相应的Queue中去（Queue和Engine对应）。后面笔者将专门写一编关于Queue的文章。目前我们只要关心大致上Mantle支持三种Queue：Universal（Gfx和Compute）、Compute（通用计算，可以异步与渲染Queue）和DMA。</p><pre><code>// create the universal queue
result = grGetDeviceQueue(m_device, GR_QUEUE_UNIVERSAL, 0, &amp;m_universalQueue);
</code></pre><p>到此为止，mantle初始化完成，device和queue也都准备好了。</p><p><br>本文地址 <a href="http://yjaelex.github.io/2015/09/09/原-AMD-Mantle-API-学习笔记-Mantle初始化/">http://yjaelex.github.io/2015/09/09/原-AMD-Mantle-API-学习笔记-Mantle初始化/</a> 作者为<a href="/about/"> Alex</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Dec 29 2016 17:14:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;本系列文章是笔者研究mantle的一些心得；其目的是为了学习新的图形API标准Vulkan。因为Vulkan还没有正式发布，而它事实上是基于mantle的，所以研究mantle可以让我们对新一代图形标准（Vulkan和D3D12）有一个提前认识。在Vulkan正式发布后，笔者也会写一系列的文章来介绍Vulkan。事实上，mantle的API函数都是以gr开头的；而Vulkan很多API只是简单的替换为vk开头而已。这进一步说明学习mantle的价值。&lt;/p&gt;&lt;p&gt;要学习一个新的API，最好就是用它来写个简单的demo。国外有位牛人已经写了个mantle版的Hello World：&lt;a href=&quot;https://medium.com/@Overv/implementing-hello-triangle-in-mantle-4302450fbcd2&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Implementing Hello Triangle with Mantle&lt;/a&gt;。这个例子的代码在：&lt;a href=&quot;https://github.com/Overv/MantleHelloTriangle&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;MantleHelloTriangle&lt;/a&gt;。笔者接下来的文章都是基于这个例子，试着把mantle的一系列基本概念剖析一下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Mantle" scheme="http://yjaelex.github.io/tags/Mantle/"/>
    
      <category term="Graphics" scheme="http://yjaelex.github.io/tags/Graphics/"/>
    
  </entry>
  
</feed>
